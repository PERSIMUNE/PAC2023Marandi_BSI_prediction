{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a7d8031c",
   "metadata": {},
   "source": [
    "# Prediction of bloodstream infection based on biochemical data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8fa3646a",
   "metadata": {},
   "source": [
    "## Load data and libraries and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4eaeab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn import preprocessing, ensemble\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Constants\n",
    "SEED = 123\n",
    "FOLDS = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Load datasets\n",
    "devset = pd.read_csv('devset.csv')\n",
    "testset = pd.read_csv('testset.csv')\n",
    "\n",
    "# Copy IDs for reference\n",
    "devset_ID = devset['ID'].copy()\n",
    "testset_ID = testset['ID'].copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['ID', 'most_common_pathogens']\n",
    "devset = devset.drop(columns=columns_to_drop)\n",
    "testset = testset.drop(columns=columns_to_drop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73607a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary with old column names as keys and new column names as values\n",
    "column_names = {'PrevInfectionRate': 'PBCR', 'PrevAdmissionRate': 'CER', 'biochemical_abnormality_score': 'BVA', 'biochemical_abnormality_score_NAexcluded': 'NBVA', 'modified_biochemical_abnormality_score': 'SBVA'}\n",
    "\n",
    "# Rename the columns using the rename() method\n",
    "devset = devset.rename(columns=column_names)\n",
    "testset = testset.rename(columns=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493315f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = devset.drop(columns='PBCR')\n",
    "testset = testset.drop(columns='PBCR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711b857",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.concat([devset,testset],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f5b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb97c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mydata = pd.concat([devset,testset],axis=0)\n",
    "# Convert 'TestDate' column to datetime format\n",
    "mydata['TestDate'] = pd.to_datetime(mydata['TestDate'], errors='coerce')\n",
    "\n",
    "# Drop rows with invalid datetime values\n",
    "mydata.dropna(subset=['TestDate'], inplace=True)\n",
    "\n",
    "# Extract numeric component from 'TestDate' column\n",
    "numeric_days = (mydata['TestDate'] - mydata['TestDate'].min()).dt.total_seconds() / (24 * 60 * 60)  # Convert to days\n",
    "\n",
    "# Determine the bin size automatically using the Freedman-Diaconis rule\n",
    "n = len(numeric_days)\n",
    "iqr = np.percentile(numeric_days, 75) - np.percentile(numeric_days, 25)\n",
    "bin_size = 2.0 * iqr / (n ** (1/3))\n",
    "\n",
    "# Create bins based on the determined bin size\n",
    "min_date = mydata['TestDate'].min()\n",
    "numeric_days = numeric_days - numeric_days.min()\n",
    "mydata['time_bin'] = (np.floor(numeric_days / bin_size) * bin_size + numeric_days.min()).apply(lambda x: min_date + pd.Timedelta(days=x))\n",
    "\n",
    "# Calculate correlations for each bin (using Spearman correlation)\n",
    "# outcome_var = 'outcome'  # Replace 'outcome' with the actual name of your outcome variable\n",
    "correlations = {}\n",
    "for bin_start, group in mydata.groupby('time_bin'):\n",
    "    correlations[bin_start] = group.corr(method='spearman')[outcome_var]\n",
    "\n",
    "# Create a DataFrame with correlation coefficients for each variable\n",
    "correlation_df = pd.DataFrame(correlations)\n",
    "\n",
    "# Sort features by median correlation in descending order\n",
    "median_correlations = correlation_df.median(axis=1)\n",
    "sorted_features = median_correlations.sort_values(ascending=False).index\n",
    "# Exclude the outcome variable from the sorted features\n",
    "sorted_features = sorted_features[sorted_features != outcome_var]\n",
    "correlation_df_sorted = correlation_df.loc[sorted_features]\n",
    "\n",
    "translated_feature_names = [data_dictionary.get(feature, feature) for feature in correlation_df_sorted.index]\n",
    "# Set font size for the plot\n",
    "plt.rc('font', size=7)\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 11))\n",
    "ax = sns.heatmap(correlation_df_sorted, cmap='bwr', vmin=-1, vmax=1, annot=False, fmt=\".2f\")\n",
    "plt.xlabel(f'Time Bins: {round(bin_size)} days')\n",
    "plt.ylabel('Features')\n",
    "plt.title(f'Spearman correlation of features with {outcome_var} over time')\n",
    "\n",
    "# Set x-axis tick labels to show a maximum of 10 ticks\n",
    "num_ticks = min(10, len(correlation_df.columns))\n",
    "x_ticks_indices = np.linspace(0, len(correlation_df.columns) - 1, num_ticks).astype(int)\n",
    "x_ticks_labels = [label.strftime('%Y-%m-%d') for label in correlation_df.columns[x_ticks_indices]]\n",
    "plt.xticks(x_ticks_indices + 0.5, x_ticks_labels, rotation=45)\n",
    "y_ticks_labels = [f\"{translated_feature_names[i]}: {round(median_correlations[feature], 2)}\" for i, feature in enumerate(sorted_features)]\n",
    "plt.yticks(np.arange(len(correlation_df_sorted)) + 0.5, y_ticks_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('Spearman_correlation_overtime.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b901d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Calculate mutual information for each bin\n",
    "mutual_infos = {}\n",
    "for bin_start, group in mydata.groupby('time_bin'):\n",
    "    X = group.drop([outcome_var, 'time_bin'], axis=1).select_dtypes(include=[np.number])  # Numeric features only\n",
    "    y = group[outcome_var]  # Outcome variable\n",
    "    # Compute mutual information with class labels\n",
    "    mi_scores = mutual_info_classif(X, y, discrete_features='auto', random_state=SEED)  # Use mutual_info_classif\n",
    "    mutual_infos[bin_start] = mi_scores\n",
    "\n",
    "# Create a DataFrame with mutual information scores for each variable\n",
    "mi_df = pd.DataFrame(mutual_infos)\n",
    "\n",
    "# Sort features by median mutual information in descending order\n",
    "median_mi_scores = mi_df.median(axis=1)\n",
    "sorted_features = median_mi_scores.sort_values(ascending=False).index\n",
    "mi_df_sorted = mi_df.loc[sorted_features]\n",
    "\n",
    "# Set font size for the plot\n",
    "plt.rc('font', size=7)\n",
    "\n",
    "translated_feature_names = [data_dictionary.get(feature, feature) for feature in X.columns[sorted_features]]\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 11))\n",
    "ax = sns.heatmap(mi_df_sorted, cmap='Reds', annot=False, fmt=\".2f\")\n",
    "plt.xlabel(f'Time Bins: {round(bin_size)} days')\n",
    "plt.ylabel('Features')\n",
    "plt.title(f'Mutual Information of features with {outcome_var} over time')\n",
    "\n",
    "# Set x-axis tick labels to show a maximum of 10 ticks\n",
    "num_ticks = min(10, len(mi_df.columns))\n",
    "x_ticks_indices = np.linspace(0, len(mi_df.columns) - 1, num_ticks).astype(int)\n",
    "x_ticks_labels = [label.strftime('%Y-%m-%d') for label in mi_df.columns[x_ticks_indices]]\n",
    "plt.xticks(x_ticks_indices + 0.5, x_ticks_labels, rotation=45)\n",
    "\n",
    "# Set y-axis tick labels with translated feature names and median mutual information scores\n",
    "y_ticks_labels = [f\"{translated_feature_names[i]}: {round(median_mi_scores[feature], 2)}\" for i, feature in enumerate(sorted_features)]\n",
    "plt.yticks(np.arange(len(mi_df_sorted)) + 0.5, y_ticks_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d508370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pointbiserialr\n",
    "\n",
    "# Calculate association measure for each bin\n",
    "association_measures = {}\n",
    "for bin_start, group in mydata.groupby('time_bin'):\n",
    "    X = group.drop([outcome_var, 'time_bin'], axis=1).select_dtypes(include=[np.number])  # Numeric features only\n",
    "    y = group[outcome_var]  # Outcome variable\n",
    "    \n",
    "    # Compute association measure (Point-Biserial Correlation) for each feature\n",
    "    association_scores = [pointbiserialr(X[feature], y)[0] for feature in X.columns]\n",
    "    \n",
    "    association_measures[bin_start] = association_scores\n",
    "\n",
    "# Create a DataFrame with association measures for each variable\n",
    "association_df = pd.DataFrame(association_measures)\n",
    "\n",
    "# Sort features by median association measure in descending order\n",
    "median_association_scores = association_df.median(axis=1)\n",
    "sorted_features = median_association_scores.sort_values(ascending=False).index\n",
    "association_df_sorted = association_df.loc[sorted_features]\n",
    "\n",
    "# Set font size for the plot\n",
    "plt.rc('font', size=7)\n",
    "\n",
    "# Replace feature names with translated names from the data_dictionary\n",
    "\n",
    "translated_feature_names = [data_dictionary.get(feature, feature) for feature in X.columns[sorted_features]]\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 11))\n",
    "ax = sns.heatmap(correlation_df_sorted, cmap='bwr', vmin=-1, vmax=1, annot=False, fmt=\".2f\")\n",
    "plt.xlabel(f'Time Bins: {round(bin_size)} days')\n",
    "plt.ylabel('Features')\n",
    "plt.title(f'Point-Biserial correlation of the features with {outcome_var} over time')\n",
    "\n",
    "# Set x-axis tick labels to show a maximum of 10 ticks\n",
    "num_ticks = min(10, len(association_df.columns))\n",
    "x_ticks_indices = np.linspace(0, len(association_df.columns) - 1, num_ticks).astype(int)\n",
    "x_ticks_labels = [label.strftime('%Y-%m-%d') for label in association_df.columns[x_ticks_indices]]\n",
    "plt.xticks(x_ticks_indices + 0.5, x_ticks_labels, rotation=45)\n",
    "\n",
    "# Set y-axis tick labels with translated feature names and median association scores\n",
    "y_ticks_labels = [f\"{translated_feature_names[i]}: {round(median_association_scores[feature], 2)}\" for i, feature in enumerate(sorted_features)]\n",
    "plt.yticks(np.arange(len(association_df_sorted)) + 0.5, y_ticks_labels)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2037459b",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = devset.drop(columns=['TestDate']) # \n",
    "testset = testset.drop(columns=['TestDate']) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e74556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features and columns that should not be used in this model\n",
    "cat_features = [\"Sex\"]\n",
    "devset[cat_features] = devset[cat_features].astype('category')\n",
    "testset[cat_features] = testset[cat_features].astype('category')\n",
    "\n",
    "for feature in cat_features:\n",
    "    devset[feature] = devset[feature].astype('str')\n",
    "    testset[feature] = testset[feature].astype('str')\n",
    "devset[cat_features] = devset[cat_features].astype('category')\n",
    "testset[cat_features] = testset[cat_features].astype('category')\n",
    "\n",
    "\n",
    "for feature in cat_features:\n",
    "    categories = pd.Categorical(devset[feature])\n",
    "    print(f\"Categories of {feature}: {categories.categories}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3380a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Replacing -1 with NaN in devset\n",
    "devset_withmissing = devset.replace(-1, np.nan)\n",
    "\n",
    "# Replacing -1 with NaN in testset\n",
    "testset_withmissing = testset.replace(-1, np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05be288",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Calculate the ratio in devset\n",
    "devset_withmissing['NEUTRO_to_LYMFO'] = np.where(devset_withmissing['LYMFO'] != 0, devset_withmissing['NEUTRO'] / devset_withmissing['LYMFO'], np.nan)\n",
    "devset_withmissing['Platelet-to-lymphocyte'] = np.where(devset_withmissing['LYMFO'] != 0, devset_withmissing['THROM'] / devset_withmissing['LYMFO'], np.nan)\n",
    "# Calculate the ratio in testset\n",
    "testset_withmissing['NEUTRO_to_LYMFO'] = np.where(testset_withmissing['LYMFO'] != 0, testset_withmissing['NEUTRO'] / testset_withmissing['LYMFO'], np.nan)\n",
    "testset_withmissing['Platelet-to-lymphocyte'] = np.where(testset_withmissing['LYMFO'] != 0, testset_withmissing['THROM'] / testset_withmissing['LYMFO'], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e15af68",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_withmissing = pd.concat([devset_withmissing,testset_withmissing], axis = 0)\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_cleaned = all_data_withmissing.dropna(subset=['NEUTRO_to_LYMFO'])\n",
    "\n",
    "# Calculate median\n",
    "median = np.median(df_cleaned['NEUTRO_to_LYMFO']).round(decimals=2)\n",
    "\n",
    "# Calculate quartiles\n",
    "Q1 = np.percentile(df_cleaned['NEUTRO_to_LYMFO'], 25).round(decimals=2)\n",
    "Q3 = np.percentile(df_cleaned['NEUTRO_to_LYMFO'], 75).round(decimals=2)\n",
    "\n",
    "print(\"Median:\", median)\n",
    "print(\"First Quartile (Q1):\", Q1)\n",
    "print(\"Third Quartile (Q3):\", Q3)\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "missing_percentage = all_data_withmissing['NEUTRO_to_LYMFO'].isnull().mean() * 100\n",
    "\n",
    "print(\"Percentage of missing values:\", missing_percentage.round(decimals=2))\n",
    "print(\"number of missing values:\", all_data_withmissing['NEUTRO_to_LYMFO'].isnull().sum().round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55184362",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data_withmissing = pd.concat([devset_withmissing,testset_withmissing], axis = 0)\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_cleaned = all_data_withmissing.dropna(subset=['Platelet-to-lymphocyte'])\n",
    "\n",
    "# Calculate median\n",
    "median = np.median(df_cleaned['Platelet-to-lymphocyte']).round(decimals=2)\n",
    "\n",
    "# Calculate quartiles\n",
    "Q1 = np.percentile(df_cleaned['Platelet-to-lymphocyte'], 25).round(decimals=2)\n",
    "Q3 = np.percentile(df_cleaned['Platelet-to-lymphocyte'], 75).round(decimals=2)\n",
    "\n",
    "print(\"Median:\", median)\n",
    "print(\"First Quartile (Q1):\", Q1)\n",
    "print(\"Third Quartile (Q3):\", Q3)\n",
    "\n",
    "# Calculate the percentage of missing values\n",
    "missing_percentage = all_data_withmissing['Platelet-to-lymphocyte'].isnull().mean() * 100\n",
    "\n",
    "print(\"Percentage of missing values:\", missing_percentage.round(decimals=2))\n",
    "print(\"number of missing values:\", all_data_withmissing['Platelet-to-lymphocyte'].isnull().sum().round(decimals=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd7af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Separate features and target variable for the training set\n",
    "X_train_withmissing = devset_withmissing.drop(outcome_var, axis=1)\n",
    "y_train = devset_withmissing[outcome_var]\n",
    "\n",
    "# Separate features and target variable for the test set\n",
    "X_test_withmissing = testset_withmissing.drop(outcome_var, axis=1)\n",
    "y_test = testset_withmissing[outcome_var]\n",
    "\n",
    "# Create an instance of the IterativeImputer\n",
    "mice_imputer = IterativeImputer(max_iter=10, random_state=SEED)  # You can adjust the number of iterations as needed\n",
    "\n",
    "# Fit and transform X_train_withmissing with MICE imputation\n",
    "X_train_imputed = mice_imputer.fit_transform(X_train_withmissing)\n",
    "\n",
    "# Convert the imputed array back to a DataFrame with column names\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train_withmissing.columns)\n",
    "\n",
    "# Combine X_train_imputed and y_train into a single DataFrame\n",
    "devset_imputed = pd.concat([X_train_imputed, y_train], axis=1)\n",
    "\n",
    "# Transform X_test_withmissing using the same MICE imputer\n",
    "X_test_imputed = mice_imputer.transform(X_test_withmissing)\n",
    "\n",
    "# Convert the imputed array back to a DataFrame with column names\n",
    "X_test_imputed = pd.DataFrame(X_test_imputed, columns=X_test_withmissing.columns)\n",
    "\n",
    "# Combine X_test_imputed and y_test into a single DataFrame\n",
    "testset_imputed = pd.concat([X_test_imputed, y_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba8d8ea",
   "metadata": {},
   "source": [
    "#### feature selection (optional)\n",
    "\n",
    "uncomment the following blocks if you want to do feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236e0d14",
   "metadata": {},
   "source": [
    "#### end of feature selection (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c4f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save devset_withmissing as CSV\n",
    "devset_withmissing.to_csv('devset_withmissing.csv', index=False)\n",
    "\n",
    "# Save testset_withmissing as CSV\n",
    "testset_withmissing.to_csv('testset_withmissing.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201e7247",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f89e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove features and columns that should not be used in this model\n",
    "cat_features = [\"Sex\"]\n",
    "devset_imputed[cat_features] = devset_imputed[cat_features].astype('category')\n",
    "testset_imputed[cat_features] = testset_imputed[cat_features].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987c1520",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e475bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_withmissing.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2741d3ee",
   "metadata": {},
   "source": [
    "the only variable with missing values was prior_tx where the missing values are now replaced by 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46912223",
   "metadata": {},
   "source": [
    "### data overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "653d277e",
   "metadata": {},
   "source": [
    "### check for missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bfdc5d53",
   "metadata": {},
   "source": [
    "### Use data dictionary to have a description of the variables in results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c41825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the column names from the DataFrame\n",
    "data_columns = devset_imputed.columns\n",
    "\n",
    "# Find keys in data dictionary but not in data columns\n",
    "keys_not_in_columns = set(data_dictionary.keys()) - set(data_columns)\n",
    "\n",
    "# Print the keys that are not in the data columns\n",
    "for key in keys_not_in_columns:\n",
    "    print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b344a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove keys from data dictionary\n",
    "data_dictionary = {key: value for key, value in data_dictionary.items() if key not in keys_not_in_columns}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e908fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns in mydata that are not in data dictionary keys\n",
    "columns_not_in_keys = set(data_columns) - set(data_dictionary.keys())\n",
    "\n",
    "# Print the columns that are not in the data dictionary keys\n",
    "for column in columns_not_in_keys:\n",
    "    print(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mydata = pd.concat(devset_withmissing, testset_withmissing)\n",
    "\n",
    "mydata = pd.concat([devset_withmissing, testset_withmissing]) # , ignore_index=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1267f148",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = mydata.isnull().sum()\n",
    "\n",
    "# Step 2: Divide by the total number of rows\n",
    "total_rows = len(mydata)\n",
    "missing_percentage = (missing_values / total_rows) * 100\n",
    "\n",
    "# Step 3: Round the percentages to two decimal points\n",
    "missing_percentage = missing_percentage.round(2)\n",
    "\n",
    "# Step 4: Sort the percentages in ascending order\n",
    "missing_percentage = missing_percentage.sort_values(ascending=False)\n",
    "\n",
    "# Step 5: Calculate the mean and standard deviation of the missingness\n",
    "mean_missingness = np.mean(missing_percentage)\n",
    "std_missingness = np.std(missing_percentage)\n",
    "\n",
    "# Step 6: Display the missing percentages, mean, and standard deviation\n",
    "print(\"Missing Value Percentages:\")\n",
    "print(missing_percentage)\n",
    "print(\"Mean ± Standard Deviation of Missingness: {:.2f} ± {:.2f}\".format(mean_missingness, std_missingness))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2b51d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select continuous variables from the DataFrame\n",
    "continuous_vars = mydata.select_dtypes(include=['float64', 'int64'])\n",
    "categorical_vars = mydata.select_dtypes(include=['category'])\n",
    "\n",
    "mydata[outcome_var] = mydata[outcome_var].map({0: 'negative', 1: 'positive'}).astype('category')\n",
    "\n",
    "outcome_variable = mydata[outcome_var].copy()\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_continuous_vars = len(continuous_vars.columns)\n",
    "num_categorical_vars = len(categorical_vars.columns)\n",
    "num_cols_to_plot = 5\n",
    "num_rows = (num_continuous_vars + num_categorical_vars + num_cols_to_plot - 1) // num_cols_to_plot + 1  # Adjust the number of rows based on the number of variables\n",
    "\n",
    "# Create subplots for continuous variables\n",
    "fig, axes = plt.subplots(num_rows, num_cols_to_plot, figsize=(12, num_rows * 2))  # Adjust the figsize as desired\n",
    "\n",
    "# Iterate over continuous variables\n",
    "for i, column in enumerate(continuous_vars.columns):\n",
    "    # Determine the subplot indices\n",
    "    row_idx = i // num_cols_to_plot\n",
    "    col_idx = i % num_cols_to_plot\n",
    "\n",
    "    # Check if subplot index is within the bounds of axes\n",
    "    if row_idx < num_rows:\n",
    "        # Get the axis for the current subplot\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Iterate over each outcome category\n",
    "        for outcome_category, ax_offset in zip(outcome_variable.unique(), [-0.2, 0.2]):\n",
    "            # Filter the data for the current outcome category\n",
    "            filtered_data = continuous_vars[outcome_variable == outcome_category][column]\n",
    "\n",
    "            # Create a box plot for the current outcome category\n",
    "            positions = np.array([1 + ax_offset])\n",
    "            ax.boxplot(filtered_data.dropna(), positions=positions, widths=0.3, vert=False)  # Vert=False for horizontal box plots\n",
    "\n",
    "        ax.set_title(f'{column}', fontsize=8)\n",
    "        ax.set_yticks([1 - ax_offset, 1 + ax_offset])\n",
    "        ax.set_yticklabels(outcome_variable.unique(), fontsize=8)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "        ax.legend(fontsize=6)\n",
    "\n",
    "# Iterate over categorical variables\n",
    "for i, column in enumerate(categorical_vars.columns):\n",
    "    # Determine the subplot indices\n",
    "    row_idx = (i + num_continuous_vars) // num_cols_to_plot\n",
    "    col_idx = (i + num_continuous_vars) % num_cols_to_plot\n",
    "\n",
    "    # Check if subplot index is within the bounds of axes\n",
    "    if row_idx < num_rows:\n",
    "        # Get the axis for the current subplot\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Normalize the counts for the current categorical variable stratified by outcome variable\n",
    "        category_counts = categorical_vars.groupby(outcome_variable)[column].value_counts(normalize=True).unstack()\n",
    "        category_counts.plot(kind='barh', ax=ax)\n",
    "\n",
    "        # Set the title with the feature name\n",
    "        ax.set_title(f'{column}', fontsize=8)\n",
    "\n",
    "        ax.set_ylabel(None)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "        ax.legend(fontsize=6)\n",
    "\n",
    "# Remove any empty subplots at the end\n",
    "if num_continuous_vars + num_categorical_vars < num_rows * num_cols_to_plot:\n",
    "    for i in range(num_continuous_vars + num_categorical_vars, num_rows * num_cols_to_plot):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "# Remove the subplot for outcome_var at the end\n",
    "if num_continuous_vars + num_categorical_vars == num_rows * num_cols_to_plot - 1:\n",
    "    last_ax_index = num_continuous_vars + num_categorical_vars - 1\n",
    "    if last_ax_index >= 0:\n",
    "        fig.delaxes(axes.flatten()[last_ax_index])\n",
    "\n",
    "# Adjust the layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('feature_distributions.png', dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5e4b2433",
   "metadata": {},
   "source": [
    "### summary statistics of the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55defe92",
   "metadata": {},
   "source": [
    "### split the data to a development set for finding the best model and a test set for the validation of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2172521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split mydata into training and test sets\n",
    "# devset, testset = train_test_split(mydata, test_size=0.2, random_state=SEED, stratify=mydata[outcome_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5111980f",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_withmissing.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "60e9f1a6",
   "metadata": {},
   "source": [
    "### associations of the predictors and the outcome variable based on Spearman's correlation (in only the development set)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d38526ec",
   "metadata": {},
   "source": [
    "## Initiating a QLattice model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a79ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feyn\n",
    "ql = feyn.QLattice(random_seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fcfe6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed.loc[:, outcome_var] = devset_imputed[outcome_var].replace({1: True, 0: False}).astype(bool)\n",
    "testset_imputed.loc[:, outcome_var] = testset_imputed[outcome_var].replace({1: True, 0: False}).astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91116333",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# create an empty dictionary to store the stypes\n",
    "stypes = {}\n",
    "\n",
    "# iterate over each column in the dataset\n",
    "for col in devset_imputed.columns:\n",
    "    # check if the column dtype is 'category'\n",
    "    if pd.api.types.is_categorical_dtype(devset_imputed[col]):\n",
    "        # if it is, add the column name to the stypes dictionary with a value of 'c'\n",
    "        stypes[col] = 'c'\n",
    "#     else:\n",
    "#         if pd.api.types.is_numeric_dtype(devset[col]):\n",
    "#             stypes[col] = 'f'\n",
    "\n",
    "stypes[outcome_var] = 'b'\n",
    "# print the stypes dictionary\n",
    "print(stypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe23f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed[outcome_var]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a85bc6b9",
   "metadata": {},
   "source": [
    "### set model weights based on class balance from the development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49730bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=devset_imputed[outcome_var])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc40a546",
   "metadata": {},
   "source": [
    "### model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8849d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Define the number of folds for stratified k-fold\n",
    "n_splits = 5\n",
    "\n",
    "# Define the metrics to evaluate\n",
    "metrics = ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'AUC']\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "results = {metric: [] for metric in metrics}\n",
    "\n",
    "# Initialize stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "\n",
    "y_train = devset_imputed[outcome_var]\n",
    "\n",
    "fold_results_df_fold_table_QLattice = pd.DataFrame()\n",
    "\n",
    "# Convert categorical columns to strings\n",
    "devset_imputed[cat_features] = devset_imputed[cat_features].astype(str)\n",
    "testset_imputed[cat_features] = testset_imputed[cat_features].astype(str)\n",
    "# Perform stratified k-fold cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(devset_imputed, devset_imputed[outcome_var]), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Get the training and test data indices for this fold\n",
    "    X_train_indices, X_test_indices = train_index.tolist(), test_index.tolist()\n",
    "\n",
    "        # Get the training and test data for this fold\n",
    "    X_train_fold = devset_imputed.iloc[X_train_indices, :]\n",
    "    X_test_fold = devset_imputed.iloc[X_test_indices, :]\n",
    "    y_train_fold = y_train.iloc[X_train_indices]\n",
    "    y_test_fold = y_train.iloc[X_test_indices]\n",
    "    sample_weights_fold = sample_weights[X_train_indices]\n",
    "    # sample_weights_fold = compute_sample_weight(class_weight='balanced', y=devset_imputed[outcome_var])\n",
    "   \n",
    "    # Initialize qLattice\n",
    "    ql = feyn.QLattice(random_seed=SEED)\n",
    "    # Train the qLattice model\n",
    "    models = ql.auto_run(\n",
    "        data=X_train_fold,\n",
    "        output_name=outcome_var,\n",
    "        kind='classification',\n",
    "        stypes=stypes,  # Include the stypes parameter\n",
    "        n_epochs=50,\n",
    "        criterion=\"bic\",  # None or \"bic\" # BIC is more conservative than AIC\n",
    "        loss_function='binary_cross_entropy',\n",
    "        max_complexity=10,\n",
    "        sample_weights=sample_weights_fold\n",
    "    )\n",
    "\n",
    "    best_model = models[0]\n",
    "\n",
    "    # Get predictions for the test set\n",
    "    predictions = best_model.predict(X_test_fold)\n",
    "    predictions_class = [True if x >= 0.5 else False for x in predictions]\n",
    "\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test_fold, predictions_class)\n",
    "\n",
    "    # Extract true positive, true negative, false positive, and false negative counts\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate PPV, NPV, sensitivity, and specificity\n",
    "    PPV = tp / (tp + fp)\n",
    "    NPV = tn / (tn + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    auc = roc_auc_score(y_test_fold, predictions)\n",
    "\n",
    "    # Store the results for this fold\n",
    "    fold_results = [PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, auc]\n",
    "    for metric, value in zip(metrics, fold_results):\n",
    "        results[metric].append(value)\n",
    "\n",
    "    # Print the results for this fold\n",
    "    fold_results_df = pd.DataFrame(fold_results, index=metrics, columns=[fold])\n",
    "    fold_results_df = fold_results_df.round(2)\n",
    "    print(fold_results_df.to_string())\n",
    "    # Append the results for this fold to fold_results_df_fold_table_LGBM\n",
    "    fold_results_df_fold_table_QLattice = fold_results_df_fold_table_QLattice.append(fold_results_df.T)\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate the average and standard deviation across folds\n",
    "results_df = pd.DataFrame(results)\n",
    "aggregated_results = results_df.mean().round(2)\n",
    "aggregated_results_sd = results_df.std().round(2)\n",
    "aggregated_results_formatted_QLattice = aggregated_results.astype(str) + \" ± \" + aggregated_results_sd.astype(str)\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Aggregated Results:\")\n",
    "print(aggregated_results_formatted_QLattice.to_string())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d0706780",
   "metadata": {},
   "source": [
    "### model development using the whole development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85a5a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feyn\n",
    "ql = feyn.QLattice(random_seed=SEED)\n",
    "models = ql.auto_run(\n",
    "    data=devset_imputed,\n",
    "    output_name=outcome_var,\n",
    "    kind='classification',\n",
    "    stypes=stypes,\n",
    "    n_epochs=50,\n",
    "    criterion=\"bic\", # None or \"bic\" # BIC is more conservative than AIC\n",
    "    loss_function='binary_cross_entropy',\n",
    "    max_complexity=10,\n",
    "    sample_weights=sample_weights\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c716614",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "09302bb8",
   "metadata": {},
   "source": [
    "## model performance based on ROC curve on the development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b325af",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_roc_curve(devset_imputed, threshold=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5ffee8b",
   "metadata": {},
   "source": [
    "## model performance based on ROC curve on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de25109",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_roc_curve(testset_imputed, threshold=0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29c06cd6",
   "metadata": {},
   "source": [
    "## associations of the model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_signal(devset_imputed,corr_func='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_signal(testset_imputed,corr_func='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b18083",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_signal(devset_imputed,corr_func='mutual_information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d29e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_signal(testset_imputed,corr_func='mutual_information')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1b404a2",
   "metadata": {},
   "source": [
    "## model performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f89323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get predictions for the dev set\n",
    "predictions = best_model.predict(testset_imputed)\n",
    "predictions_class = [True if x >= 0.5 else False for x in predictions]\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_matrix = confusion_matrix(testset_imputed[outcome_var], predictions_class)\n",
    "\n",
    "# Extract true positive, true negative, false positive, and false negative counts\n",
    "tn, fp, fn, tp = confusion_matrix.ravel()\n",
    "\n",
    "# Calculate PPV, NPV, sensitivity, and specificity\n",
    "PPV_test = tp / (tp + fp)\n",
    "NPV_test = tn / (tn + fn)\n",
    "sensitivity_test = tp / (tp + fn)\n",
    "specificity_test = tn / (tn + fp)\n",
    "balanced_accuracy_test = (sensitivity_test + specificity_test) / 2\n",
    "MCC_test = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "auc = roc_auc_score(testset_imputed[outcome_var], predictions)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC',\"AUC\"],\n",
    "    'Value': [PPV_test, NPV_test, sensitivity_test, specificity_test, balanced_accuracy_test, MCC_test,auc]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "\n",
    "print(results_df)\n",
    "results_df_QLattice = results_df.copy()\n",
    "# Save the results to an Excel file\n",
    "results_df.to_excel('QLattice_results_test.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e8ffc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the weight for each sample based on the number of samples per patient\n",
    "patient_sample_weights = testset_ID.value_counts().apply(lambda x: 1 / x)\n",
    "\n",
    "# Apply the weights to the performance metrics calculations\n",
    "weighted_ppv = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_npv = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_sensitivity = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_specificity = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_balanced_accuracy = (weighted_sensitivity + weighted_specificity) / 2\n",
    "weighted_mcc = np.sum((tp * tn - fp * fn) * patient_sample_weights) / np.sum(((tp * patient_sample_weights + fp * patient_sample_weights) * (tp * patient_sample_weights + fn * patient_sample_weights) * (tn * patient_sample_weights + fp * patient_sample_weights) * (tn * patient_sample_weights + fn * patient_sample_weights)) ** 0.5)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['Weighted PPV', 'Weighted NPV', 'Weighted Sensitivity', 'Weighted Specificity', 'Weighted Balanced Accuracy', 'Weighted MCC'],\n",
    "    'Value': [weighted_ppv, weighted_npv, weighted_sensitivity, weighted_specificity, weighted_balanced_accuracy, weighted_mcc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_QLattice_weighted = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4666add3",
   "metadata": {},
   "source": [
    "## an overview of the model as a block diagram as well as model performance on the development vs test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f593bd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model.plot(devset_imputed, testset_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7aaef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_feats_list = best_model.features\n",
    "sel_feats_list.append(outcome_var)\n",
    "print(sel_feats_list)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "35afd78c",
   "metadata": {},
   "source": [
    "### distribution of model predicted probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f763ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "best_model.plot_probability_scores(testset_imputed)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e7d58f98",
   "metadata": {},
   "source": [
    "### model representation as a closed-form expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy_model = best_model.sympify(symbolic_lr=True, include_weights=True)\n",
    "\n",
    "sympy_model.as_expr()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb463841",
   "metadata": {},
   "source": [
    "### alternative models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "# Combine X_train_imputed and X_test_imputed into a single DataFrame\n",
    "combined = pd.concat([X_train_imputed, X_test_imputed], axis=0)\n",
    "\n",
    "# Loop through the columns of the combined DataFrame and check their data types\n",
    "for col in combined.columns:\n",
    "    if combined[col].dtype == 'object' or combined[col].dtype.name == 'category':\n",
    "        # Perform one-hot encoding on the column\n",
    "        combined = pd.get_dummies(combined, columns=[col], prefix=[col], drop_first=True)\n",
    "\n",
    "# Split the combined DataFrame back into X_train_OHE and X_test_OHE\n",
    "X_train_OHE = combined[:len(X_train_imputed)]\n",
    "X_test_OHE = combined[len(X_train_imputed):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf279ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_OHE.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b747ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_OHE.isnull().sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6d253dd4",
   "metadata": {},
   "source": [
    "### stratified 5-fold cross validation \n",
    "\n",
    "Here we do cross validation to see how the model may perform on the test set.\n",
    "This is done for each of the laternative models that is Random Forest, LightGBM, CATBoost, and Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dd063491",
   "metadata": {},
   "source": [
    "#### Balanced Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846949e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Define the number of folds for stratified k-fold\n",
    "n_splits = 5\n",
    "\n",
    "random_state = SEED\n",
    "sampling_strategy = 'not majority'\n",
    "replacement = True\n",
    "n_estimators = 100  # Modify the number of trees in the forest\n",
    "max_depth = 10      # Modify the maximum depth of each tree\n",
    "min_samples_split = 2  # Modify the minimum number of samples required to split a node\n",
    "min_samples_leaf = 1   # Modify the minimum number of samples required at each leaf node\n",
    "criterion = \"gini\"\n",
    "# Define the metrics to evaluate\n",
    "metrics = ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'AUC']\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "results = {metric: [] for metric in metrics}\n",
    "\n",
    "# Initialize stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "\n",
    "fold_results_df_fold_table_BRF = pd.DataFrame()\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_train_OHE, y_train), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Get the training and test data indices for this fold\n",
    "    X_train_indices, X_test_indices = train_index.tolist(), test_index.tolist()\n",
    "\n",
    "        # Get the training and test data for this fold\n",
    "    X_train_fold = X_train_OHE.iloc[X_train_indices, :]\n",
    "    X_test_fold = X_train_OHE.iloc[X_test_indices, :]\n",
    "    y_train_fold = y_train.iloc[X_train_indices]\n",
    "    y_test_fold = y_train.iloc[X_test_indices]\n",
    "    sample_weights_fold = sample_weights[X_train_indices]\n",
    "\n",
    "    # Train random forest\n",
    "    brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state,\n",
    "    sampling_strategy=sampling_strategy,\n",
    "    replacement=replacement,\n",
    "    criterion = criterion\n",
    "    )\n",
    "    brf.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "    # Get predictions\n",
    "    brf_predictions = brf.predict_proba(X_test_fold)\n",
    "    brf_predictions = brf_predictions[:, 1]\n",
    "    brf_predictions_class = np.where(brf_predictions >= 0.5, True, False)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test_fold, brf_predictions_class)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    PPV = tp / (tp + fp)\n",
    "    NPV = tn / (tn + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    auc = roc_auc_score(y_test_fold, brf_predictions)\n",
    "\n",
    "    # Store the results for this fold\n",
    "    fold_results = [PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, auc]\n",
    "    for metric, value in zip(metrics, fold_results):\n",
    "        results[metric].append(value)\n",
    "\n",
    "    # Print the results for this fold\n",
    "    fold_results_df = pd.DataFrame(fold_results, index=metrics, columns=[fold])\n",
    "    fold_results_df = fold_results_df.round(2)\n",
    "    print(fold_results_df.to_string())\n",
    "    fold_results_df_fold_table_BRF = fold_results_df_fold_table_BRF.append(fold_results_df.T)\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate the average and standard deviation across folds\n",
    "results_df = pd.DataFrame(results)\n",
    "aggregated_results = results_df.mean().round(2)\n",
    "aggregated_results_sd = results_df.std().round(2)\n",
    "ggregated_results_formatted_BRF = aggregated_results.astype(str) + \" ± \" + aggregated_results_sd.astype(str)\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Aggregated Results:\")\n",
    "print(ggregated_results_formatted_BRF.to_string())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92deaad7",
   "metadata": {},
   "source": [
    "#### Histogram-based Gradient Boosting Classification Tree "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85ffc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "# Define the number of folds for stratified k-fold\n",
    "n_splits = 5\n",
    "\n",
    "# Define the metrics to evaluate\n",
    "metrics = ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'AUC']\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "results = {metric: [] for metric in metrics}\n",
    "\n",
    "# Initialize stratified k-fold\n",
    "skf = StratifiedKFold(n_splits=n_splits, random_state=SEED, shuffle=True)\n",
    "\n",
    "fold_results_df_fold_table_HGBC = pd.DataFrame()\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_train_OHE, y_train), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Get the training and test data indices for this fold\n",
    "    X_train_indices, X_test_indices = train_index.tolist(), test_index.tolist()\n",
    "\n",
    "        # Get the training and test data for this fold\n",
    "    X_train_fold = X_train_OHE.iloc[X_train_indices, :]\n",
    "    X_test_fold = X_train_OHE.iloc[X_test_indices, :]\n",
    "    y_train_fold = y_train.iloc[X_train_indices]\n",
    "    y_test_fold = y_train.iloc[X_test_indices]\n",
    "    sample_weights_fold = sample_weights[X_train_indices]\n",
    "\n",
    "    # Train random forest\n",
    "    HGBC = HistGradientBoostingClassifier(random_state=SEED)\n",
    "    HGBC.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "    # Get predictions\n",
    "    HGBC_predictions = HGBC.predict_proba(X_test_fold)\n",
    "    HGBC_predictions = HGBC_predictions[:, 1]\n",
    "    HGBC_predictions_class = np.where(HGBC_predictions >= 0.5, True, False)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test_fold, HGBC_predictions_class)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    PPV = tp / (tp + fp)\n",
    "    NPV = tn / (tn + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    auc = roc_auc_score(y_test_fold, HGBC_predictions)\n",
    "\n",
    "    # Store the results for this fold\n",
    "    fold_results = [PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, auc]\n",
    "    for metric, value in zip(metrics, fold_results):\n",
    "        results[metric].append(value)\n",
    "\n",
    "    # Print the results for this fold\n",
    "    fold_results_df = pd.DataFrame(fold_results, index=metrics, columns=[fold])\n",
    "    fold_results_df = fold_results_df.round(2)\n",
    "    print(fold_results_df.to_string())\n",
    "    fold_results_df_fold_table_HGBC = fold_results_df_fold_table_HGBC.append(fold_results_df.T)\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate the average and standard deviation across folds\n",
    "results_df = pd.DataFrame(results)\n",
    "aggregated_results = results_df.mean().round(2)\n",
    "aggregated_results_sd = results_df.std().round(2)\n",
    "aggregated_results_formatted_HGBC = aggregated_results.astype(str) + \" ± \" + aggregated_results_sd.astype(str)\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Aggregated Results:\")\n",
    "print(aggregated_results_formatted_HGBC.to_string())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58826392",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e36829",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "# Define the list of metrics\n",
    "metrics = ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'AUC']\n",
    "\n",
    "# Create an empty dictionary to store the results\n",
    "results = {metric: [] for metric in metrics}\n",
    "\n",
    "fold_results_df_fold_table_LGBM = pd.DataFrame()\n",
    "\n",
    "# Perform stratified k-fold cross-validation\n",
    "skf = StratifiedKFold(n_splits=5, random_state=SEED, shuffle=True)\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_train_withmissing, y_train), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Get the training and test data indices for this fold\n",
    "    X_train_indices, X_test_indices = train_index.tolist(), test_index.tolist()\n",
    "    # Get the training and test data for this fold\n",
    "    X_train_fold = X_train_withmissing.iloc[X_train_indices, :]\n",
    "    X_test_fold = X_train_withmissing.iloc[X_test_indices, :]\n",
    "    y_train_fold = y_train.iloc[X_train_indices]\n",
    "    y_test_fold = y_train.iloc[X_test_indices]\n",
    "    sample_weights_fold = sample_weights[X_train_indices]\n",
    "    \n",
    "    # Train light gbm\n",
    "    lgbm = lgb.LGBMClassifier(random_state=SEED)\n",
    "    lgbm.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "    # Get predictions\n",
    "    lgbm_predictions = lgbm.predict_proba(X_test_fold)\n",
    "    lgbm_predictions = lgbm_predictions[:, 1]\n",
    "    lgbm_predictions_class = np.where(lgbm_predictions >= 0.5, True, False)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test_fold, lgbm_predictions_class)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    PPV = tp / (tp + fp)\n",
    "    NPV = tn / (tn + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    auc = roc_auc_score(y_test_fold, lgbm_predictions)\n",
    "\n",
    "    # Store the results for this fold\n",
    "    fold_results = [PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, auc]\n",
    "    for metric, value in zip(metrics, fold_results):\n",
    "        results[metric].append(value)\n",
    "\n",
    "    # Print the results for this fold\n",
    "    fold_results_df = pd.DataFrame(fold_results, index=metrics, columns=[fold])\n",
    "    fold_results_df = fold_results_df.round(2)\n",
    "    print(fold_results_df.to_string())\n",
    "    \n",
    "    # Append the results for this fold to fold_results_df_fold_table_LGBM\n",
    "    fold_results_df_fold_table_LGBM = fold_results_df_fold_table_LGBM.append(fold_results_df.T)\n",
    "    \n",
    "    print(\"\")\n",
    "\n",
    "# Calculate the average and standard deviation across folds\n",
    "results_df = pd.DataFrame(results)\n",
    "aggregated_results = results_df.mean().round(2)\n",
    "aggregated_results_sd = results_df.std().round(2)\n",
    "aggregated_results_formatted_LGBM = aggregated_results.astype(str) + \" ± \" + aggregated_results_sd.astype(str)\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Aggregated Results:\")\n",
    "print(aggregated_results_formatted_LGBM.to_string())\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc0b6711",
   "metadata": {},
   "source": [
    "#### CATBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e227131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the results\n",
    "results = {metric: [] for metric in metrics}\n",
    "\n",
    "fold_results_df_fold_table_CB = pd.DataFrame()\n",
    "# Perform stratified k-fold cross-validation\n",
    "for fold, (train_index, test_index) in enumerate(skf.split(X_train_withmissing, y_train), 1):\n",
    "    print(f\"Fold {fold}:\")\n",
    "\n",
    "    # Get the training and test data indices for this fold\n",
    "    X_train_indices, X_test_indices = train_index.tolist(), test_index.tolist()\n",
    "\n",
    "    # Reset the index of y_train\n",
    "    y_train_reset_index = y_train.reset_index(drop=True)\n",
    "\n",
    "        # Get the training and test data for this fold\n",
    "    X_train_fold = X_train_withmissing.iloc[X_train_indices, :]\n",
    "    X_test_fold = X_train_withmissing.iloc[X_test_indices, :]\n",
    "    y_train_fold = y_train.iloc[X_train_indices]\n",
    "    y_test_fold = y_train.iloc[X_test_indices]\n",
    "    sample_weights_fold = sample_weights[X_train_indices]\n",
    "\n",
    "    # Train catboost\n",
    "    catb = cb.CatBoostClassifier(random_state=SEED, cat_features=cat_features,iterations=500, verbose=False)\n",
    "    catb.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "    # Get predictions\n",
    "    catb_predictions = catb.predict_proba(X_test_fold)\n",
    "    catb_predictions = catb_predictions[:, 1]\n",
    "    catb_predictions_class = np.where(catb_predictions >= 0.5, True, False)\n",
    "\n",
    "    # Calculate the confusion matrix\n",
    "    cm = confusion_matrix(y_test_fold, catb_predictions_class)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "    # Calculate metrics\n",
    "    PPV = tp / (tp + fp)\n",
    "    NPV = tn / (tn + fn)\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "    MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "    auc = roc_auc_score(y_test_fold, catb_predictions)\n",
    "\n",
    "    # Store the results for this fold\n",
    "    fold_results = [PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, auc]\n",
    "    for metric, value in zip(metrics, fold_results):\n",
    "        results[metric].append(value)\n",
    "\n",
    "    # Print the results for this fold\n",
    "    fold_results_df = pd.DataFrame(fold_results, index=metrics, columns=[fold])\n",
    "    fold_results_df = fold_results_df.round(2)\n",
    "    print(fold_results_df.to_string())\n",
    "    # Append the results\n",
    "    fold_results_df_fold_table_CB = fold_results_df_fold_table_CB.append(fold_results_df.T)\n",
    "    print(\"\")\n",
    "\n",
    "# Calculate the average and standard deviation across folds\n",
    "results_df = pd.DataFrame(results)\n",
    "aggregated_results = results_df.mean().round(2)\n",
    "aggregated_results_sd = results_df.std().round(2)\n",
    "aggregated_results_formatted_CB = aggregated_results.astype(str) + \" ± \" + aggregated_results_sd.astype(str)\n",
    "\n",
    "# Print the aggregated results\n",
    "print(\"Aggregated Results:\")\n",
    "print(aggregated_results_formatted_CB.to_string())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa932ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_results_formatted_all = pd.concat([aggregated_results_formatted_QLattice,\n",
    "                                              ggregated_results_formatted_BRF,\n",
    "                                             aggregated_results_formatted_HGBC,\n",
    "                                             aggregated_results_formatted_LGBM,\n",
    "                                             aggregated_results_formatted_CB],\n",
    "                                             axis=1)\n",
    "aggregated_results_formatted_all.columns = [\"QLattice\",\"BRF\",\"HGBC\",\"LGBM\",\"CB\"]\n",
    "aggregated_results_formatted_all\n",
    "print(aggregated_results_formatted_all)\n",
    "# Save the results to an Excel file\n",
    "aggregated_results_formatted_all.to_excel('aggregated_results_formatted_all.xlsx', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c80c43c7",
   "metadata": {},
   "source": [
    "### statistical test to compare the performance of the models on cross validation\n",
    "\n",
    "Using the Kruskal-Wallis test allows you to compare the mean AUC values of multiple models without relying on the assumptions of normality and homogeneity of variances. It provides a robust nonparametric approach to assess whether there are significant differences between the models in terms of their performance.\n",
    "\n",
    "The Kruskal-Wallis test is a nonparametric equivalent of the ANOVA test and is suitable when the assumptions of normality and homogeneity of variances are not met.\n",
    "\n",
    "Here's an outline of the steps to perform a Kruskal-Wallis test:\n",
    "\n",
    "Null Hypothesis (H0): The mean AUC values of all models are equal.\n",
    "Alternative Hypothesis (HA): At least one mean AUC value is significantly different from the others.\n",
    "\n",
    "Collect the mean AUC values of each model obtained from cross-validation.\n",
    "\n",
    "Perform a Kruskal-Wallis test, which tests for differences in the distribution of a continuous variable (AUC) among multiple groups (models).\n",
    "\n",
    "Calculate the test statistic (H-statistic) and obtain the corresponding p-value.\n",
    "\n",
    "Interpret the results:\n",
    "\n",
    "If the p-value is less than a predetermined significance level (e.g., 0.05), reject the null hypothesis. It suggests that at least one model's mean AUC value is significantly different from the others.\n",
    "If the p-value is greater than the significance level, fail to reject the null hypothesis. It indicates that there is no significant difference between the mean AUC values of the models.\n",
    "If the null hypothesis is rejected (i.e., significant differences exist), you can perform post-hoc tests to determine which specific models are significantly different from each other. Common post-hoc tests for nonparametric data include the Dunn test or the Bonferroni correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "420e219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "# Extract AUC values from fold_results_df_fold_table_QLattice\n",
    "model1_auc = fold_results_df_fold_table_LGBM['AUC'].values\n",
    "model2_auc = fold_results_df_fold_table_CB['AUC'].values\n",
    "model3_auc = fold_results_df_fold_table_HGBC['AUC'].values\n",
    "model4_auc = fold_results_df_fold_table_QLattice['AUC'].values\n",
    "model5_auc = fold_results_df_fold_table_BRF['AUC'].values\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "statistic, p_value = kruskal(model1_auc, model2_auc, model3_auc, model4_auc, model5_auc)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"At least one model's mean AUC value is significantly different from the others.\")\n",
    "else:\n",
    "    print(\"No significant difference between the mean AUC values of the models.\")\n",
    "\n",
    "print(f\"Kruskal-Wallis test statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "794458e0",
   "metadata": {},
   "source": [
    "### Model training using the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6486f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train random forest\n",
    "# Define your desired hyperparameters\n",
    "random_state = SEED\n",
    "sampling_strategy = 'not majority'\n",
    "replacement = True\n",
    "n_estimators = 100  # Modify the number of trees in the forest\n",
    "max_depth = 10      # Modify the maximum depth of each tree\n",
    "min_samples_split = 2  # Modify the minimum number of samples required to split a node\n",
    "min_samples_leaf = 1   # Modify the minimum number of samples required at each leaf node\n",
    "criterion = \"gini\"\n",
    "\n",
    "brf = BalancedRandomForestClassifier(\n",
    "    n_estimators=n_estimators,\n",
    "    max_depth=max_depth,\n",
    "    min_samples_split=min_samples_split,\n",
    "    min_samples_leaf=min_samples_leaf,\n",
    "    random_state=random_state,\n",
    "    sampling_strategy=sampling_strategy,\n",
    "    replacement=replacement,\n",
    "    criterion = criterion\n",
    ")\n",
    "\n",
    "# Fit the classifier with the modified hyperparameters\n",
    "brf.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# train Histogram-based Gradient Boosting Classification Tree \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "HGBC = HistGradientBoostingClassifier(random_state=SEED)\n",
    "HGBC.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Train light gbm\n",
    "lgbm = lgb.LGBMClassifier(random_state=SEED)\n",
    "lgbm.fit(X_train_withmissing, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Train catboost\n",
    "catb = cb.CatBoostClassifier(random_state=SEED, cat_features=cat_features,iterations=500, verbose=False)\n",
    "catb.fit(X_train_withmissing, y_train, sample_weight=sample_weights)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "19e04b16",
   "metadata": {},
   "source": [
    "### evaluate alternative models on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b55501e",
   "metadata": {},
   "source": [
    "### BRF: balanced random forest classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122313ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRF: balanced random forest classifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Get predictions\n",
    "predictions = brf.predict_proba(X_test_OHE)\n",
    "predictions = predictions[:, 1]\n",
    "# print(predictions_class)\n",
    "predictions_class = [True if x >= 0.5 else False for x in predictions]\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_class)\n",
    "\n",
    "# Extract true positive, true negative, false positive, and false negative counts\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate PPV, NPV, sensitivity, and specificity\n",
    "PPV_test = tp / (tp + fp)\n",
    "NPV_test = tn / (tn + fn)\n",
    "sensitivity_test = tp / (tp + fn)\n",
    "specificity_test = tn / (tn + fp)\n",
    "balanced_accuracy_test = (sensitivity_test + specificity_test) / 2\n",
    "MCC_test = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC',\"AUC\"],\n",
    "    'Value': [PPV_test, NPV_test, sensitivity_test, specificity_test, balanced_accuracy_test, MCC_test,auc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_BRF = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(9, 4.5))\n",
    "\n",
    "# Plot ROC curve\n",
    "ax1.plot(fpr, tpr, color='blue', label='AUC = %0.2f' % roc_auc)\n",
    "ax1.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC curve')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "ax2.set_title('Confusion matrix')\n",
    "disp = ConfusionMatrixDisplay.from_estimator(HGBC, X_test_OHE, y_test, cmap = 'Blues', ax = ax2)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('ROC_CM_BRF.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04e94a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the weight for each sample based on the number of samples per patient\n",
    "patient_sample_weights = testset_ID.value_counts().apply(lambda x: 1 / x)\n",
    "\n",
    "# Apply the weights to the performance metrics calculations\n",
    "weighted_ppv = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_npv = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_sensitivity = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_specificity = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_balanced_accuracy = (weighted_sensitivity + weighted_specificity) / 2\n",
    "weighted_mcc = np.sum((tp * tn - fp * fn) * patient_sample_weights) / np.sum(((tp * patient_sample_weights + fp * patient_sample_weights) * (tp * patient_sample_weights + fn * patient_sample_weights) * (tn * patient_sample_weights + fp * patient_sample_weights) * (tn * patient_sample_weights + fn * patient_sample_weights)) ** 0.5)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['Weighted PPV', 'Weighted NPV', 'Weighted Sensitivity', 'Weighted Specificity', 'Weighted Balanced Accuracy', 'Weighted MCC'],\n",
    "    'Value': [weighted_ppv, weighted_npv, weighted_sensitivity, weighted_specificity, weighted_balanced_accuracy, weighted_mcc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_BRF_weighted = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb022f",
   "metadata": {},
   "source": [
    "### HGBC: Histogram-based Gradient Boosting Classification Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb63568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HGBC: Histogram-based Gradient Boosting Classification Tree\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "\n",
    "# Get predictions\n",
    "predictions = HGBC.predict_proba(X_test_OHE)\n",
    "predictions = predictions[:, 1]\n",
    "# print(predictions_class)\n",
    "predictions_class = [True if x >= 0.5 else False for x in predictions]\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_class)\n",
    "\n",
    "# Extract true positive, true negative, false positive, and false negative counts\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate PPV, NPV, sensitivity, and specificity\n",
    "PPV_test = tp / (tp + fp)\n",
    "NPV_test = tn / (tn + fn)\n",
    "sensitivity_test = tp / (tp + fn)\n",
    "specificity_test = tn / (tn + fp)\n",
    "balanced_accuracy_test = (sensitivity_test + specificity_test) / 2\n",
    "MCC_test = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC',\"AUC\"],\n",
    "    'Value': [PPV_test, NPV_test, sensitivity_test, specificity_test, balanced_accuracy_test, MCC_test,auc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_HGBC = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c388b869",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Get predictions for the dev set\n",
    "predictions = HGBC.predict_proba(X_test_OHE)\n",
    "predictions = predictions[:, 1]\n",
    "# print(predictions_class)\n",
    "predictions_class = [True if x >= 0.5 else False for x in predictions]\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_class)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate the weight for each sample based on the number of samples per patient\n",
    "patient_sample_weights = testset_ID.value_counts().apply(lambda x: 1 / x)\n",
    "\n",
    "# Apply the weights to the performance metrics calculations\n",
    "weighted_ppv = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_npv = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_sensitivity = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_specificity = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_balanced_accuracy = (weighted_sensitivity + weighted_specificity) / 2\n",
    "weighted_mcc = np.sum((tp * tn - fp * fn) * patient_sample_weights) / np.sum(((tp * patient_sample_weights + fp * patient_sample_weights) * (tp * patient_sample_weights + fn * patient_sample_weights) * (tn * patient_sample_weights + fp * patient_sample_weights) * (tn * patient_sample_weights + fn * patient_sample_weights)) ** 0.5)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['Weighted PPV', 'Weighted NPV', 'Weighted Sensitivity', 'Weighted Specificity', 'Weighted Balanced Accuracy', 'Weighted MCC'],\n",
    "    'Value': [weighted_ppv, weighted_npv, weighted_sensitivity, weighted_specificity, weighted_balanced_accuracy, weighted_mcc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_HGBC_weighted = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56347be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(9, 4.5))\n",
    "\n",
    "# Plot ROC curve\n",
    "ax1.plot(fpr, tpr, color='blue', label='AUC = %0.2f' % roc_auc)\n",
    "ax1.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC curve')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "ax2.set_title('Confusion matrix')\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_true=y_test,y_pred=predictions_class, cmap = 'Blues', ax = ax2)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('ROC_CM_HGBC.png', dpi=300)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e950b0",
   "metadata": {},
   "source": [
    "### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23ed407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# light GBM\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Get predictions for the dev set\n",
    "predictions = lgbm.predict_proba(X_test_withmissing)\n",
    "predictions = predictions[:, 1]\n",
    "# print(predictions_class)\n",
    "predictions_class = [True if x >= 0.5 else False for x in predictions]\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_class)\n",
    "\n",
    "# Extract true positive, true negative, false positive, and false negative counts\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate PPV, NPV, sensitivity, and specificity\n",
    "PPV_test = tp / (tp + fp)\n",
    "NPV_test = tn / (tn + fn)\n",
    "sensitivity_test = tp / (tp + fn)\n",
    "specificity_test = tn / (tn + fp)\n",
    "balanced_accuracy_test = (sensitivity_test + specificity_test) / 2\n",
    "MCC_test = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC',\"AUC\"],\n",
    "    'Value': [PPV_test, NPV_test, sensitivity_test, specificity_test, balanced_accuracy_test, MCC_test,auc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_LGBM = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the weight for each sample based on the number of samples per patient\n",
    "patient_sample_weights = testset_ID.value_counts().apply(lambda x: 1 / x)\n",
    "\n",
    "# Apply the weights to the performance metrics calculations\n",
    "weighted_ppv = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_npv = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_sensitivity = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_specificity = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_balanced_accuracy = (weighted_sensitivity + weighted_specificity) / 2\n",
    "weighted_mcc = np.sum((tp * tn - fp * fn) * patient_sample_weights) / np.sum(((tp * patient_sample_weights + fp * patient_sample_weights) * (tp * patient_sample_weights + fn * patient_sample_weights) * (tn * patient_sample_weights + fp * patient_sample_weights) * (tn * patient_sample_weights + fn * patient_sample_weights)) ** 0.5)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['Weighted PPV', 'Weighted NPV', 'Weighted Sensitivity', 'Weighted Specificity', 'Weighted Balanced Accuracy', 'Weighted MCC'],\n",
    "    'Value': [weighted_ppv, weighted_npv, weighted_sensitivity, weighted_specificity, weighted_balanced_accuracy, weighted_mcc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_LGBM_weighted = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc02a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(9, 4.5))\n",
    "\n",
    "# Plot ROC curve\n",
    "ax1.plot(fpr, tpr, color='blue', label='AUC = %0.2f' % roc_auc)\n",
    "ax1.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC curve')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "ax2.set_title('Confusion matrix')\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_true=y_test,y_pred=predictions_class, cmap = 'Blues', ax = ax2)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('ROC_CM_LGBM.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd8257f",
   "metadata": {},
   "source": [
    "### CATBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbf358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATBOOST\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "# Get predictions for the dev set\n",
    "predictions = catb.predict_proba(X_test_withmissing)\n",
    "predictions = predictions[:, 1]\n",
    "# print(predictions_class)\n",
    "predictions_class = [True if x >= 0.5 else False for x in predictions]\n",
    "# predictions_class = predictions_class == \"True\"\n",
    "# print(predictions_class)\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, predictions_class)\n",
    "\n",
    "# Extract true positive, true negative, false positive, and false negative counts\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculate PPV, NPV, sensitivity, and specificity\n",
    "PPV_test = tp / (tp + fp)\n",
    "NPV_test = tn / (tn + fn)\n",
    "sensitivity_test = tp / (tp + fn)\n",
    "specificity_test = tn / (tn + fp)\n",
    "balanced_accuracy_test = (sensitivity_test + specificity_test) / 2\n",
    "MCC_test = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "auc = roc_auc_score(y_test, predictions)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC',\"AUC\"],\n",
    "    'Value': [PPV_test, NPV_test, sensitivity_test, specificity_test, balanced_accuracy_test, MCC_test,auc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_CB = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353d4a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the weight for each sample based on the number of samples per patient\n",
    "patient_sample_weights = testset_ID.value_counts().apply(lambda x: 1 / x)\n",
    "\n",
    "# Apply the weights to the performance metrics calculations\n",
    "weighted_ppv = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_npv = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_sensitivity = np.sum(tp * patient_sample_weights) / np.sum(tp * patient_sample_weights + fn * patient_sample_weights)\n",
    "weighted_specificity = np.sum(tn * patient_sample_weights) / np.sum(tn * patient_sample_weights + fp * patient_sample_weights)\n",
    "weighted_balanced_accuracy = (weighted_sensitivity + weighted_specificity) / 2\n",
    "weighted_mcc = np.sum((tp * tn - fp * fn) * patient_sample_weights) / np.sum(((tp * patient_sample_weights + fp * patient_sample_weights) * (tp * patient_sample_weights + fn * patient_sample_weights) * (tn * patient_sample_weights + fp * patient_sample_weights) * (tn * patient_sample_weights + fn * patient_sample_weights)) ** 0.5)\n",
    "\n",
    "# Create a dictionary to hold the results\n",
    "results = {\n",
    "    'Metric': ['Weighted PPV', 'Weighted NPV', 'Weighted Sensitivity', 'Weighted Specificity', 'Weighted Balanced Accuracy', 'Weighted MCC'],\n",
    "    'Value': [weighted_ppv, weighted_npv, weighted_sensitivity, weighted_specificity, weighted_balanced_accuracy, weighted_mcc]\n",
    "}\n",
    "\n",
    "# Create a DataFrame to display the results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Round the values to two decimal places\n",
    "results_df['Value'] = results_df['Value'].round(2)\n",
    "results_df_CB_weighted = results_df.copy()\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b951518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Calculate ROC curve and AUC\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# Set up the figure and axes\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(9, 4.5))\n",
    "\n",
    "# Plot ROC curve\n",
    "ax1.plot(fpr, tpr, color='blue', label='AUC = %0.2f' % roc_auc)\n",
    "ax1.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "ax1.set_xlim([0, 1])\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('ROC curve')\n",
    "ax1.legend(loc=\"lower right\")\n",
    "\n",
    "# Plot Confusion Matrix\n",
    "ax2.set_title('Confusion matrix')\n",
    "disp = ConfusionMatrixDisplay.from_predictions(y_true=y_test,y_pred=predictions_class, cmap = 'Blues', ax = ax2)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('ROC_CM_CB.png', dpi=300)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "434bd483",
   "metadata": {},
   "source": [
    "### model performance for the best performing model based on the whole data\n",
    "\n",
    "LightGBM is chosen as best alternative model as it had the highest AUC on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a5424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data frames based on the \"Metric\" column\n",
    "merged_df = pd.merge(results_df_QLattice_weighted, results_df_BRF_weighted, on='Metric', suffixes=('_1', '_2'))\n",
    "merged_df = pd.merge(merged_df, results_df_HGBC_weighted, on='Metric')\n",
    "merged_df = pd.merge(merged_df, results_df_LGBM_weighted, on='Metric', suffixes=('_3', '_4'))\n",
    "merged_df = pd.merge(merged_df, results_df_CB_weighted, on='Metric', suffixes=('_4', '_5'))\n",
    "\n",
    "merged_df.columns = [\"Measures\",\"QLattice\",\"BRF\",\"HGBC\",\"LGBM\",\"CB\"]\n",
    "aggregated_results_test_all_weighted = merged_df.copy()\n",
    "\n",
    "print(aggregated_results_test_all_weighted)\n",
    "# Save the results to an Excel file\n",
    "aggregated_results_test_all_weighted.to_excel('aggregated_results_test_all_weighted.xlsx', index=False)\n",
    "del merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a0edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data frames based on the \"Metric\" column\n",
    "merged_df = pd.merge(results_df_QLattice, results_df_BRF, on='Metric', suffixes=('_1', '_2'))\n",
    "merged_df = pd.merge(merged_df, results_df_HGBC, on='Metric')\n",
    "merged_df = pd.merge(merged_df, results_df_LGBM, on='Metric', suffixes=('_3', '_4'))\n",
    "merged_df = pd.merge(merged_df, results_df_CB, on='Metric', suffixes=('_4', '_5'))\n",
    "\n",
    "merged_df.columns = [\"Measures\",\"QLattice\",\"BRF\",\"HGBC\",\"LGBM\",\"CB\"]\n",
    "aggregated_results_test_all = merged_df.copy()\n",
    "\n",
    "print(aggregated_results_test_all)\n",
    "# Save the results to an Excel file\n",
    "aggregated_results_test_all.to_excel('aggregated_results_test_all.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5651380c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_model = lgbm\n",
    "\n",
    "# selected_model = HGBC\n",
    "data = aggregated_results_test_all.copy()\n",
    "\n",
    "# Get the average value for the numerical columns\n",
    "average_values = data.iloc[5:, 2:].mean(axis=0) # QLattice excluded from comparison\n",
    "\n",
    "# Get the name of the one with the highest average\n",
    "highest_average = average_values.idxmax()\n",
    "\n",
    "model_dictionary = {\"BRF\": brf,\n",
    "                    \"HGBC\": HGBC,\n",
    "                    \"LGBM\": lgbm,\n",
    "                    \"CB\": catb\n",
    "}\n",
    "print(\"Selected Model:\", highest_average)\n",
    "selected_model =  model_dictionary[highest_average]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c6947407",
   "metadata": {},
   "source": [
    "### SHAP values association with predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e792cba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Calculate SHAP values for the positive class\n",
    "positive_class_index = 1  # Adjust this index based on the class labels of your problem\n",
    "\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier)):\n",
    "    explainer = shap.TreeExplainer(selected_model)\n",
    "    shap_values = explainer.shap_values(X_test_OHE)\n",
    "else:\n",
    "    explainer = shap.TreeExplainer(selected_model)\n",
    "    shap_values = explainer.shap_values(X_test_withmissing)[positive_class_index]\n",
    "\n",
    "# Calculate the sum of SHAP values for each sample\n",
    "shap_sum = shap_values.sum(axis=1)\n",
    "\n",
    "# Get the predicted probabilities of the model\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier)):\n",
    "    predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "else:\n",
    "    predicted_probabilities = selected_model.predict_proba(X_test_withmissing)[:, positive_class_index]\n",
    "\n",
    "    \n",
    "# Plot the SHAP sum against the predicted probabilities\n",
    "plt.scatter(shap_sum, predicted_probabilities)\n",
    "plt.xlabel('Sum of SHAP values')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.title('Sum of SHAP Values vs. Predicted Probability')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "79b1cc40",
   "metadata": {},
   "source": [
    "### interpret the model based on SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Calculate the absolute SHAP values\n",
    "abs_shap_values = np.abs(shap_values)\n",
    "\n",
    "# Compute the feature importance based on the sum of absolute SHAP values\n",
    "feature_importance = np.mean(abs_shap_values, axis=0)\n",
    "\n",
    "# Create a DataFrame to store feature importance\n",
    "# feature_importance_df = pd.DataFrame({'Feature': X_train.columns.tolist(), 'Importance': feature_importance})\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier)):\n",
    "    feature_importance_df = pd.DataFrame({'Feature': [data_dictionary.get(feature, feature) for feature in X_test_OHE.columns.tolist()], 'Importance': feature_importance})\n",
    "else:\n",
    "    feature_importance_df = pd.DataFrame({'Feature': [data_dictionary.get(feature, feature) for feature in X_test_withmissing.columns.tolist()], 'Importance': feature_importance})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Print the top 10 most important features\n",
    "top_10_features = feature_importance_df.head(10)\n",
    "print(top_10_features)\n",
    "# Reverse the order of the sorted data\n",
    "top_10_features = top_10_features[::-1]\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "top_10_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Plot the top 10 most important features\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.barh(top_10_features.index, top_10_features['Importance'])\n",
    "plt.yticks(top_10_features.index, top_10_features['Feature'])\n",
    "\n",
    "# plt.barh(top_10_features['Feature'], top_10_features['Importance'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.rcParams['figure.autolayout'] = True  # Automatically adjust the figure margins\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('feature_importance_shap_plot.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebd12903",
   "metadata": {},
   "source": [
    "### SHAP summary plot\n",
    "\n",
    "Note: the plot cannot show categorical features in color codes and thus they are plotted in grey (not mistaken with missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b724cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib notebook\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Retrieve feature names from the data dictionary\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier)):\n",
    "    feature_names_with_shapvalues = [\n",
    "        data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "        for feature, value in zip(X_test_OHE.columns, np.mean(np.abs(shap_values), axis=0)) # np.abs(shap_values).mean(axis=0)\n",
    "    ]\n",
    "    shap.summary_plot(shap_values, X_test_OHE, feature_names=feature_names_with_shapvalues, show=False, alpha = 0.8, max_display=10)\n",
    "\n",
    "else:\n",
    "        feature_names_with_shapvalues = [\n",
    "        data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "        for feature, value in zip(X_test_withmissing.columns, np.mean(np.abs(shap_values), axis=0)) # np.abs(shap_values).mean(axis=0)\n",
    "    ]\n",
    "        shap.summary_plot(shap_values, X_test_withmissing, feature_names=feature_names_with_shapvalues, show=False, alpha = 0.8, max_display=10)\n",
    "\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('shap_summary_top10_plot.png', dpi=300)\n",
    "plt.rcParams['figure.autolayout'] = True  # Automatically adjust the figure margins\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "7dc069371dbb04ae7c3a551c0b6a0e14ac90d7336ba49c8127b1c2df831cbec8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
