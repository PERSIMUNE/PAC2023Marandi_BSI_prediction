{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec94155f",
   "metadata": {},
   "source": [
    "### About the data\n",
    "\n",
    "mentioned in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3646a",
   "metadata": {},
   "source": [
    "## Load Data, Libraries, and Set Parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399989cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we load dataset and required libraries \n",
    "# Remove Future Warnings\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import uniform, randint\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef, average_precision_score, balanced_accuracy_score, f1_score, confusion_matrix, roc_auc_score, brier_score_loss, precision_recall_curve, roc_curve, auc, matthews_corrcoef, ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, MinMaxScaler\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "from scipy.stats import pointbiserialr, wilcoxon, mannwhitneyu, chi2_contingency, norm, iqr, kruskal, spearmanr, ttest_rel, linregress\n",
    "from joblib import Parallel, delayed\n",
    "from mrmr import mrmr_classif # for feature selection (optional)\n",
    "from imblearn.over_sampling import RandomOverSampler # for oversampling (optional)\n",
    "# loading models (libraries or packages for base models)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import feyn\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sksurv.util import Surv\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.metrics import integrated_brier_score, cumulative_dynamic_auc, concordance_index_censored\n",
    "from sklearn.ensemble import IsolationForest, RandomForestRegressor, HistGradientBoostingClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder, QuantileTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, silhouette_score\n",
    "from joblib import dump\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import joblib\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis\n",
    "from sklearn.semi_supervised import LabelPropagation\n",
    "from survshap import SurvivalModelExplainer, PredictSurvSHAP, ModelSurvSHAP\n",
    "from sksurv.compare import compare_survival\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from matplotlib import cm\n",
    "from matplotlib.table import table\n",
    "from scipy.integrate import trapezoid\n",
    "from sympy import init_printing\n",
    "import itertools\n",
    "from io import StringIO\n",
    "from IPython import get_ipython\n",
    "from IPython.display import display, Javascript\n",
    "import sys\n",
    "import re\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import platform\n",
    "import subprocess\n",
    "import psutil\n",
    "%matplotlib inline\n",
    "\n",
    "# Initialize MinMaxScaler for normalization\n",
    "minmax_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebab3d",
   "metadata": {},
   "source": [
    "### Most important settings\n",
    "\n",
    "Below you can find parameters and configurations to set and complete like an entry form that are critical and important, so please make sure that you have an understanding on the parameters by reading the comments and the documentation of the pipeline (the MANUAL page on GitHub)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5bf2fa8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# settings for categorical variables\n",
    "# specify the names of categorical features (variables) - if no categorical feature leave it empty as []\n",
    "cat_features = [\"Sex\"]\n",
    "merged_rare_categories = False # merge rare categories\n",
    "rarity_threshold = 0.05  # Define a threshold for rarity (e.g., 0.05 means 5%) this is used to merge rare categories in categorical features (optional)\n",
    "\n",
    "###################################################################################\n",
    "# specify columns that must be removed\n",
    "columns_to_drop = ['ID', 'date', 'allbac',\"BSImulticlass\"] \n",
    "###################################################################################\n",
    "# import data\n",
    "# mydata = pd.read_csv(\"Persimune_condensed 20240917.csv\")\n",
    "external_val = False # True if you have a dataset that can be used as an external validation data\n",
    "# load any external data\n",
    "# extval_data = pd.read_excel('external_validation_data.xlsx')\n",
    "ext_val_demo = False # only used to run a demo for external validation - this creates external validation dataset for simulation \n",
    "\n",
    "###################################################################################\n",
    "# random data split\n",
    "data_split = True # True to apply stratified data split by outcome variable (e.g., 80% training or development data and 20% test data) \n",
    "# if data_split = False, all the dataset will be used for cross validation (can be used when there is no enough data to set aside for the test set)\n",
    "train_size_perc = 0.8 # percentage of the samples to be used for training (e.g. 0.8 means 80% of samples for training)\n",
    "# see the following conditions to check if you need to do any custom data split based on your data\n",
    "# below can be used in the case where multiple samples (instances) are available from same patients\n",
    "data_split_by_patients = False # True to apply data split by patient ID (the column name that contains patient ID should then be specified)\n",
    "if data_split_by_patients:\n",
    "    patient_id_col = \"patient_ID\" # the column name that contains patient ID should then be specified (if not patients, it could be any individual identification number for example)\n",
    "data_split_multi_strats = False # True if you need to use more than one variable for stratification\n",
    "if data_split_multi_strats: # the names of the columns used for multiple stratification should be specified by user\n",
    "    strat_var1 = \"pato_hist_test_tum\"\n",
    "already_split = True # indicate if the data is already split to train and test sets\n",
    "if already_split: # specify the names of the train (development) and test sets\n",
    "    # Splitting based on values\n",
    "    mydata = pd.read_csv(\"train_set_ADMper_7_multiclassBSI.csv\")\n",
    "    testset = pd.read_csv(\"test_set_ADMper_7_multiclassBSI.csv\")\n",
    "# so data_split = True is used for MAIT Discovery and Prediciton whereas data_split = False is used for MAIT Discovery pipeline (only cross validation is done)\n",
    "###################################################################################\n",
    "\n",
    "# available binary classification models in the pipeline to use (7 in total) - you can delete the name of any algorithms/models from the list to exclude them from the pipeline\n",
    "models_to_include = [\"QLattice_mdl\", \"NaiveBayes_mdl\", \"RandomForest_mdl\", \"LightGBM_mdl\", \"CatBoost_mdl\", \"LogisticRegression_mdl\", \"HistGBC_mdl\"]\n",
    "# models_to_include = [\"QLattice_mdl\", \"NaiveBayes_mdl\", \"RandomForest_mdl\", \"LightGBM_mdl\", \"CatBoost_mdl\", \"LogisticRegression_mdl\", \"HistGBC_mdl\"]\n",
    "\n",
    "# outcome variable (e.g., class 1 indicated as \"0\" and class 2 as \"1\")\n",
    "outcome_var = \"BSIClass\" # specify the name of the column for the binary outcome variable (note: it should not contain any missingness)\n",
    "\n",
    "###################################################################################\n",
    "# set a directory to save the results\n",
    "main_folder_name = 'results_BSI_27092024'\n",
    "# Define class labels for display\n",
    "class_labels_display = ['no BSI', 'BSI']   # Specify the labels for the two classes to display in figures\n",
    "\n",
    "# Specify the class labels\n",
    "class_0 = class_labels_display[0]\n",
    "class_1 = class_labels_display[1]\n",
    "\n",
    "# Create a mapping dictionary for class labels\n",
    "class_label_dict = {\"noBSI\":class_0, \"BSI\":class_1} # this has to be set by user based on class labels in the outcome variable\n",
    "\n",
    "###################################################################################\n",
    "# feature selection\n",
    "feat_sel = False # feature selection based on minimum Redundancy - Maximum Relevance (mRMR) algorithm\n",
    "num_features_sel = 20 # number of features to select using mRMR algorithm within each fold (common selected features are then used for machine learning). \n",
    "# If there was no common selected features, increase num_features_sel.\n",
    "top_n_f = 20 # number of top features (most impactful features) based on SHAP values to be displayed for SHAP plots\n",
    "\n",
    "###################################################################################\n",
    "# survival analysis\n",
    "# Two models are included: random survival forest (RSF) as main model and Cox proportional hazard (CPH) model as a baseline model to compare against the RSF\n",
    "survival_analysis = False # True to conduct survival analyses. To do this you should provide a backup data that contains a column for time-to-event\n",
    "if survival_analysis:\n",
    "    survival_demo = False # only used to create a column for time to event just to showcase how the results would look like if survival models are used\n",
    "    time_to_event_column = \"T_RFS\" # use the column name for time-to-event in your data\n",
    "    if survival_demo: \n",
    "        # Adding a new column with random integers between 90 to 365 (only for demonstration purpose - not to be used when the data is available)\n",
    "        mydata[time_to_event_column] = randint(90, 366, size=len(mydata))\n",
    "        \n",
    "    mydata_copy_survival = mydata.copy() # get a copy of your data as back up for the time-to-event column\n",
    "    \n",
    "    if external_val:\n",
    "        extval_data_copy = extval_data.copy() # get a copy of extval_data that contains time_to_event_column\n",
    "    \n",
    "    # mydata.drop(columns = [time_to_event_column], inplace = True) # remove the time-to-event column for the data that's going to be used for binary classification\n",
    "\n",
    "###################################################################################\n",
    "# regression analysis\n",
    "# Two models are included: random forest regressor (RFR) as main model and linear regression model as a baseline model to be compared against the RFR\n",
    "regression_analysis = False\n",
    "if regression_analysis:\n",
    "    regression_outcome = \"regression_outcome_var\"\n",
    "    demo_regression_analysis = True # only used for demonstration (simulation) purpose when the data is not available - not to be used otherwise\n",
    "    if demo_regression_analysis:\n",
    "        mydata_copy_regression = mydata.copy()\n",
    "        # Generate random features\n",
    "        np.random.seed(123)\n",
    "        X = np.random.randn(mydata_copy_regression.shape[0], mydata_copy_regression.shape[1])\n",
    "        # Define coefficients for each feature\n",
    "        np.random.seed(123)\n",
    "        true_calculate = np.random.randn(mydata_copy_regression.shape[1])\n",
    "        # Generate outcome variable (target) based on features and calculate\n",
    "        # Adding some noise for randomness\n",
    "        np.random.seed(123)\n",
    "        noise = np.random.randn(mydata_copy_regression.shape[0]) * 0.5  # Adjust the magnitude of noise\n",
    "        mydata_copy_regression[regression_outcome] = np.dot(X, true_calculate) + noise\n",
    "\n",
    "###################################################################################\n",
    "# settings for processing resouces\n",
    "GPU_avail = True # True if GPU is available in your machine otherwise set to False\n",
    "hp_tuning = True # True if you want to conduct hyperparameter tuning otherwise set to False\n",
    "n_cpu_for_tuning = 20 # number of CPUs to be available for hyperparameter tuning\n",
    "n_cpu_model_training = 20 # number of CPUs to be available for model training\n",
    "n_rep_feature_permutation = 100 # number of repetitions for feature permutation\n",
    "n_iter_hptuning = 10 # number of iterations in repeated cross validation for hyperparameter tuning\n",
    "SEED = 123 # arbitrarily chosen, this modifies computer randomization, if there are significant differences between train and test sets due to the random data split, this can be modified for example\n",
    "\n",
    "###################################################################################\n",
    "\n",
    "###################################################################################\n",
    "cv_folds = 5 # number of folds for the outer loop in cross validation\n",
    "cv_folds_hptuning = 5 # number of folds for hyperparameter tuning (inner loop - nested cross validation)\n",
    "use_default_threshold = True # use default threshold of 0.5 bor binary classification, otherwise it optimize the threshold based on the development set\n",
    "test_only_best_cvmodel = True # True to test only the best performing model from cross validation, this option speeds up the process\n",
    "###################################################################################\n",
    "\n",
    "###################################################################################\n",
    "# handle missingness\n",
    "exclude_highly_missing_columns = True # True to exclude features with high missingness\n",
    "exclude_highly_missing_rows = True # True to exclude rows (samples) with high missingness\n",
    "column_threshold = 0.99  # Threshold for variables - columns (e.g., 99% missingness)\n",
    "row_threshold = 0.90     # Threshold for samples - rows (e.g., 90% missingness)\n",
    "\n",
    "###################################################################################\n",
    "remove_outliers = False # True to enable outlier detection and removal using Isolation Forest algorithm\n",
    "###################################################################################\n",
    "# Specify the filename of this Jupyter notebook so that it can be saved after execution\n",
    "JupyterNotebook_filename = \"MAIT_BSI_27092024.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa365923",
   "metadata": {},
   "source": [
    "### Less important settings\n",
    "\n",
    "Here you have configurations that can be set but they are usually fine to be set as it is (default settings)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76d79c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################################\n",
    "# continuous variables\n",
    "specify_continuous_variables = False  # optional but recommended in case there are continuous variables that may have entries that could be recognized as categorical variables\n",
    "continuous_features = []\n",
    "###################################################################################\n",
    "export_missclassified = False\n",
    "if export_missclassified:\n",
    "    mydata_backup = mydata.copy()\n",
    "    mydata_backup['ID'] = mydata[\"record_id\"]\n",
    "    if already_split:\n",
    "        testset_backup = testset.copy()\n",
    "        testset_backup['ID'] = testset[\"record_id\"]\n",
    "        mydata_backup = pd.concat([mydata_backup, testset_backup])\n",
    "    # Generate random IDs in the format \"ID_randomnumber\"\n",
    "    # mydata_backup['ID'] = mydata[\"ID\"]\n",
    "\n",
    "###################################################################################\n",
    "# data manipulation settings\n",
    "oversampling = False # apply oversampling using random oversampling only on train set to increase the number of samples of the minority class\n",
    "scale_data = False # data scale using robust scaling (see scikit-learn)\n",
    "semi_supervised = False # if True it applies a method to impute missingness in the outcome variable using label propagation method otherwise they are excluded\n",
    "\n",
    "###################################################################################\n",
    "# supplementary analyses\n",
    "model_uncertainty_reduction = False # True to use model uncertainty reduction (MUR) approach to build more robust models (filtering samples based on SHAP values and predicted probabilities being close to the chance level)\n",
    "do_decision_curve_analysis = True # True to conduct decision curve analysis\n",
    "model_calibration = True # True to conduct model calibration for the best performing binary classification model (recommended only if a large dataset is available)\n",
    "find_interacting_feature_permutation = False # True to enable the analysis based on feature permutation on the final model to inspect potential interactions among pairs of features\n",
    "\n",
    "###################################################################################\n",
    "# settings for displaying feature names\n",
    "shorten_feature_names = True # True to shorten feature names (for visualization purposes if your data has features with too long names)\n",
    "if shorten_feature_names:\n",
    "    fname_max_length = 30 # Specify max length (number of characters for the feature names to appear)\n",
    "data_dictionary = {}\n",
    "# optional, using the data dictionary the name of the variables are displayed in figures and tables to facilitate reporting.\n",
    "###################################################################################\n",
    "\n",
    "###################################################################################\n",
    "demo_configs = False # True only if using the Breast Cancer dataset to add missingness and a categorical feature to the data for demonstration - not to be used otherwise\n",
    "\n",
    "###################################################################################\n",
    "# following is the default custom metric (Mean of ROCAUC and PRAUC) used for hyperparameter tuning\n",
    "def combined_metric(y_true, y_pred_proba):\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    return (roc_auc + pr_auc) / 2  # Mean of ROCAUC and PRAUC\n",
    "\n",
    "custom_scorer = make_scorer(combined_metric, needs_proba=True)\n",
    "use_single_metric = False # in case you want to use a single metric for training and hyperparameter tuninig\n",
    "if use_single_metric:\n",
    "    single_score = \"ROCAUC\" # choose between ROCAUC and PRAUC\n",
    "    if single_score == \"ROCAUC\":\n",
    "        custom_scorer = make_scorer(roc_auc_score, needs_proba=True)\n",
    "    elif single_score == \"PRAUC\":\n",
    "        custom_scorer = make_scorer(average_precision_score, needs_proba=True)\n",
    "\n",
    "###################################################################################\n",
    "# Check if the main folder exists, and create it if not\n",
    "if not os.path.exists(main_folder_name):\n",
    "    os.makedirs(main_folder_name)\n",
    "\n",
    "# Change the current working directory to the main folder\n",
    "os.chdir(main_folder_name)\n",
    "\n",
    "# reporting file formats\n",
    "fig_file_format = \"tif\" # specify desired file format for figures (e.g. tif, svg)\n",
    "\n",
    "# limit the number of lines to display\n",
    "pd.set_option('display.max_rows', 200)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 200)\n",
    "\n",
    "skip_block = False # should not be changed\n",
    "###################################################################################\n",
    "# Record start time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee37c8d",
   "metadata": {},
   "source": [
    "#### Data preparation\n",
    "\n",
    "The data may require some preparation before feeding them to machine learning models (algorithms).\n",
    "The most important things to check are:\n",
    "data types (feature types: categorical, numerical, etc.)\n",
    "missingness\n",
    "if there are more than one dataset, all variables must have the same definition and type across datasets \n",
    "redundant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ded96de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the outcome variable exists in the dataset\n",
    "if outcome_var in mydata.columns:\n",
    "    # Map the class labels in the outcome variable\n",
    "    mydata[outcome_var] = mydata[outcome_var].map(class_label_dict)\n",
    "    if already_split: \n",
    "        testset[outcome_var] = testset[outcome_var].map(class_label_dict)\n",
    "    if external_val:\n",
    "        extval_data[outcome_var] = extval_data[outcome_var].map(class_label_dict)\n",
    "else:\n",
    "    raise ValueError(f\"'{outcome_var}' column not found in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "776c600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missingness\n",
    "# empty entries replaced by NaN\n",
    "mydata.replace(\" \", np.nan, inplace=True)\n",
    "\n",
    "# If there are lost to follow ups (missing endpoint/event) in the data, one option is to use semi-supervised learning methods to impute missing target labels (outcome variable)\n",
    "# LabelPropagation assigns the labels based on similarity of samples (instances)\n",
    "if semi_supervised:\n",
    "    # Replace NaN values in the outcome variable with -1\n",
    "    mydata[outcome_var] = mydata[outcome_var].replace(np.nan, -1)\n",
    "\n",
    "    # Create a LabelPropagation model\n",
    "    label_prop_model = LabelPropagation()\n",
    "\n",
    "    # Separate the features (X) and labels (y)\n",
    "    X = mydata.drop(columns=[outcome_var])\n",
    "    y = mydata[outcome_var]\n",
    "\n",
    "    # Fit the model on the dataset (X: features, y: labels including -1 for missing labels)\n",
    "    label_prop_model.fit(X, y)\n",
    "\n",
    "    # Get the predicted labels (transduction_ includes predictions for both labeled and unlabeled)\n",
    "    predicted_labels = label_prop_model.transduction_\n",
    "\n",
    "    # Update the original DataFrame with predicted labels where labels were missing (-1)\n",
    "    mydata.loc[mydata[outcome_var] == -1, outcome_var] = predicted_labels[mydata[outcome_var] == -1]\n",
    "\n",
    "else:\n",
    "    # Remove rows where outcome_var column contains NaN (missing) values\n",
    "    mydata = mydata.dropna(subset=[outcome_var])\n",
    "\n",
    "if demo_configs: # used only for the Breast Cancer dataset\n",
    "    # Randomly selecting some indices to set as missing values\n",
    "    np.random.seed(SEED)\n",
    "    rows_to_change = np.random.choice(mydata.index, size=5, replace=False)\n",
    "    np.random.seed(SEED)\n",
    "    cols_to_change = np.random.choice(mydata.columns, size=5, replace=False)\n",
    "    outcome_var_backup = mydata[outcome_var]\n",
    "    # Setting these selected entries to NaN\n",
    "    mydata.loc[rows_to_change, cols_to_change] = np.nan\n",
    "    mydata[outcome_var] = outcome_var_backup\n",
    "\n",
    "    # Adding a column for race with three categories\n",
    "    races = ['White', 'Black', 'Asian']\n",
    "    np.random.seed(SEED)\n",
    "    mydata['Race'] = np.random.choice(races, size=mydata.shape[0])\n",
    "\n",
    "mydata.drop(columns=columns_to_drop, inplace=True)\n",
    "    \n",
    "mydata[cat_features] = mydata[cat_features].astype('category')\n",
    "\n",
    "# Convert categories to strings for each categorical column\n",
    "for col in cat_features:\n",
    "    mydata[col] = mydata[col].astype(str).astype('category')\n",
    "\n",
    "if ext_val_demo:\n",
    "    # Randomly select a few samples from the dataframe\n",
    "    num_samples = 100  # Change this number to select different number of samples\n",
    "    extval_data = mydata.sample(n=num_samples).reset_index(drop=True)\n",
    "    \n",
    "if external_val:\n",
    "    columns_present = [col for col in columns_to_drop if col in extval_data.columns]\n",
    "    if columns_present:\n",
    "        extval_data.drop(columns=columns_present, inplace=True)\n",
    "\n",
    "    extval_data[cat_features] = extval_data[cat_features].astype('category')\n",
    "\n",
    "    # Convert categories to strings for each categorical column\n",
    "    for col in cat_features:\n",
    "        extval_data[col] = extval_data[col].astype(str).astype('category')\n",
    "    extval_data.replace(\" \", np.nan, inplace=True)\n",
    "\n",
    "if already_split:\n",
    "    columns_present = [col for col in columns_to_drop if col in testset.columns]\n",
    "    if columns_present:\n",
    "        testset.drop(columns=columns_present, inplace=True)\n",
    "\n",
    "    testset[cat_features] = testset[cat_features].astype('category')\n",
    "\n",
    "    # Convert categories to strings for each categorical column\n",
    "    for col in cat_features:\n",
    "        testset[col] = testset[col].astype(str).astype('category')\n",
    "    testset.replace(\" \", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d86ea62d",
   "metadata": {},
   "source": [
    "#### Specify data types for numerical features (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "242b2fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if specify_continuous_variables:\n",
    "\n",
    "    # Replace non-numeric values (including empty strings) with NaN\n",
    "    mydata[continuous_features] = mydata[continuous_features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Convert to float64 after replacing non-numeric values with NaN\n",
    "    mydata[continuous_features] = mydata[continuous_features].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b835ce54",
   "metadata": {},
   "source": [
    "#### Defined missingness\n",
    "\n",
    "make sure all missing values are defined. For categorical features, missing values should be encoded as \"missing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "751df11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty entries replaced by NaN (if NaN occurs after the previous code chunk)\n",
    "mydata.replace(\" \", np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f6a61d",
   "metadata": {},
   "source": [
    "#### Rare categories in categorical variables\n",
    "\n",
    "If there are rare categories in categorical features, they can have negativae effect on learning process and thus the following code can handle merging such rare categories. It is however more favorable to be done by the data engineer or the researcher who knows the context of the data to merge rare categories in a meaningful way rather than automated merging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2046702e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sometimes there are rare categories that can be merged in the data\n",
    "if merged_rare_categories:\n",
    "    if external_val:\n",
    "        mydata_backup = mydata.copy()\n",
    "        mydata = pd.concat([mydata,extval_data])\n",
    "    categorical_columns = mydata.select_dtypes(include=['category']).columns\n",
    "    category_frequencies = {}\n",
    "    for col in categorical_columns:\n",
    "        category_counts = mydata[col].value_counts()\n",
    "        category_frequencies[col] = category_counts\n",
    "\n",
    "    print(category_frequencies)\n",
    "\n",
    "    # Categorical columns\n",
    "    categorical_columns = mydata.select_dtypes(include=['category']).columns\n",
    "\n",
    "    # Dictionary to store category frequencies\n",
    "    category_frequencies = {}\n",
    "\n",
    "    # Loop through categorical columns\n",
    "    for col in categorical_columns:\n",
    "        # Calculate category frequencies\n",
    "        category_counts = mydata[col].value_counts()\n",
    "        \n",
    "        # Identify rare categories\n",
    "        rare_categories = category_counts[category_counts / len(mydata) < rarity_threshold].index\n",
    "        \n",
    "        # Group rare categories into a single category and eliminate individual rare categories\n",
    "        grouped_category_name = \"\"\n",
    "        for cat in rare_categories:\n",
    "            grouped_category_name += f\" or {cat}\"\n",
    "            # Replace individual rare category with an empty string\n",
    "            mydata[col] = mydata[col].replace({cat: \"\"})\n",
    "        \n",
    "        # Replace the empty strings with the grouped category name\n",
    "        mydata[col] = mydata[col].replace({\"\": grouped_category_name.lstrip(\" or \")})\n",
    "        \n",
    "        # Store updated category frequencies\n",
    "        category_frequencies[col] = mydata[col].value_counts()\n",
    "\n",
    "        # Create a new categorical Series with only used categories\n",
    "        used_categories = mydata[col].cat.categories\n",
    "        mydata[col] = pd.Categorical(mydata[col], categories=used_categories)\n",
    "\n",
    "    # Print updated categories with the original category index\n",
    "    for col in categorical_columns:\n",
    "        updated_categories = mydata[col].cat.categories\n",
    "        print(f\"Categories for {col}: {updated_categories}\")\n",
    "\n",
    "    # Iterate through each categorical column\n",
    "    for col in categorical_columns:\n",
    "        # Check if \"missing\" is not already in the categories\n",
    "        if \"missing\" not in mydata[col].cat.categories:\n",
    "            # Add \"missing\" as a category\n",
    "            mydata[col] = mydata[col].cat.add_categories(\"missing\")\n",
    "        \n",
    "        # Replace NaN values with \"missing\"\n",
    "        mydata[col].fillna(\"missing\", inplace=True)\n",
    "\n",
    "\n",
    "    # Find features with mixed category data types\n",
    "    mixed_category_features = []\n",
    "    for col in mydata.columns:\n",
    "        if mydata[col].dtype == 'category':\n",
    "            unique_categories = mydata[col].cat.categories\n",
    "            unique_dtypes = set(type(cat) for cat in unique_categories)\n",
    "            if len(unique_dtypes) > 1:\n",
    "                mixed_category_features.append(col)\n",
    "\n",
    "    # Convert categories to strings for mixed category features\n",
    "    for feature in mixed_category_features:\n",
    "        mydata[feature] = mydata[feature].astype('str').astype('category')\n",
    "\n",
    "\n",
    "    # Identify categorical columns\n",
    "    categorical_columns = mydata.select_dtypes(include=['category']).columns\n",
    "\n",
    "    # Convert categories to strings for each categorical column\n",
    "    for col in categorical_columns:\n",
    "        mydata[col] = mydata[col].astype(str).astype('category')\n",
    "\n",
    "    category_frequencies = {}\n",
    "    for col in categorical_columns:\n",
    "        category_counts = mydata[col].value_counts()\n",
    "        category_frequencies[col] = category_counts\n",
    "\n",
    "    print(category_frequencies)\n",
    "    \n",
    "    # Restore original index for mydata\n",
    "    mydata = mydata.iloc[mydata_backup.index]\n",
    "    if external_val:\n",
    "        # Restore original index for extval_data\n",
    "        extval_data = extval_data.iloc[extval_data.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d550c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5439d8",
   "metadata": {},
   "source": [
    "#### Shorten the name of features (optional)\n",
    "If feature names are longer than a specific number of characters specified by user, it cuts it down to that so when the feature names appear on the plots they're shortened for visualization purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e939c396",
   "metadata": {},
   "outputs": [],
   "source": [
    "if shorten_feature_names:\n",
    "    def shorten_column_names(df, max_length):\n",
    "        new_columns = []\n",
    "        used_names = set()  # Keep track of used names to avoid duplicates\n",
    "        for col in df.columns:\n",
    "            # Check if column name is longer than max_length\n",
    "            if len(col) > max_length:\n",
    "                # Shorten column name by keeping only the beginning part and adding ...\n",
    "                new_col = col[:max_length] + '...'\n",
    "            else:\n",
    "                new_col = col\n",
    "            \n",
    "            # If the shortened name already exists, add a numeric suffix\n",
    "            suffix = 1\n",
    "            base_name = new_col\n",
    "            while new_col in used_names:\n",
    "                new_col = f\"{base_name}_{suffix}\"\n",
    "                suffix += 1\n",
    "            \n",
    "            used_names.add(new_col)\n",
    "            new_columns.append(new_col)\n",
    "        return new_columns\n",
    "\n",
    "    # Shorten column names\n",
    "    mydata.columns = shorten_column_names(mydata, fname_max_length)\n",
    "\n",
    "    if external_val:\n",
    "        extval_data.columns = shorten_column_names(extval_data, fname_max_length)\n",
    "\n",
    "    def shorten_data_dictionary(data_dict, max_length):\n",
    "        new_data_dict = {}\n",
    "        used_values = set()  # Keep track of used values to avoid duplicates\n",
    "        for key, value in data_dict.items():\n",
    "            # Check if column name is longer than max_length\n",
    "            if len(value) > max_length:\n",
    "                # Shorten column name by keeping only the beginning part and adding ...\n",
    "                new_value = value[:max_length] + '...'\n",
    "            else:\n",
    "                new_value = value\n",
    "            \n",
    "            # If the shortened value already exists, add a numeric suffix\n",
    "            suffix = 1\n",
    "            base_value = new_value\n",
    "            while new_value in used_values:\n",
    "                new_value = f\"{base_value}_{suffix}\"\n",
    "                suffix += 1\n",
    "            \n",
    "            used_values.add(new_value)\n",
    "            new_data_dict[key] = new_value\n",
    "        return new_data_dict\n",
    "\n",
    "    # Shorten data dictionary\n",
    "    data_dictionary = shorten_data_dictionary(data_dictionary, fname_max_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156f3b24",
   "metadata": {},
   "source": [
    "The code chunk below applies outlier (anomaly) detection and removal based on isolation forest algorithm. It's optional and is done if chosen by the user (remove_outliers = True). It follows these steps:\n",
    "\n",
    "1. Data Preparation:\n",
    "\n",
    "Separates the input features (X) and the target variable (y) from the original dataset (mydata).\n",
    "Encodes categorical features using one-hot encoding to convert them into numerical format, avoiding multicollinearity by dropping the first category.\n",
    "\n",
    "2. Handling Missing Values:\n",
    "\n",
    "Imputes missing values in the combined dataset (X_combined), which includes both numerical and encoded categorical features, using the K-Nearest Neighbors (KNN) imputation method. The number of neighbors used for imputation is calculated based on the size of the dataset.\n",
    "\n",
    "3. Outlier Detection:\n",
    "\n",
    "Initializes an IsolationForest model to detect outliers.\n",
    "Fits the model to the data and predicts outliers, labeling them as -1.\n",
    "\n",
    "4. Filtering Outliers:\n",
    "\n",
    "Filters out rows marked as outliers from both the features (X) and the target variable (y).\n",
    "Combines the cleaned features and target variable back into a single DataFrame (mydata).\n",
    "\n",
    "5. Final Cleanup:\n",
    "\n",
    "Removes the 'outlier' column from the final DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "914232cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove outliers automatically detected using isolation forest\n",
    "if remove_outliers:\n",
    " \n",
    "    # Separate features and outcome\n",
    "    X = mydata.drop(columns=[outcome_var])\n",
    "    y = mydata[outcome_var]\n",
    "    \n",
    "    # One-Hot Encode categorical features\n",
    "    encoder = OneHotEncoder(drop='first', sparse=False)  # drop='first' to avoid multicollinearity, sparse=False to get a dense output\n",
    "    X_encoded = encoder.fit_transform(X[cat_features])\n",
    "\n",
    "    # Convert encoded features into a DataFrame with preserved index\n",
    "    encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(cat_features), index=X.index)\n",
    "\n",
    "    # Combine encoded features with numerical features\n",
    "    X_combined = pd.concat([X.drop(columns=cat_features), encoded_df], axis=1)\n",
    "\n",
    "    # Impute missing values in continuous features of X_train using KNN\n",
    "    nn = int(np.sqrt(X_combined.shape[0])) # from Devroye, L., Györfi, L., & Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition. Springer. https://doi.org/10.1007/978-1-4612-0711-5.\n",
    "    cont_imputer = KNNImputer(n_neighbors=nn, weights = 'distance', keep_empty_features = True)  \n",
    "    \n",
    "    X_combined = pd.DataFrame(cont_imputer.fit_transform(X_combined), index=X_combined.index)\n",
    "    \n",
    "    # Step 1: Initialize the IsolationForest model\n",
    "    iso_forest = IsolationForest(contamination='auto', random_state=SEED)\n",
    "\n",
    "    # Step 2: Fit the model to the DataFrame\n",
    "    iso_forest.fit(X_combined)\n",
    "\n",
    "    # Step 3: Predict the outliers (-1 indicates an outlier)\n",
    "    X['outlier'] = iso_forest.predict(X_combined)\n",
    "\n",
    "    # Step 4: Filter out the outliers using the original index\n",
    "    mask = X['outlier'] != -1\n",
    "    X = X[mask]\n",
    "    y = y[mask]  # Filter y using the same mask\n",
    "    \n",
    "    mydata = pd.concat([X, y], axis=1)\n",
    "\n",
    "    # Drop the 'outlier' column if you don't need it anymore\n",
    "    mydata = mydata.drop(columns=['outlier'])\n",
    "    \n",
    "    if already_split:\n",
    "        # Separate features and outcome\n",
    "        X = testset.drop(columns=[outcome_var])\n",
    "        y = testset[outcome_var]\n",
    "        \n",
    "        # One-Hot Encode categorical features\n",
    "        encoder = OneHotEncoder(drop='first', sparse=False)  # drop='first' to avoid multicollinearity, sparse=False to get a dense output\n",
    "        X_encoded = encoder.fit_transform(X[cat_features])\n",
    "\n",
    "        # Convert encoded features into a DataFrame with preserved index\n",
    "        encoded_df = pd.DataFrame(X_encoded, columns=encoder.get_feature_names_out(cat_features), index=X.index)\n",
    "\n",
    "        # Combine encoded features with numerical features\n",
    "        X_combined = pd.concat([X.drop(columns=cat_features), encoded_df], axis=1)\n",
    "\n",
    "        # Impute missing values in continuous features of X_train using KNN\n",
    "        nn = int(np.sqrt(X_combined.shape[0])) # from Devroye, L., Györfi, L., & Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition. Springer. https://doi.org/10.1007/978-1-4612-0711-5.\n",
    "        cont_imputer = KNNImputer(n_neighbors=nn, weights = 'distance', keep_empty_features = True)  \n",
    "        \n",
    "        X_combined = pd.DataFrame(cont_imputer.fit_transform(X_combined), index=X_combined.index)\n",
    "        \n",
    "        # Step 1: Initialize the IsolationForest model\n",
    "        iso_forest = IsolationForest(contamination='auto', random_state=SEED)\n",
    "\n",
    "        # Step 2: Fit the model to the DataFrame\n",
    "        iso_forest.fit(X_combined)\n",
    "\n",
    "        # Step 3: Predict the outliers (-1 indicates an outlier)\n",
    "        X['outlier'] = iso_forest.predict(X_combined)\n",
    "\n",
    "        # Step 4: Filter out the outliers using the original index\n",
    "        mask = X['outlier'] != -1\n",
    "        X = X[mask]\n",
    "        y = y[mask]  # Filter y using the same mask\n",
    "        \n",
    "        testset = pd.concat([X, y], axis=1)\n",
    "\n",
    "        # Drop the 'outlier' column if you don't need it anymore\n",
    "        testset = testset.drop(columns=['outlier'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6126bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85fba1e",
   "metadata": {},
   "source": [
    "### Data split (prediction vs. discovery)\n",
    "\n",
    "This section covers performing a random data split. It's important to note that the default data split may not always be the optimal approach. In certain cases, a custom data split, such as one stratified by multiple variables (data_split_multi_strats) or by patient ID (or other individual identifiers), may be more appropriate. The following code can be adjusted to accommodate these specific data splitting needs.\n",
    "\n",
    "Additionally, if the primary aim of a study is to explore relationships between independent variables (features) and the outcome variable using cross-validation of machine learning models, it is recommended to use the entire dataset for cross-validation. This approach is particularly useful when the dataset is too small to be divided into a training (development) set and a test set.\n",
    "\n",
    "However, if the study's goal is both to investigate associations and to develop a model for prognostic or diagnostic purposes, then data splitting becomes relevant. The first approach is considered a discovery phase, while the second approach is aimed at prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbe328",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcee8624",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c908e738",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd306664",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_size_perc = 1 - train_size_perc\n",
    "if data_split:\n",
    "    if data_split_by_patients: \n",
    "        # If there are more than one sample per patient, this split is suggested\n",
    "        # to avoid having the same patient data in both training and test sets.\n",
    "        # In this case, the column that specifies patient ID is also used in the data split.\n",
    "        unique_patients = mydata[patient_id_col].unique()\n",
    "        train_patients, test_patients = train_test_split(unique_patients, test_size=test_size_perc, random_state=SEED)\n",
    "        trainset = mydata[mydata[patient_id_col].isin(train_patients)]\n",
    "        testset = mydata[mydata[patient_id_col].isin(test_patients)]\n",
    "\n",
    "    elif data_split_multi_strats:\n",
    "        # If the data split must be stratified by more than one variable.\n",
    "        # In this case, if there are two variables specified by the user,\n",
    "        # they should be combined to create a combined variable.\n",
    "        # Then the combined variable is used for the stratification so that\n",
    "        # the same portion of categories exist in both train and test sets.\n",
    "        # this is defined for two variables, if you need more then you should modify the following code\n",
    "        combined_strats = mydata[strat_var1].astype(str) + '_' + mydata[outcome_var].astype(str)\n",
    "        mydata['combined_strats'] = combined_strats\n",
    "        mydata, testset = train_test_split(mydata, test_size=test_size_perc, random_state=SEED, stratify=mydata['combined_strats'])\n",
    "        mydata.drop(columns=['combined_strats',strat_var1], inplace=True)\n",
    "        testset.drop(columns=['combined_strats',strat_var1], inplace=True)\n",
    "        # mydata.drop(columns = ['combined_strats',strat_var1], inplace=True)\n",
    "        if external_val:\n",
    "            if not extval_data.empty and strat_var1 in extval_data.columns:\n",
    "                extval_data.drop(columns=strat_var1, inplace=True)\n",
    "    elif already_split:\n",
    "        # If you have done data split already using a different approach,\n",
    "        # here just check if trainset and testset are comparable\n",
    "        # (have the same variables and datatype).\n",
    "        if set(mydata.columns) != set(testset.columns):\n",
    "            raise ValueError(\"Trainset and testset have different columns.\")\n",
    "        if not all(mydata.dtypes == testset.dtypes):\n",
    "            raise ValueError(\"Trainset and testset have different data types for columns.\")\n",
    "        # Optionally, you can check for other comparability metrics as needed.\n",
    "    else:\n",
    "        # stratified data split based on outcome variable (see also the other conditions if they may be relevant for your dataset)\n",
    "        mydata, testset = train_test_split(mydata, test_size= test_size_perc, random_state=SEED, stratify= mydata[outcome_var])\n",
    "else:\n",
    "    _, testset = train_test_split(mydata, test_size=test_size_perc, random_state=SEED, stratify= mydata[outcome_var]) # this way we keep a dummy testset just to avoid including too many conditions in the pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854f4cc",
   "metadata": {},
   "source": [
    "#### Checking the availability of all categories\n",
    "make sure both train and test sets have all categorical levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63159dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_split:\n",
    "    # Get list of categorical variable names\n",
    "    categorical_vars = mydata.select_dtypes(include=['category']).columns.tolist()\n",
    "\n",
    "    for var in categorical_vars:\n",
    "        unique_categories = set(mydata[var]).union(set(testset[var]))\n",
    "        print(var)\n",
    "        print(unique_categories)\n",
    "        \n",
    "        # Exclude old categories before adding new categories to the train set\n",
    "        new_categories_train = unique_categories.difference(mydata[var].cat.categories)\n",
    "        mydata[var] = mydata[var].cat.add_categories(new_categories_train)\n",
    "        \n",
    "        # Exclude old categories before adding new categories to the test set\n",
    "        new_categories_test = unique_categories.difference(testset[var].cat.categories)\n",
    "        testset[var] = testset[var].cat.add_categories(new_categories_test)\n",
    "        \n",
    "        if external_val:\n",
    "            # Exclude old categories before adding new categories to the extval_data set\n",
    "            new_categories_test = unique_categories.difference(extval_data[var].cat.categories)\n",
    "            extval_data[var] = extval_data[var].cat.add_categories(new_categories_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787fa82",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3bf09d",
   "metadata": {},
   "source": [
    "#### Filter highly missing data\n",
    "\n",
    "Missing data can significantly impact model performance and introduce bias, making consistent preprocessing crucial. \n",
    "\n",
    "To address this, the following steps are undertaken:\n",
    "\n",
    "1. **Filter Columns in `mydata`:** Identify and retain columns in `mydata` where the proportion of missing values is below a specified threshold. This step removes columns with excessive missing data that could skew analysis or model training.\n",
    "\n",
    "2. **Apply Identified Columns to Other Datasets:** Ensure that `testset` and `extval_data` are aligned with `mydata` by selecting only the columns present in the filtered `mydata`. This maintains consistency across datasets, which is essential for reliable model evaluation and comparison.\n",
    "\n",
    "3. **Filter Rows in All Datasets:** After aligning columns, filter out rows from all datasets where the proportion of missing values exceeds the threshold. This step ensures that all datasets have comparable completeness, supporting fair and accurate modeling.\n",
    "\n",
    "By following this approach, all datasets are harmonized with respect to both columns and rows, ensuring consistency and reducing potential bias from missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad8209f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_columns(df, threshold=0.90):\n",
    "    \"\"\"Filter out columns with missingness greater than the threshold.\"\"\"\n",
    "    # Calculate the missingness ratio for each column\n",
    "    column_missingness = df.isnull().mean(axis=0)\n",
    "    # Identify columns to keep\n",
    "    columns_to_keep = column_missingness[column_missingness <= threshold].index\n",
    "    return df[columns_to_keep]\n",
    "\n",
    "def filter_rows(df, threshold=0.90):\n",
    "    \"\"\"Filter out rows with missingness greater than the threshold.\"\"\"\n",
    "    # Calculate the missingness ratio for each row\n",
    "    row_missingness = df.isnull().mean(axis=1)\n",
    "    # Keep rows with missingness less than or equal to the threshold\n",
    "    return df[row_missingness <= threshold]\n",
    "\n",
    "# Apply the filtering to `mydata`\n",
    "if exclude_highly_missing_columns:\n",
    "    mydata = filter_columns(mydata, threshold=column_threshold)\n",
    "\n",
    "# Apply column filtering to `testset` and `extval_data` using columns identified from `mydata`\n",
    "if exclude_highly_missing_columns:\n",
    "    testset = testset[mydata.columns]\n",
    "    if external_val:\n",
    "        extval_data = extval_data[mydata.columns]\n",
    "\n",
    "# Apply row filtering to all datasets\n",
    "if exclude_highly_missing_rows:\n",
    "    mydata = filter_rows(mydata, threshold=row_threshold)\n",
    "    testset = filter_rows(testset, threshold=row_threshold)\n",
    "    if external_val:\n",
    "        extval_data = filter_rows(extval_data, threshold=row_threshold)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5459238c",
   "metadata": {},
   "source": [
    "#### Feature selection (optional but recommended if the dataset is high dimensional, e.g >100 features)\n",
    "\n",
    "Minimum Redundancy Maximum Relevance (mRMR) is one of the most popular algorithms for feature selection. For more information on its implementation see https://github.com/smazzanti/mrmr.\n",
    "\n",
    "Reference: \n",
    "F. Long, H. Peng and C. Ding, \"Feature Selection Based on Mutual Information: Criteria of Max-Dependency, Max-Relevance, and Min-Redundancy\" in IEEE Transactions on Pattern Analysis & Machine Intelligence, vol. 27, no. 08, pp. 1226-1238, 2005.\n",
    "doi: 10.1109/TPAMI.2005.159"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6477804d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feat_sel: # feature selection\n",
    "    # Separate features and outcome variable\n",
    "    X = mydata.drop(columns=[outcome_var])\n",
    "    y = mydata[outcome_var]\n",
    "    \n",
    "    numerical_columns = X.select_dtypes(include=['float64', 'int64']).columns\n",
    "    categorical_columns = X.select_dtypes(include=['category']).columns\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv_folds, random_state=SEED, shuffle=True)\n",
    "    # Initialize list to store selected features for each fold\n",
    "    selected_features_per_fold = []\n",
    "    selected_features_fold = []\n",
    "    # all_selected_features = set()\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "        \n",
    "        # for details see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\n",
    "        if scale_data:\n",
    "            robust_scaler = RobustScaler().fit(X_train_fold[numerical_columns]) \n",
    "            \n",
    "            # Use the RobustScaler to scale the numerical features\n",
    "            X_train_fold[numerical_columns] = robust_scaler.fit_transform(X_train_fold[numerical_columns])\n",
    "\n",
    "        # Impute missing values in continuous features of X_train using KNN\n",
    "        nn = int(np.sqrt(X_train_fold.shape[0])) # from Devroye, L., Györfi, L., & Lugosi, G. (1996). A Probabilistic Theory of Pattern Recognition. Springer. https://doi.org/10.1007/978-1-4612-0711-5.\n",
    "        cont_imputer = KNNImputer(n_neighbors=nn, weights = 'distance', keep_empty_features = True)  \n",
    "        \n",
    "        X_train_fold_filled = pd.DataFrame(cont_imputer.fit_transform(X_train_fold[numerical_columns]), columns=numerical_columns, index=X_train_fold.index)\n",
    "        \n",
    "        # Combine the categorical features with the normalized continuous features for the training set and the test set\n",
    "        X_train_fold = pd.concat([X_train_fold[categorical_columns], X_train_fold_filled], axis=1)\n",
    "        \n",
    "        # Replace categorical features with integers using LabelEncoder\n",
    "        label_encoders = {}\n",
    "        for col in categorical_columns:\n",
    "            label_encoders[col] = LabelEncoder()\n",
    "            X_train_fold[col] = label_encoders[col].fit_transform(X_train_fold[col])\n",
    "\n",
    "        # Select top num_features_sel features using mRMR\n",
    "        selected_features_fold = mrmr_classif(X=X_train_fold, y=y_train_fold, K=num_features_sel)\n",
    "\n",
    "        # Append selected features for this fold to the list\n",
    "        selected_features_per_fold.append(selected_features_fold)\n",
    "\n",
    "    # Find features common across all folds\n",
    "    selected_features = set(selected_features_per_fold[0]).intersection(*selected_features_per_fold[1:])\n",
    "    \n",
    "    # # Convert the set to a list if needed\n",
    "    selected_features = list(selected_features)\n",
    "    print(f'Final union of selected features across all folds: {selected_features}')\n",
    "\n",
    "    # Remove categorical features that are not selected\n",
    "    # categorical_vars = [feat_name for feat_name in categorical_vars if feat_name in selected_features]\n",
    "    cat_features = [feat_name for feat_name in cat_features if feat_name in selected_features]\n",
    "    \n",
    "    print(\"Selected Features:\")\n",
    "    print(selected_features)\n",
    "\n",
    "    mydata = mydata[selected_features + [outcome_var]]\n",
    "    testset = testset[selected_features + [outcome_var]]\n",
    "    if external_val:\n",
    "        extval_data = extval_data[selected_features + [outcome_var]]\n",
    "    \n",
    "    # Sort columns alphabetically (column order can affect the reproducibility of some estimators)\n",
    "    mydata = mydata.reindex(sorted(mydata.columns), axis=1)\n",
    "    testset = testset.reindex(sorted(testset.columns), axis=1)\n",
    "    if external_val:\n",
    "        extval_data = extval_data.reindex(sorted(extval_data.columns), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e6dd9d",
   "metadata": {},
   "source": [
    "#### Cross correlation of variables\n",
    "\n",
    "Here the correlation coefficients between every pair of variables are calculated and presented as a heatmap. It also includes the outcome variable.\n",
    "method: Spearman rank-order correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a6867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace NaN with median for each class\n",
    "mydata_filled = mydata.copy()\n",
    "# One-hot encode categorical features\n",
    "mydata_filled = pd.get_dummies(mydata_filled, drop_first= True)\n",
    "for column in mydata.columns:\n",
    "    if mydata[column].isna().any():\n",
    "        mydata_filled[column] = mydata[column].fillna(mydata.groupby(outcome_var)[column].transform('median'))\n",
    "\n",
    "# Now, impute any remaining NaN values with the median of the entire column\n",
    "mydata_filled.fillna(mydata_filled.median(), inplace=True)\n",
    "\n",
    "# Calculate Spearman rank-order correlation\n",
    "correlation_matrix = mydata_filled.corr(method='spearman')\n",
    "\n",
    "# Find pairs of features with NaN values\n",
    "nan_pairs = []\n",
    "for col in correlation_matrix.columns:\n",
    "    nan_in_col = correlation_matrix[col].isna()\n",
    "    nan_pairs.extend([(col, row) for row, val in nan_in_col.iteritems() if val])\n",
    "\n",
    "# Replace NaN values with 0\n",
    "correlation_matrix.fillna(0, inplace=True)\n",
    "\n",
    "# Print names of pairs with NaN values\n",
    "if nan_pairs:\n",
    "    print(\"Pairs of features with undefined correlation values:\")\n",
    "    for pair in nan_pairs:\n",
    "        print(pair)\n",
    "else:\n",
    "    print(\"No pairs of features had undefined correlation values.\")\n",
    "\n",
    "\n",
    "# Calculate figure size based on the number of features\n",
    "num_features = correlation_matrix.shape[0]\n",
    "\n",
    "height = round(np.max([10, np.log(num_features)])) \n",
    "# Ensure height does not exceed the maximum allowed dimension\n",
    "max_height = 65535 / 72  # Convert pixels to inches\n",
    "if height > max_height:\n",
    "    height = max_height\n",
    "\n",
    "# Create the clustermap\n",
    "g = sns.clustermap(correlation_matrix,\n",
    "                   cmap=\"viridis\",\n",
    "                   figsize=(height, height),\n",
    "                   cbar_pos=(0.05, 0.95, 0.9, 0.05),\n",
    "                   method=\"ward\"\n",
    "                   )\n",
    "\n",
    "# Create a mask to hide the upper triangle\n",
    "mask = np.triu(np.ones_like(correlation_matrix))\n",
    "\n",
    "# Apply the mask to the heatmap\n",
    "values = g.ax_heatmap.collections[0].get_array().reshape(correlation_matrix.shape)\n",
    "new_values = np.ma.array(values, mask=mask)\n",
    "g.ax_heatmap.collections[0].set_array(new_values)\n",
    "\n",
    "# Adjust x-axis and y-axis ticks\n",
    "g.ax_heatmap.set_xticks(np.arange(correlation_matrix.shape[0]) + 0.5, minor=False)\n",
    "g.ax_heatmap.set_yticks(np.arange(correlation_matrix.shape[1]) + 0.5, minor=False)\n",
    "g.ax_heatmap.set_xticklabels(correlation_matrix.index, fontsize=8)\n",
    "g.ax_heatmap.set_yticklabels(correlation_matrix.index, fontsize=8)\n",
    "\n",
    "g.ax_heatmap.set_facecolor('white')\n",
    "\n",
    "# display grid lines\n",
    "g.ax_heatmap.grid(which='both', color = \"grey\")\n",
    "\n",
    "# modify grid lines\n",
    "g.ax_heatmap.grid(which='minor', alpha=0.1)\n",
    "g.ax_heatmap.grid(which='major', alpha=0.2)\n",
    "\n",
    "# Hide the x-axis dendrogram\n",
    "g.ax_col_dendrogram.set_visible(False)\n",
    "\n",
    "# Save as TIFF\n",
    "g.savefig(\"feature_cor_clustermap.tif\", bbox_inches='tight')\n",
    "\n",
    "# Depict the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7835f794",
   "metadata": {},
   "source": [
    "### Sample size assessment \n",
    "\n",
    "The following script provides an analysis of dataset characteristics, including class imbalance, dataset size, and sample distribution per class, to determine the suitability of hyperparameter tuning. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd70796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_hyperparameter_tuning_suitable(data, outcome_var, train_size=train_size_perc, n_splits_outer=cv_folds, n_splits_inner=cv_folds_hptuning):\n",
    "    # Extract features and target variable\n",
    "    X = data.drop(columns=[outcome_var])\n",
    "    y = data[outcome_var]\n",
    "    y = y.replace({class_1: 1, class_0: 0})\n",
    "        \n",
    "    # Calculate number of samples used in model training\n",
    "    n_train_samples = int(len(X) * train_size)\n",
    "    \n",
    "    # Print the number of samples used in model training\n",
    "    print(f\"Number of samples used in model training: {n_train_samples}\")\n",
    "  \n",
    "    # Define cross-validation strategy for outer loop\n",
    "    cv_outer = StratifiedKFold(n_splits=n_splits_outer, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    # Initialize arrays to store number of samples per class per fold for outer CV\n",
    "    n_samples_per_class_outer = np.zeros((n_splits_outer, 2))\n",
    "    n_samples_per_class_inner = []\n",
    "    \n",
    "    # Iterate over outer folds\n",
    "    for i, (train_index, _) in enumerate(cv_outer.split(X, y)):\n",
    "        # Get class distribution in the training fold for outer CV\n",
    "        y_train_fold_outer = y.iloc[train_index]\n",
    "        class_counts_outer = np.bincount(y_train_fold_outer)\n",
    "        n_samples_per_class_outer[i, :] = class_counts_outer\n",
    "        \n",
    "        # Define cross-validation strategy for inner loop\n",
    "        cv_inner = StratifiedKFold(n_splits=n_splits_inner, shuffle=True, random_state=SEED)\n",
    "        \n",
    "        # Initialize array to store number of samples per class per fold for inner CV\n",
    "        n_samples_per_class_inner_fold = np.zeros((n_splits_inner, 2))\n",
    "        \n",
    "        # Iterate over inner folds\n",
    "        for j, (train_index_inner, _) in enumerate(cv_inner.split(X.iloc[train_index], y.iloc[train_index])):\n",
    "            # Get class distribution in the testing fold for inner CV\n",
    "            y_train_fold_inner = y.iloc[train_index].iloc[train_index_inner]\n",
    "            class_counts_inner = np.bincount(y_train_fold_inner)\n",
    "            n_samples_per_class_inner_fold[j, :] = class_counts_inner\n",
    "        \n",
    "        # Append the array for inner CV to the list\n",
    "        n_samples_per_class_inner.append(n_samples_per_class_inner_fold)\n",
    "    \n",
    "    # Convert the list of arrays to a numpy array for inner CV\n",
    "    n_samples_per_class_inner = np.array(n_samples_per_class_inner)\n",
    "            \n",
    "    # Print mean and standard deviation of samples per class per fold for outer CV\n",
    "    print(\"Samples per class per fold for outer CV:\")\n",
    "    print(n_samples_per_class_outer)\n",
    "    \n",
    "    # Print mean and standard deviation of samples per class per fold for inner CV\n",
    "    print(\"Samples per class per fold for inner CV:\")\n",
    "    print(n_samples_per_class_inner)\n",
    "    \n",
    "    # Check if the number of samples per class in the inner CV is for example less than 10 (10 is chosen arbitrarily here)\n",
    "    if np.any(n_samples_per_class_inner < 10):\n",
    "        print(\"Warning: Number of samples per class in the inner cross-validation is less than 10. Hyperparameter tuning may not be suitable.\")\n",
    "    \n",
    "    # Combine outer and inner CV samples per class matrices\n",
    "    combined_samples_per_class = np.concatenate([n_samples_per_class_outer[:, np.newaxis, :], n_samples_per_class_inner], axis=1)\n",
    "\n",
    "    # Plot heatmap of samples per class per fold\n",
    "    plot_samples_per_class_per_fold(combined_samples_per_class)\n",
    "\n",
    "def plot_samples_per_class_per_fold(samples_per_class_per_fold):\n",
    "    fig, axes = plt.subplots(1, samples_per_class_per_fold.shape[0], figsize=(3 * samples_per_class_per_fold.shape[0], 6), sharey=True)\n",
    "    for i in range(samples_per_class_per_fold.shape[0]):\n",
    "        ax = axes[i]\n",
    "        im = ax.imshow(samples_per_class_per_fold[i], cmap='coolwarm', interpolation='nearest')\n",
    "\n",
    "        # Add color bar\n",
    "        cbar = ax.figure.colorbar(im, ax=ax)\n",
    "        cbar.set_label('Number of samples')\n",
    "\n",
    "        # Set axis labels and title\n",
    "        ax.set_title(f'Fold {i+1}')\n",
    "        ax.set_xlabel('Classes')\n",
    "        ax.set_xticks(np.arange(2))\n",
    "        ax.set_xticklabels(['Class 0', 'Class 1'])\n",
    "        ax.set_ylabel('Cross validation folds')\n",
    "        ax.set_yticks(np.arange(6))\n",
    "        ax.set_yticklabels(['Outer CV'] + [f'Inner CV {j+1}' for j in range(5)])\n",
    "        # We change the fontsize of minor ticks label \n",
    "        ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "        ax.tick_params(axis='both', which='minor', labelsize=8)\n",
    "\n",
    "        # Loop over data dimensions and create text annotations\n",
    "        for j in range(samples_per_class_per_fold.shape[1]):\n",
    "            for k in range(samples_per_class_per_fold.shape[2]):\n",
    "                text = ax.text(k, j, f'{samples_per_class_per_fold[i, j, k]:.0f}',\n",
    "                               ha=\"center\", va=\"center\", color=\"black\")\n",
    "\n",
    "    fig.suptitle('Samples per class per fold', fontsize=10)\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "    # it shows how many samples are expected to be available for the cross validation and hyperparameter tuning\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    fig.savefig(\"Samples_CV_map.tif\", bbox_inches='tight') \n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "is_hyperparameter_tuning_suitable(mydata, outcome_var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94891423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in case of applying oversampling\n",
    "if oversampling:\n",
    "    # Define your features and outcome variable\n",
    "    X_train = mydata.drop(outcome_var, axis=1)\n",
    "    y_train = mydata[outcome_var]\n",
    "\n",
    "    # Initialize RandomOverSampler to oversample the minority class\n",
    "    random_oversampler = RandomOverSampler(sampling_strategy='auto', random_state=SEED)\n",
    "\n",
    "    # Oversample the minority class in the training set\n",
    "    X_train_resampled, y_train_resampled = random_oversampler.fit_resample(X_train, y_train)\n",
    "    # Concatenate resampled features and outcome variable back into a dataframe\n",
    "    mydata = pd.concat([pd.DataFrame(X_train_resampled, columns=X_train.columns), \n",
    "                            pd.Series(y_train_resampled, name=outcome_var)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff95646",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3831f",
   "metadata": {},
   "source": [
    "##### Statistical comparision of the training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec574a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_split:\n",
    "\n",
    "    # Define a function to check statistical difference for numerical variables\n",
    "    def check_numerical_difference(train_data, test_data):\n",
    "        numerical_vars = train_data.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        results = {}\n",
    "        for var in numerical_vars:\n",
    "            train_values = train_data[var].dropna()  # Drop missing values\n",
    "            test_values = test_data[var].dropna()    # Drop missing values\n",
    "            if len(train_values) > 0 and len(test_values) > 0:\n",
    "                stat, p = mannwhitneyu(train_values, test_values)\n",
    "                results[var] = {'Statistic': stat, 'p-value': p}\n",
    "        return results\n",
    "\n",
    "    # Check statistical difference for numerical variables\n",
    "    numerical_results = check_numerical_difference(mydata, testset)\n",
    "    # Print results\n",
    "    print(\"Statistical Difference for Numerical Variables:\")\n",
    "    for var, result in numerical_results.items():\n",
    "        print(f\"{var}: Statistic = {result['Statistic']}, p-value = {result['p-value']}\")\n",
    "\n",
    "    def check_categorical_difference(train_data, test_data):\n",
    "        categorical_vars = train_data.select_dtypes(include=['category']).columns.tolist()\n",
    "        results = {}\n",
    "        for var in categorical_vars:\n",
    "            train_categories = set(train_data[var].cat.categories)\n",
    "            test_categories = set(test_data[var].cat.categories)\n",
    "            common_categories = train_categories.intersection(test_categories)\n",
    "            \n",
    "            if len(common_categories) > 0:\n",
    "                train_counts = train_data[var].fillna('missing').value_counts()\n",
    "                test_counts = test_data[var].fillna('missing').value_counts()\n",
    "                \n",
    "                # Create the contingency table manually\n",
    "                contingency_table = pd.DataFrame(index=list(common_categories), columns=['Train', 'Test'])\n",
    "                for category in common_categories:\n",
    "                    contingency_table.loc[category, 'Train'] = train_counts.get(category, 0)\n",
    "                    contingency_table.loc[category, 'Test'] = test_counts.get(category, 0)\n",
    "                \n",
    "                if contingency_table.shape == (2, 2):  # Chi-square test requires at least 2x2 contingency table\n",
    "                    chi2, p, _, _ = chi2_contingency(contingency_table)\n",
    "                    results[var] = {'Chi-square': chi2, 'p-value': p}\n",
    "        return results\n",
    "\n",
    "    # 'mydata' is the original dataset and 'testset' is the test set obtained from train_test_split\n",
    "    categorical_results = check_categorical_difference(mydata, testset)\n",
    "\n",
    "    # Print results\n",
    "    print(\"Statistical Difference for Categorical Variables (if available):\")\n",
    "    for var, result in categorical_results.items():\n",
    "        print(f\"{var}: Chi-square = {result['Chi-square']}, p-value = {result['p-value']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65b1161",
   "metadata": {},
   "source": [
    "### Data overview \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85494e8",
   "metadata": {},
   "source": [
    "#### Display the type of the variables (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283064c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2cea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07661bf",
   "metadata": {},
   "source": [
    "#### Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3333fe2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with missing values\n",
    "columns_with_missing_values = mydata.columns[mydata.isnull().any()]\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a559a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a0bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "108830b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to report missingness both for categorical and continuous variables, it saves the results in an excel file\n",
    "def calculate_missingness(data, output_file='missingness_report.xlsx'):\n",
    "    # Create a copy of the dataframe\n",
    "    df = data.copy()\n",
    "\n",
    "    # Identify categorical variables\n",
    "    categorical_variables = df.select_dtypes(include=['category']).columns\n",
    "\n",
    "    # Create a dataframe to store missing counts for categorical variables\n",
    "    missing_counts = pd.DataFrame(index=categorical_variables, columns=['Missing Count'])\n",
    "\n",
    "    # Count missing values for each categorical variable\n",
    "    for column in categorical_variables:\n",
    "        missing_counts.loc[column, 'Missing Count'] = (df[column] == 'missing').sum()\n",
    "\n",
    "    # Calculate the total number of missing values for each column\n",
    "    missing_values = df.isnull().sum()\n",
    "\n",
    "    # Divide by the total number of rows to get missing percentage\n",
    "    total_rows = len(data)\n",
    "    missing_percentage = (missing_values / total_rows) * 100\n",
    "\n",
    "    # Correct missing percentages for categorical columns\n",
    "    for column in categorical_variables:\n",
    "        if column in missing_percentage.index:\n",
    "            if missing_counts.loc[column, 'Missing Count'] > 0:  # Only adjust if missing categories exist\n",
    "                missing_percentage[column] = (missing_counts.loc[column, 'Missing Count'] / total_rows) * 100\n",
    "\n",
    "    # Round the percentages to two decimal points\n",
    "    missing_percentage = missing_percentage.round(2)\n",
    "\n",
    "    # Sort the percentages in ascending order\n",
    "    missing_percentage = missing_percentage.sort_values(ascending=False)\n",
    "\n",
    "    # Calculate the mean and standard deviation of the missingness\n",
    "    mean_missingness = np.mean(missing_percentage)\n",
    "    std_missingness = np.std(missing_percentage)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"Missing Value Percentages:\")\n",
    "    print(missing_percentage)\n",
    "    print(\"Mean ± Standard Deviation of Missingness: {:.2f} ± {:.2f}\".format(mean_missingness, std_missingness))\n",
    "    \n",
    "    # Save results to an Excel file\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        # Save the missing percentage\n",
    "        missing_percentage.to_excel(writer, sheet_name='Missing Percentages')\n",
    "        \n",
    "        # Save mean and std as a separate sheet\n",
    "        summary_df = pd.DataFrame({\n",
    "            'Mean Missingness': [mean_missingness],\n",
    "            'Std Missingness': [std_missingness]\n",
    "        })\n",
    "        summary_df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd9c1e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_missingness(data=mydata, output_file='missingness_trainingset.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243ccf11",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_missingness(data=testset, output_file='missingness_testset.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3123e2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(mydata)\n",
    "df2 = pd.DataFrame(testset)\n",
    "if external_val:\n",
    "    df3 = pd.DataFrame(extval_data)\n",
    "# Function to create the summary table for a single dataset\n",
    "def create_summary_table(dataframe, dataset_name):\n",
    "    summary_data = {'Variable': [], 'Value': []}\n",
    "\n",
    "    for col in sorted(dataframe.columns):  # Sort variable names alphabetically\n",
    "        # Variable name\n",
    "        summary_data['Variable'].append(col)\n",
    "        summary_data['Value'].append('Variable Name')\n",
    "\n",
    "        # For numerical variables - Median (lower quantile, higher quantile)\n",
    "        if pd.api.types.is_numeric_dtype(dataframe[col]):\n",
    "            median = dataframe[col].median()\n",
    "            q25 = dataframe[col].quantile(0.25)\n",
    "            q75 = dataframe[col].quantile(0.75)\n",
    "            summary_data['Variable'].append('')\n",
    "            summary_data['Value'].append(f'{median:.2f} ({q25:.2f}, {q75:.2f})')\n",
    "\n",
    "        # For categorical variables - Categories, Counts, and Percentages\n",
    "        elif pd.api.types.is_categorical_dtype(dataframe[col]):\n",
    "            categories = dataframe[col].value_counts()\n",
    "            total_count = len(dataframe[col])\n",
    "            summary_data['Variable'].extend(['', ''])\n",
    "            summary_data['Value'].extend(['Categories', 'Counts'])\n",
    "            \n",
    "            for category, count in categories.items():\n",
    "                percentage = (count / total_count) * 100\n",
    "                summary_data['Variable'].append(category)\n",
    "                summary_data['Value'].append(f'{count} - {percentage:.2f}%')\n",
    "\n",
    "        # Missing values for all variable types\n",
    "        missing_count = dataframe[col].isnull().sum()\n",
    "        missing_percentage = (missing_count / len(dataframe)) * 100\n",
    "        summary_data['Variable'].append('')\n",
    "        summary_data['Value'].append(f' {missing_percentage:.2f}%')\n",
    "\n",
    "    # Add a column for the dataset name\n",
    "    summary_data['Variable'].append('Dataset')\n",
    "    summary_data['Value'].append(dataset_name)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df\n",
    "\n",
    "# Create summary tables for each dataset\n",
    "summary_table1 = create_summary_table(df1, 'training data')\n",
    "# Save the final summary table to an Excel file\n",
    "summary_table1.to_excel('summary_table_devset.xlsx', index=False)\n",
    "\n",
    "if data_split:\n",
    "    summary_table2 = create_summary_table(df2, 'test data')\n",
    "    summary_table2.to_excel('summary_table_testset.xlsx', index=False)\n",
    "if external_val:\n",
    "    summary_table3 = create_summary_table(df3, 'external validation data')\n",
    "    summary_table3.to_excel('summary_table_extvalset.xlsx', index=False)\n",
    "\n",
    "\n",
    "# final summary table\n",
    "print(summary_table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28956c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74aea7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for details see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\n",
    "if scale_data:\n",
    "    # Specify the numerical features you want to scale\n",
    "    numerical_columns = mydata.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    robust_scaler = RobustScaler().fit(mydata[numerical_columns]) \n",
    "    \n",
    "    # Use the RobustScaler to scale the numerical features\n",
    "    mydata[numerical_columns] = robust_scaler.fit_transform(mydata[numerical_columns])\n",
    "    testset[numerical_columns] = robust_scaler.fit_transform(testset[numerical_columns])\n",
    "    if external_val:\n",
    "        extval_data[numerical_columns] = robust_scaler.fit_transform(extval_data[numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d1e63e",
   "metadata": {},
   "source": [
    "### Data imputation\n",
    "\n",
    "Here we apply k-nearest neighbors (KNN) algorithm to impute missing values in continuous variables. This is done in fold-wise as in cross validation so that the informaiton from one fold does not leak to other folds. This means that the training data is split to a number of folds as the same as in cross validation and then the imputation is performed on the fold under test, for all folds. then they are merged back to recreate the training set with imputation. The test set and external datasets are also imputed based on the KNN algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "399063d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for details see https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n",
    "\n",
    "# Separate features and outcome variable\n",
    "X_train = mydata.drop(columns=[outcome_var])\n",
    "y_train = mydata[outcome_var]\n",
    "\n",
    "X_test = testset.drop(columns=[outcome_var])\n",
    "y_test = testset[outcome_var]\n",
    "\n",
    "numerical_columns = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_columns = X_train.select_dtypes(include=['category','object']).columns\n",
    "\n",
    "skf = StratifiedKFold(n_splits=cv_folds, random_state=SEED, shuffle=True)\n",
    "\n",
    "# List to hold the imputed validation sets\n",
    "imputed_train_data = []\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(skf.split(X_train, y_train), 1):\n",
    "    X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "    # Impute missing values in continuous features of X_train using KNN\n",
    "    nn = int(np.sqrt(X_train_fold.shape[0]))\n",
    "    random.seed(SEED)\n",
    "    cont_imputer = KNNImputer(n_neighbors=nn, weights = 'distance', keep_empty_features = True)  \n",
    "    cont_imputer.fit(X_train_fold[numerical_columns])  # Fit the imputer on the training portion of the fold\n",
    "\n",
    "    X_val_fold_filled = pd.DataFrame(cont_imputer.transform(X_val_fold[numerical_columns]), columns=numerical_columns, index=X_val_fold.index)\n",
    "    \n",
    "    # Combine the categorical features with the imputed continuous features for the validation set\n",
    "    X_val_fold_combined = pd.concat([X_val_fold[categorical_columns], X_val_fold_filled], axis=1)\n",
    "\n",
    "    # Combine features and target for the validation fold and append to the list\n",
    "    imputed_train_data.append(pd.concat([X_val_fold_combined, y_val_fold], axis=1))\n",
    "\n",
    "mydata_imputed = pd.concat(imputed_train_data) # this is used for cross validation\n",
    "\n",
    "nn = int(np.sqrt(X_train.shape[0]))\n",
    "random.seed(SEED)\n",
    "cont_imputer = KNNImputer(n_neighbors=5, weights = 'distance', keep_empty_features = True)\n",
    "X_train_filled = pd.DataFrame(cont_imputer.fit_transform(X_train[numerical_columns]), columns=numerical_columns, index=X_train.index)\n",
    "mydata_imputed_nocv = pd.concat([X_train[categorical_columns], X_train_filled, y_train], axis=1) # this is used for training the model to be tested on the test set (after cross-validation)\n",
    "\n",
    "X_test_filled = pd.DataFrame(cont_imputer.transform(X_test[numerical_columns]), columns=numerical_columns, index=X_test.index)\n",
    "# Combine the categorical features with the normalized continuous features for the external validation set\n",
    "testset_imputed = pd.concat([X_test[categorical_columns], X_test_filled, y_test], axis=1)\n",
    "    \n",
    "\n",
    "if external_val:\n",
    "    X_extval_data = extval_data.drop(outcome_var, axis=1)\n",
    "    y_extval_data = extval_data[outcome_var]\n",
    "    X_extval_data_filled_cont = pd.DataFrame(cont_imputer.transform(X_extval_data[numerical_columns]), columns=numerical_columns, index=y_extval_data.index)\n",
    "    # Combine the categorical features with the normalized continuous features for the external validation set\n",
    "    extval_data_imputed = pd.concat([X_extval_data[categorical_columns], X_extval_data_filled_cont, y_extval_data], axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6704a45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the rows of mydata_imputed to match the order of rows in mydata\n",
    "mydata_imputed = mydata_imputed.reindex(mydata.index)\n",
    "mydata_imputed_nocv = mydata_imputed_nocv.reindex(mydata.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1fc715",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata_imputed.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3896862",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = mydata_imputed.drop(outcome_var, axis=1)\n",
    "X_train_imputed_nocv = mydata_imputed_nocv.drop(outcome_var, axis=1)\n",
    "X_test_imputed = testset_imputed.drop(outcome_var, axis=1)\n",
    "\n",
    "# Perform one-hot encoding\n",
    "if external_val: # if there is an external validation set (in addition to the test set)\n",
    "    X_extval_data_imputed = extval_data_imputed.drop(outcome_var, axis=1)\n",
    "    \n",
    "    combined_imputed = pd.concat([X_train_imputed, X_test_imputed, X_train_imputed_nocv, X_extval_data_imputed], keys=['train', 'test', 'train_nocv','ext_val'])\n",
    "    combined_encoded = pd.get_dummies(combined_imputed, drop_first=True)\n",
    "    X_train_OHE = combined_encoded.xs('train')\n",
    "    X_test_OHE = combined_encoded.xs('test')\n",
    "    X_train_OHE_nocv = combined_encoded.xs('train_nocv')\n",
    "    X_extval_data_OHE = combined_encoded.xs('ext_val')\n",
    "    extval_data_imputed_OHE = pd.concat([X_extval_data_OHE, y_extval_data], axis=1)\n",
    "else: # no external validation\n",
    "    combined_imputed = pd.concat([X_train_imputed, X_test_imputed, X_train_imputed_nocv], keys=['train', 'test', 'train_nocv'])\n",
    "    combined_encoded = pd.get_dummies(combined_imputed, drop_first=True)\n",
    "    X_train_OHE = combined_encoded.xs('train')\n",
    "    X_test_OHE = combined_encoded.xs('test')\n",
    "    X_train_OHE_nocv = combined_encoded.xs('train_nocv')\n",
    "    \n",
    "mydata_imputed_OHE = pd.concat([X_train_OHE, y_train], axis=1) # for cross validation - imputed on folds of the training set\n",
    "mydata_imputed_OHE_nocv = pd.concat([X_train_OHE_nocv, y_train], axis=1) # for external validation - imputed on the entire training set\n",
    "testset_imputed_OHE = pd.concat([X_test_OHE, y_test], axis=1)\n",
    "\n",
    "  \n",
    "# Display the resulting dataframe\n",
    "print(X_train_OHE.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d748fbc",
   "metadata": {},
   "source": [
    "### Correlation analysis\n",
    "\n",
    "Here we use univariable correlation based on point-biserial correlation and mutual informaiton between the variables (features) and the outcome variable. This is based on one-hot encoded and imputed data (only development/training set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5be30302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = mydata_imputed_OHE.copy()\n",
    "# Convert 'outcome_var' to numerical variable\n",
    "df_imputed[outcome_var] = df_imputed[outcome_var].replace({class_1: 1, class_0: 0})\n",
    "\n",
    "# Generate 1000 subsamples of df_imputed\n",
    "def generate_subsample(df, seed):\n",
    "    rng = np.random.RandomState(seed)  # Set the random state\n",
    "    return df.sample(frac=1, replace=True, random_state=rng)\n",
    "\n",
    "# Calculate point biserial correlation for each variable against the target\n",
    "def calculate_biserial_corr(subsample, outcome_var):\n",
    "    corr_values = {}\n",
    "    for col in subsample.columns:\n",
    "        if col != outcome_var:\n",
    "            corr_values[col] = pointbiserialr(subsample[col], subsample[outcome_var])[0]\n",
    "    return corr_values\n",
    "\n",
    "# Generate subsamples in parallel\n",
    "num_iterations = 1000\n",
    "seeds = np.random.randint(0, 10000, size=num_iterations)  # Generate unique seeds for each iteration\n",
    "\n",
    "subsamples = Parallel(n_jobs=n_cpu_model_training, backend='loky')(\n",
    "    delayed(generate_subsample)(df_imputed, seed) for seed in seeds\n",
    ")\n",
    "\n",
    "# Calculate point biserial correlation for each subsample\n",
    "biserial_corr_values = Parallel(n_jobs=n_cpu_model_training, backend='loky')(\n",
    "    delayed(calculate_biserial_corr)(subsample, outcome_var) for subsample in subsamples\n",
    ")\n",
    "\n",
    "# Create a dataframe with correlation coefficients\n",
    "corr_df = pd.DataFrame(biserial_corr_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "597b92bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper quantiles of point-biserial correlation for each feature\n",
    "\n",
    "\n",
    "# Calculate lower quartile (25th percentile) excluding NaN values\n",
    "lower_quantile_corr = np.nanpercentile(corr_df, 25, axis=0)\n",
    "\n",
    "# Calculate median (50th percentile) excluding NaN values\n",
    "median_corr = np.nanpercentile(corr_df, 50, axis=0)\n",
    "\n",
    "# Calculate upper quartile (75th percentile) excluding NaN values\n",
    "upper_quantile_corr = np.nanpercentile(corr_df, 75, axis=0)\n",
    "\n",
    "# Filter features based on quantiles\n",
    "significant_features = [feature for feature, lower, upper in zip(corr_df.columns, lower_quantile_corr, upper_quantile_corr) if lower > 0 or upper < 0]\n",
    "\n",
    "# Filter the original dataframe to include only significant features\n",
    "df_imputed_filtered = df_imputed[significant_features]\n",
    "\n",
    "# Create a DataFrame with feature names, lower and upper quantiles of point-biserial correlation\n",
    "corr_summary_df = pd.DataFrame({'Feature': df_imputed.drop(outcome_var, axis=1).columns,\n",
    "                                'median_Corr': median_corr,\n",
    "                                'Lower_Quantile_Corr': lower_quantile_corr,\n",
    "                                'Upper_Quantile_Corr': upper_quantile_corr})\n",
    "corr_summary_df\n",
    "corr_summary_df.to_excel('pb_corr_summary_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2503ba21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort DataFrame by median correlation for better visualization\n",
    "corr_summary_df = corr_summary_df.sort_values(by='median_Corr', ascending=True)\n",
    "# corr_summary_df = corr_summary_df[~(corr_summary_df[\"median_Corr\"] == 0) & ~(corr_summary_df[\"median_Corr\"].isna())]\n",
    "\n",
    "num_rows = corr_summary_df.shape[0]\n",
    "# Set the fixed width\n",
    "width = 6\n",
    "# Calculate the height based on the number of rows\n",
    "height = num_rows / 5  # Assuming each row takes about 0.2 inches\n",
    "# Ensure height does not exceed the maximum allowed dimension\n",
    "max_height = 65535 / 72  # Convert pixels to inches\n",
    "if height > max_height:\n",
    "    height = max_height\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(width, height))\n",
    "# Plot error bars for all features\n",
    "plt.errorbar(x=corr_summary_df['median_Corr'], y=corr_summary_df['Feature'],\n",
    "             xerr=[corr_summary_df['median_Corr'] - corr_summary_df['Lower_Quantile_Corr'], \n",
    "                   corr_summary_df['Upper_Quantile_Corr'] - corr_summary_df['median_Corr']],\n",
    "             fmt='o', color='black', capsize=5, label='IQR')\n",
    "\n",
    "# Add a vertical dotted line at x=0\n",
    "plt.axvline(x=0, linestyle='--', color='grey', alpha=0.5)\n",
    "\n",
    "plt.title('Median and quantile correlation coefficients for features\\nbased on random subsamples of the development set\\nwith replication across 1000 iterations', fontsize=10)\n",
    "plt.xlabel('PB Correlation Values', fontsize=8)\n",
    "plt.ylabel('Feature', fontsize=8) \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.gca().tick_params(axis='both', labelsize=8) \n",
    "plt.gca().legend(fontsize=\"medium\")\n",
    "plt.gca().set_facecolor('white')\n",
    "# display grid lines\n",
    "plt.grid(which='both', color=\"grey\")\n",
    "plt.grid(which='minor', alpha=0.1)\n",
    "plt.grid(which='major', alpha=0.2)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.savefig(\"pointbiserial.tif\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3419f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c704dcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imputed = mydata_imputed_OHE.copy()\n",
    "# Convert 'outcome_var' to numerical variable\n",
    "df_imputed[outcome_var] = df_imputed[outcome_var].replace({class_1: 1, class_0: 0})\n",
    "\n",
    "# Generate subsample with a seed for reproducibility\n",
    "def generate_subsample(df, seed):\n",
    "    rng = np.random.RandomState(seed)  # Set the random state\n",
    "    return df.sample(frac=1, replace=True, random_state=rng)\n",
    "\n",
    "# Calculate mutual information for each variable against the target\n",
    "def calculate_mutual_info(subsample, outcome_var):\n",
    "    mi_values = {}\n",
    "    for col in subsample.columns:\n",
    "        if col != outcome_var:\n",
    "            mi_values[col] = mutual_info_classif(subsample[[col]], subsample[outcome_var])[0]\n",
    "    return mi_values\n",
    "\n",
    "np.random.seed(SEED)\n",
    "num_iterations = 1000\n",
    "seeds = randint(0, 10000, size=num_iterations)  # Generate unique seeds for each iteration\n",
    "\n",
    "subsamples = Parallel(n_jobs=n_cpu_model_training, backend='loky')(\n",
    "    delayed(generate_subsample)(df_imputed, seed) for seed in seeds\n",
    ")\n",
    "\n",
    "mi_values = Parallel(n_jobs=n_cpu_model_training, backend='loky')(\n",
    "    delayed(calculate_mutual_info)(subsample, outcome_var) for subsample in subsamples\n",
    ")\n",
    "\n",
    "mi_df = pd.DataFrame(mi_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a59463ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the lower and upper quantiles of point-biserial correlation for each feature\n",
    "lower_quantile_mi = np.percentile(mi_df, 25, axis=0)\n",
    "median_mi = np.percentile(mi_df, 50, axis=0)\n",
    "upper_quantile_mi = np.percentile(mi_df, 75, axis=0)\n",
    "\n",
    "# Filter features based on quantiles\n",
    "significant_features = [feature for feature, lower in zip(mi_df.columns, lower_quantile_mi) if lower != 0]\n",
    "\n",
    "# Filter the original dataframe to include only significant features\n",
    "df_imputed_filtered = df_imputed[significant_features]\n",
    "\n",
    "# Create a dataframe with feature names, lower and upper quantiles of point-biserial correlation\n",
    "mi_summary_df = pd.DataFrame({'Feature': df_imputed.drop(outcome_var, axis=1).columns,\n",
    "                                'median_MI': median_mi,\n",
    "                                'Lower_Quantile_MI': lower_quantile_mi,\n",
    "                                'Upper_Quantile_MI': upper_quantile_mi})\n",
    "mi_summary_df\n",
    "mi_summary_df.to_excel('mi_summary_df.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975fce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Mark significant features with a different color\n",
    "mi_summary_df['Color'] = np.where(mi_summary_df['Feature'].isin(significant_features), 'significant', 'not significant')\n",
    "\n",
    "# Sort dataframe by median correlation for better visualization\n",
    "mi_summary_df = mi_summary_df.sort_values(by='median_MI', ascending=True)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(width, height))\n",
    "\n",
    "# Plot error bars for all features\n",
    "plt.errorbar(x=mi_summary_df['median_MI'], y=mi_summary_df['Feature'],\n",
    "             xerr=[mi_summary_df['median_MI'] - mi_summary_df['Lower_Quantile_MI'], \n",
    "                   mi_summary_df['Upper_Quantile_MI'] - mi_summary_df['median_MI']],\n",
    "             fmt='o', color='black', capsize=5, label='IQR')\n",
    "\n",
    "# Add a vertical dotted line at x=0\n",
    "plt.axvline(x=0, linestyle='--', color='grey', alpha=0.5)\n",
    "\n",
    "plt.title('Median and quantile mutual informaiton for features\\nbased on random subsamples of the development set\\nwith replication across 1000 iterations', fontsize=10)\n",
    "plt.xlabel('mutual information', fontsize=8)\n",
    "plt.ylabel('Feature', fontsize=8) \n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.gca().tick_params(axis='both', labelsize=8) \n",
    "plt.gca().legend(fontsize=\"medium\")\n",
    "plt.gca().set_facecolor('white')\n",
    "# display grid lines\n",
    "plt.grid(which='both', color=\"grey\")\n",
    "plt.grid(which='minor', alpha=0.1)\n",
    "plt.grid(which='major', alpha=0.2)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.savefig(\"mutual_information.tif\", bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2ab1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f532d8e",
   "metadata": {},
   "source": [
    "#### Checking the outcome variable and its categories (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85641a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mydata[outcome_var].unique())\n",
    "print(mydata_imputed[outcome_var].unique())\n",
    "print(mydata_imputed_OHE[outcome_var].unique())\n",
    "print(mydata_imputed_nocv[outcome_var].unique())\n",
    "print(mydata_imputed_OHE_nocv[outcome_var].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c4b77a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var] = mydata[outcome_var].map({class_0: False, class_1: True}).astype(bool)\n",
    "mydata_imputed[outcome_var] = mydata_imputed[outcome_var].map({class_0: False, class_1: True}).astype(bool)\n",
    "mydata_imputed_nocv[outcome_var] = mydata_imputed_nocv[outcome_var].map({class_0: False, class_1: True}).astype(bool)\n",
    "mydata_imputed_OHE[outcome_var] = mydata_imputed_OHE[outcome_var].map({class_0: False, class_1: True}).astype(bool)\n",
    "mydata_imputed_OHE_nocv[outcome_var] = mydata_imputed_OHE_nocv[outcome_var].map({class_0: False, class_1: True}).astype(bool)\n",
    "testset_imputed[outcome_var] = testset_imputed[outcome_var].map({class_0: False, class_1: True}).astype(bool)\n",
    "if external_val:\n",
    "    extval_data_imputed[outcome_var] = extval_data_imputed[outcome_var].map({class_0: False, class_1: True}).astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e318c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mydata[outcome_var].unique())\n",
    "print(mydata_imputed[outcome_var].unique())\n",
    "print(mydata_imputed_OHE[outcome_var].unique())\n",
    "print(mydata_imputed_nocv[outcome_var].unique())\n",
    "print(mydata_imputed_OHE_nocv[outcome_var].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c45caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5507ca79",
   "metadata": {},
   "source": [
    "### Data visualization\n",
    "\n",
    "Here we plot all variables of the dataset both categorical and numerical ones in box plots that represents the distributions of the variables. Here you can inspect if there is any outlier or data anomally (e.g., values outside range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c21584",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select continuous variables from the dataframe\n",
    "continuous_vars = mydata_imputed.select_dtypes(include=['float64', 'int64'])\n",
    "# select categorical variables\n",
    "categorical_vars = mydata_imputed.select_dtypes(include=['category',\"object\",\"bool\"])\n",
    "# get a copy of the outcome variable\n",
    "outcome_variable = mydata_imputed[outcome_var].copy()\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_continuous_vars = len(continuous_vars.columns)\n",
    "num_categorical_vars = len(categorical_vars.columns)\n",
    "num_cols_to_plot = 3\n",
    "num_rows = (num_continuous_vars + num_categorical_vars + num_cols_to_plot - 1) // num_cols_to_plot + 1  # Adjust the number of rows based on the number of variables\n",
    "\n",
    "mapping = {True: class_1, False: class_0}\n",
    "outcome_variable_mapped = outcome_variable.map(mapping)\n",
    "\n",
    "# Create subplots for continuous variables\n",
    "fig, axes = plt.subplots(num_rows, num_cols_to_plot, figsize=(12, num_rows * 2)) \n",
    "\n",
    "# Iterate over continuous variables\n",
    "for i, column in enumerate(continuous_vars.columns):\n",
    "    # Determine the subplot indices\n",
    "    row_idx = i // num_cols_to_plot\n",
    "    col_idx = i % num_cols_to_plot\n",
    "\n",
    "    # Check if subplot index is within the bounds of axes\n",
    "    if row_idx < num_rows:\n",
    "        # Get the axis for the current subplot\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Iterate over each outcome category\n",
    "        for outcome_category, ax_offset in zip(outcome_variable.unique(), [-0.2, 0.2]):\n",
    "            # Filter the data for the current outcome category\n",
    "            filtered_data = continuous_vars[outcome_variable == outcome_category][column]\n",
    "\n",
    "            # Create a box plot for the current outcome category\n",
    "            positions = np.array([1 + ax_offset])\n",
    "            ax.boxplot(filtered_data.dropna(), positions=positions, widths=0.3, vert=False)  # Vert=False for horizontal box plots\n",
    "\n",
    "        ax.set_title(f'{column}', fontsize=8)\n",
    "        ax.set_yticks([1 - ax_offset, 1 + ax_offset])\n",
    "        # ax.set_yticklabels(outcome_variable.unique(), fontsize=8)\n",
    "        ax.set_yticklabels(outcome_variable_mapped.unique(), fontsize=8)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "        ax.legend(fontsize=\"medium\")\n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "        # show both grid lines\n",
    "        ax.grid(which='both', color = \"grey\")\n",
    "\n",
    "        # modify grid lines:\n",
    "        ax.grid(which='minor', alpha=0.1)\n",
    "        ax.grid(which='major', alpha=0.2)\n",
    "\n",
    "# Iterate over categorical variables\n",
    "for i, column in enumerate(categorical_vars.columns):\n",
    "    # Determine the subplot indices\n",
    "    row_idx = (i + num_continuous_vars) // num_cols_to_plot\n",
    "    col_idx = (i + num_continuous_vars) % num_cols_to_plot\n",
    "\n",
    "    # Check if subplot index is within the bounds of axes\n",
    "    if row_idx < num_rows:\n",
    "        # Get the axis for the current subplot\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Normalize the counts for the current categorical variable stratified by outcome variable\n",
    "        category_counts = categorical_vars.groupby(outcome_variable)[column].value_counts(normalize=True).unstack()\n",
    "        category_counts.plot(kind='barh', ax=ax)\n",
    "\n",
    "        # Set the title with the feature name\n",
    "        ax.set_title(f'{column}', fontsize=8)\n",
    "        ax.set_yticklabels(outcome_variable_mapped.unique(), fontsize=8)\n",
    "        ax.set_ylabel(None)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "        ax.legend(fontsize=\"medium\")\n",
    "        ax.set_facecolor('white')\n",
    "        \n",
    "        # display grid lines\n",
    "        ax.grid(which='both', color = \"grey\")\n",
    "\n",
    "        # modify grid lines\n",
    "        ax.grid(which='minor', alpha=0.1)\n",
    "        ax.grid(which='major', alpha=0.2)\n",
    "\n",
    "# Remove any empty subplots at the end\n",
    "if num_continuous_vars + num_categorical_vars < num_rows * num_cols_to_plot:\n",
    "    for i in range(num_continuous_vars + num_categorical_vars, num_rows * num_cols_to_plot):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "# Remove the subplot for outcome_var at the end\n",
    "last_ax_index = num_continuous_vars + num_categorical_vars - 1\n",
    "fig.delaxes(axes.flatten()[last_ax_index])\n",
    "\n",
    "# Adjust the layout and spacing\n",
    "plt.tight_layout()\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.size'] = 8\n",
    "plt.savefig(\"data_distribution.tif\", bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e61029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of samples per class in devset\n",
    "ymap = {True: class_1, False: class_0}\n",
    "mydata_class_counts = mydata[outcome_var].replace(ymap).value_counts()\n",
    "\n",
    "# Calculate the percentage of samples per class in devset\n",
    "mydata_class_percentages = (mydata_class_counts / len(mydata)) * 100\n",
    "\n",
    "# Count the number of samples per class in testset\n",
    "testset_class_counts = testset[outcome_var].value_counts()\n",
    "\n",
    "# Calculate the percentage of samples per class in testset\n",
    "testset_class_percentages = (testset_class_counts / len(testset)) * 100\n",
    "\n",
    "# summary of the number of samples per class and their percentages\n",
    "if data_split:\n",
    "    print(\"training set:\")\n",
    "    print(mydata_class_counts)\n",
    "    print(mydata_class_percentages)\n",
    "    print(\"\\nTest Set:\")\n",
    "    print(testset_class_counts)\n",
    "    print(testset_class_percentages)\n",
    "else:\n",
    "    print(\"whole dataset:\")\n",
    "    print(mydata_class_counts)\n",
    "    print(mydata_class_percentages)\n",
    "\n",
    "if external_val:\n",
    "    # Count the number of samples per class in extval_data\n",
    "    extval_data_class_counts = extval_data[outcome_var].value_counts()\n",
    "\n",
    "    # Calculate the percentage of samples per class in extval_data\n",
    "    extval_data_class_percentages = (extval_data_class_counts / len(extval_data)) * 100\n",
    "    print(\"\\nExternal validation set:\")\n",
    "    print(extval_data_class_counts)\n",
    "    print(extval_data_class_percentages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae2697b",
   "metadata": {},
   "source": [
    "##### Function to evaluate models and generate ROC curve, PR curve and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8ae89c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a function for bootstrap sampling\n",
    "def bootstrap_sample(data, n_samples):\n",
    "    np.random.seed(SEED)\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    return data[indices]\n",
    "\n",
    "def calculate_confidence_interval(metric_values, alpha=0.95):\n",
    "    # Filter out NaN values from metric_values\n",
    "    non_nan_values = metric_values[~np.isnan(metric_values)]\n",
    "    \n",
    "    # Check if there are non-NaN values to calculate the confidence interval\n",
    "    if len(non_nan_values) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Calculate confidence intervals for non-NaN values\n",
    "    lower_bound = np.percentile(non_nan_values, (1 - alpha) / 2 * 100)\n",
    "    upper_bound = np.percentile(non_nan_values, (1 + alpha) / 2 * 100)\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def evaluate_and_plot_model(model, testset, y_test, filename, class_labels = class_labels_display, threshold=0.5, bootstrap_samples=1000, min_positive_instances=1):\n",
    "\n",
    "\n",
    "    bootstrap_values = []\n",
    "\n",
    "    for _ in range(bootstrap_samples):\n",
    "        # Perform bootstrap sampling\n",
    "        bootstrap_sample_indices = np.random.choice(len(testset), len(testset), replace=True)\n",
    "        bootstrap_sample_testset = testset.iloc[bootstrap_sample_indices]\n",
    "        bootstrap_sample_y_test = y_test.iloc[bootstrap_sample_indices]\n",
    "\n",
    "        if isinstance(model, (cb.CatBoostClassifier, lgb.LGBMClassifier, GaussianNB,RandomForestClassifier, LogisticRegression, RandomForestClassifier, HistGradientBoostingClassifier)):\n",
    "            predictions = model.predict_proba(bootstrap_sample_testset)[:, 1]\n",
    "            # print(predictions)\n",
    "        else:\n",
    "            predictions = model.predict(bootstrap_sample_testset)\n",
    "\n",
    "        predictions_class = [True if x >= threshold else False for x in predictions]\n",
    "\n",
    "        # Check if the number of positive instances is below the threshold\n",
    "        if np.sum(bootstrap_sample_y_test) < min_positive_instances:\n",
    "            # Set metrics to NaN or another suitable value\n",
    "            PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, roc_auc, pr_auc, brier_score, f1 = [np.nan] * 10\n",
    "        else:\n",
    "            cm = confusion_matrix(bootstrap_sample_y_test, predictions_class)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "            PPV = tp / (tp + fp)\n",
    "            NPV = tn / (tn + fn)\n",
    "            sensitivity = tp / (tp + fn)\n",
    "            specificity = tn / (tn + fp)\n",
    "            balanced_accuracy = (sensitivity + specificity) / 2\n",
    "            MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "            roc_auc = roc_auc_score(y_true=bootstrap_sample_y_test, y_score=predictions)\n",
    "            brier_score = brier_score_loss(y_true=bootstrap_sample_y_test, y_prob=predictions, pos_label=True)\n",
    "            precision, recall, _ = precision_recall_curve(y_true=bootstrap_sample_y_test, probas_pred=predictions, pos_label=True)\n",
    "            pr_auc = metrics.auc(x=recall, y=precision)\n",
    "            f1 = f1_score(y_test, predictions_class)\n",
    "\n",
    "        # Store the metric values for each bootstrap iteration\n",
    "        bootstrap_values.append([PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, roc_auc, pr_auc, brier_score, f1])\n",
    "\n",
    "\n",
    "    # Convert the list of metric values into a numpy array for easier manipulation\n",
    "    bootstrap_values = np.array(bootstrap_values)\n",
    "\n",
    "    # Calculate confidence intervals for each metric\n",
    "    lower_bounds, upper_bounds = zip(*[calculate_confidence_interval(bootstrap_values[:, i]) for i in range(bootstrap_values.shape[1])])\n",
    "\n",
    "    # Calculate the measures for the whole testset\n",
    "    if np.sum(y_test) < min_positive_instances:\n",
    "        # Set metrics to NaN or another suitable value\n",
    "        PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, roc_auc, pr_auc, brier_score, f1 = [np.nan] * 10\n",
    "    else:\n",
    "        if isinstance(model, (cb.CatBoostClassifier, lgb.LGBMClassifier, GaussianNB, RandomForestClassifier, LogisticRegression, HistGradientBoostingClassifier)):\n",
    "            predictions = model.predict_proba(testset)[:, 1]\n",
    "            # print(predictions)\n",
    "        else:\n",
    "            predictions = model.predict(testset)\n",
    "\n",
    "        predictions_class = [True if x >= threshold else False for x in predictions]\n",
    "\n",
    "        cm = confusion_matrix(y_test, predictions_class)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        PPV = tp / (tp + fp)\n",
    "        NPV = tn / (tn + fn)\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "        roc_auc = roc_auc_score(y_true=y_test, y_score=predictions)\n",
    "        brier_score = brier_score_loss(y_true=y_test, y_prob=predictions, pos_label=True)\n",
    "        precision, recall, _ = precision_recall_curve(y_true=y_test, probas_pred=predictions, pos_label=True)\n",
    "        pr_auc = metrics.auc(x=recall, y=precision)\n",
    "        f1 = f1_score(y_test, predictions_class)\n",
    "\n",
    "    # Convert the list of metric values into a numpy array for easier manipulation\n",
    "    bootstrap_values = np.array(bootstrap_values)\n",
    "\n",
    "    # Calculate confidence intervals for each metric\n",
    "    lower_bounds, upper_bounds = zip(*[calculate_confidence_interval(bootstrap_values[:, i]) for i in range(bootstrap_values.shape[1])])\n",
    "\n",
    "    # Calculate the measures for the whole testset\n",
    "    results = {\n",
    "        'Metric': ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'ROCAUC', 'PRAUC', 'Brier Score', 'F1 Score'],\n",
    "        'Value': [PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, roc_auc, pr_auc, brier_score, f1],\n",
    "        'Lower Bound': lower_bounds,\n",
    "        'Upper Bound': upper_bounds\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['Value'] = results_df['Value'].round(2)\n",
    "    results_df['Lower Bound'] = results_df['Lower Bound'].round(2)\n",
    "    results_df['Upper Bound'] = results_df['Upper Bound'].round(2)\n",
    "    print(results_df)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions, pos_label=True, drop_intermediate=False)\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_test, predictions, pos_label=True)\n",
    "    \n",
    "    # Find missclassified samples\n",
    "    predictions_class = [True if x >= threshold else False for x in predictions]\n",
    "    missclassified_samples = y_test.index[np.where(y_test != predictions_class)[0]]\n",
    "\n",
    "    # Finding the index closest to the custom threshold instead of 0.5\n",
    "    threshold_index = (np.abs(thresholds - threshold)).argmin()\n",
    "    threshold_custom = thresholds[threshold_index]\n",
    "    tpr_custom = tpr[threshold_index]\n",
    "    fpr_custom = fpr[threshold_index]\n",
    "\n",
    "    pr_threshold_index = (np.abs(pr_thresholds - threshold)).argmin()\n",
    "    pr_threshold_custom = pr_thresholds[pr_threshold_index]\n",
    "    precision_custom = precision[pr_threshold_index]\n",
    "    recall_custom = recall[pr_threshold_index]\n",
    "\n",
    "    def display_confusion_matrix(y_true, y_pred, labels):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot(cmap='Blues', ax=ax3, xticks_rotation='vertical', values_format='d')\n",
    "        ax3.set_xticklabels(ax3.get_xticklabels(), fontsize=8)\n",
    "        ax3.set_yticklabels(ax3.get_yticklabels(), fontsize=8)\n",
    "        plt.xlabel('Predicted', fontsize=8)\n",
    "        plt.ylabel('True', fontsize=8)\n",
    "        ax3.legend(fontsize=8)\n",
    "        ax3.invert_yaxis()\n",
    "  \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "    \n",
    "    ax1.set_facecolor('white')\n",
    "    # show both grid lines\n",
    "    ax1.grid(which='both', color = \"grey\")\n",
    "    # modify grid lines:\n",
    "    ax1.grid(which='minor', alpha=0.1)\n",
    "    ax1.grid(which='major', alpha=0.2)\n",
    "    ax1.plot(fpr, tpr, color='blue', label='ROC AUC ≈ {:.2f}'.format(roc_auc))\n",
    "    ax1.plot([0, 1], [0, 1], color='black', linestyle='--', linewidth=0.5, label='chance level')\n",
    "    ax1.scatter(fpr_custom, tpr_custom, color='red', label=f'Threshold = {threshold_custom:.2f}', s=50, marker='x')\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, 1.1])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=8)\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=8)\n",
    "    ax1.set_title('ROC curve', fontsize=8)\n",
    "    ax1.legend(loc=\"lower right\", fontsize=8)\n",
    "    ax1.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)  \n",
    "\n",
    "    \n",
    "\n",
    "    chance_level_precision = np.sum(y_test) / len(y_test)\n",
    "    ax2.set_facecolor('white')\n",
    "    # show both grid lines\n",
    "    ax2.grid(which='both', color = \"grey\")\n",
    "    # modify grid lines:\n",
    "    ax2.grid(which='minor', alpha=0.1)\n",
    "    ax2.grid(which='major', alpha=0.2)\n",
    "    ax2.plot(recall, precision, color='green', label='PR AUC ≈ {:.2f}'.format(pr_auc))\n",
    "    ax2.scatter(recall_custom, precision_custom, color='orange', label=f'Threshold = {pr_threshold_custom:.2f}', s=50, marker='x')\n",
    "    ax2.axhline(y=chance_level_precision, color='black', linestyle='--', label='chance level', linewidth=0.5)\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, 1.1])\n",
    "    ax2.set_xlabel('Recall', fontsize=8)\n",
    "    ax2.set_ylabel('Precision', fontsize=8)\n",
    "    ax2.set_title('Precision-Recall curve', fontsize=8)\n",
    "    ax2.legend(loc=\"upper right\", fontsize=8)\n",
    "    ax2.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)  \n",
    "\n",
    "\n",
    "    # ax2.legend(loc=\"lower left\", fontsize=8)\n",
    "\n",
    "    display_confusion_matrix(y_test, predictions_class, labels=class_labels_display)\n",
    "    ax3.set_title('Confusion matrix', fontsize=8)\n",
    "    plt.grid(False)\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.size'] = 8\n",
    "\n",
    "    plt.savefig(filename, dpi=300)\n",
    "\n",
    "    print(f'Threshold closest to {threshold} (ROC): {threshold_custom:.2f}')\n",
    "    print(f'Threshold closest to {threshold} (PR): {pr_threshold_custom:.2f}')\n",
    "\n",
    "    return results_df, missclassified_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47cc7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c15752d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert outcome variable to boolean\n",
    "outcome_mapping = {class_1: True, class_0: False}\n",
    "y_train = y_train.replace(outcome_mapping).astype(bool)\n",
    "y_test = y_test.replace(outcome_mapping).astype(bool)\n",
    "if external_val:\n",
    "    y_extval_data = y_extval_data.replace(outcome_mapping).astype(bool)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c9a25",
   "metadata": {},
   "source": [
    "## Initiate machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407127b5",
   "metadata": {},
   "source": [
    "##### Variable type encoding for QLattice model (only required for QLattice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2b7bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dictionary to store the stypes\n",
    "stypes = {}\n",
    "\n",
    "# iterate over each column in the dataset\n",
    "for col in mydata_imputed.columns:\n",
    "    # check if the column dtype is 'category'\n",
    "    if pd.api.types.is_categorical_dtype(mydata_imputed[col]):\n",
    "        # if it is, add the column name to the stypes dictionary with a value of 'c'\n",
    "        stypes[col] = 'c'\n",
    "\n",
    "stypes[outcome_var] = 'b'\n",
    "# print the stypes dictionary\n",
    "print(stypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f47b6e",
   "metadata": {},
   "source": [
    "#### Set model weights based on class balance from the development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b7e5eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weights = compute_sample_weight(class_weight='balanced', y=mydata_imputed[outcome_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac27f33c",
   "metadata": {},
   "source": [
    "#### Define the parameter grid for random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "420be6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is done when hyperparameter tuning is done\n",
    "def adjust_hyperparameters(n_rows, n_cols):\n",
    "    np.random.seed(SEED)\n",
    "    # Adjust hyperparameters based on dataset size and class proportion\n",
    "    # Random Forest Classifier parameters:\n",
    "    adjusted_rf_param_dist = {\n",
    "        # Number of trees in the forest\n",
    "        \"n_estimators\": np.linspace(100, max(1000, int(np.sqrt(n_rows))), num=10, dtype=int),\n",
    "        # Maximum depth of the tree\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, n_cols],\n",
    "        # Minimum number of samples required to split a node\n",
    "        'min_samples_split': [2, 5, 10, int(np.sqrt(n_rows))],\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        'min_samples_leaf': [1, 2, 4, int(np.sqrt(n_rows))],\n",
    "        # The number of features to consider when looking for the best split\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    }\n",
    "    \n",
    "    # LightGBM Classifier parameters:\n",
    "    adjusted_lgbm_param_dist = {\n",
    "        # Maximum number of leaves in one tree\n",
    "        \"num_leaves\": np.linspace(6, 100, num=10, dtype=int),\n",
    "        # Minimum number of data needed in a child (leaf) node\n",
    "        'min_child_samples': randint(4, min(int(np.sqrt(n_rows)), 100), size = 10),\n",
    "        # Minimum sum of instance weight (hessian) needed in a child (leaf) node\n",
    "        'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "        # Subsample ratio of the training instance\n",
    "        'subsample': np.linspace(0.5, 0.8, 5),\n",
    "        # Subsample ratio of columns when constructing each tree\n",
    "        'colsample_bytree': np.linspace(0.4, 0.6, 5),\n",
    "        # L1 regularization term on weights\n",
    "        'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "        # L2 regularization term on weights\n",
    "        'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "        # Number of boosting iterations\n",
    "        'n_estimators': np.linspace(100, max(1000, int(np.sqrt(n_rows))), num=10, dtype=int),\n",
    "        # Maximum depth of tree\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10]\n",
    "    }\n",
    "\n",
    "    # Histogram-Based Gradient Boosting Classifier parameters:\n",
    "    adjusted_hgbc_param_dist = {\n",
    "        # maximum iterations (number of trees)\n",
    "        \"max_iter\": np.linspace(100, max(1000, int(np.sqrt(n_rows))), num=10, dtype=int),\n",
    "        # validation data proportion\n",
    "        \"validation_fraction\": np.linspace(0.1, 0.3, 10),\n",
    "        # Boosting learning rate\n",
    "        'learning_rate': np.linspace(0.01, 0.2, 10),\n",
    "        # Maximum depth of the individual estimators\n",
    "        'max_depth': [3, 4, 5, 6, 7, 8, 9, 10, n_cols],\n",
    "        # Minimum number of samples per leaf\n",
    "        'min_samples_leaf': randint(1, min(5,n_rows), size = 10),\n",
    "        # Grow trees with max_leaf_nodes in best-first fashion\n",
    "        'max_leaf_nodes': randint(10, 100, size = 10),\n",
    "        # L2 regularization term on weights\n",
    "        'l2_regularization': np.linspace(0.01, 0.2, 10)\n",
    "    }\n",
    "    \n",
    "    # CatBoost Classifier parameters:\n",
    "    adjusted_cb_param_dist = {\n",
    "        # Learning rate\n",
    "        'learning_rate': np.logspace(-3, 0, num=10),\n",
    "        # Depth of the trees\n",
    "        'depth': [3, 4, 5, 6, 7, 8, 9, 10, n_cols],\n",
    "        # L2 regularization coefficient\n",
    "        'l2_leaf_reg': np.logspace(-1, 3, num=100),\n",
    "        # The number of trees to fit\n",
    "        'iterations': np.linspace(100, max(1000, int(np.sqrt(n_rows))), num=10, dtype=int),\n",
    "        # Subsample ratio of the training instance\n",
    "        'subsample': np.linspace(0.1, 1, 10),\n",
    "        # Random strength\n",
    "        'random_strength': np.linspace(0, 10, 10)\n",
    "    }\n",
    "\n",
    "    # Logistic Regression parameters:\n",
    "    adjusted_lr_param_dist = {\n",
    "        # Inverse of regularization strength\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        # Maximum number of iterations for optimization\n",
    "        'max_iter': [500, 1000, 2000, 5000, int(np.sqrt(n_rows))], \n",
    "        # Tolerance for stopping criteria\n",
    "        'tol': [1e-3, 1e-4, 1e-5]\n",
    "    }\n",
    "\n",
    "    return adjusted_rf_param_dist, adjusted_lgbm_param_dist, adjusted_hgbc_param_dist, adjusted_cb_param_dist, adjusted_lr_param_dist\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48568a9",
   "metadata": {},
   "source": [
    "#### Set parameters for models (when the datset is small)\n",
    "\n",
    "This is used when there is no hyperparameter tuning. The parameters are set according to the data characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ff18d2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is done when hyperparameter tuning is not done\n",
    "def set_parameters(n_rows, n_cols, class_proportion):\n",
    "    # Balanced Random Forest Classifier parameters:\n",
    "    rf_params = {\n",
    "        # Number of trees in the forest\n",
    "        'n_estimators': max(100, int(n_rows / 100)),\n",
    "        # Maximum depth of the tree\n",
    "        'max_depth': 10,\n",
    "        # Minimum number of samples required to split a node\n",
    "        'min_samples_split': 2 if class_proportion > 0.1 else 10,\n",
    "        # Minimum number of samples required at each leaf node\n",
    "        'min_samples_leaf': 1 if class_proportion > 0.1 else 4,\n",
    "        # The number of features to consider when looking for the best split\n",
    "        'max_features': 'sqrt'\n",
    "    }\n",
    "\n",
    "    # LightGBM Classifier parameters:\n",
    "    lgbm_params = {\n",
    "        # Maximum number of leaves in one tree\n",
    "        'num_leaves': min(6, min(50, 2*n_rows)),\n",
    "        # Minimum number of data needed in a child (leaf) node\n",
    "        'min_child_samples': min(100, int(n_rows / 20)),\n",
    "        # Minimum sum of instance weight (hessian) needed in a child (leaf) node\n",
    "        'min_child_weight': 1e-3,\n",
    "        # Subsample ratio of the training instance\n",
    "        'subsample': min(0.8, max(0.2, 0.5 + (class_proportion - 0.5) / 2)),\n",
    "        # Subsample ratio of columns when constructing each tree\n",
    "        'colsample_bytree': 0.8,\n",
    "        # L1 regularization term on weights\n",
    "        'reg_alpha': 0.01,\n",
    "        # L2 regularization term on weights\n",
    "        'reg_lambda': 0.01,\n",
    "        # Number of boosting iterations\n",
    "        'n_estimators': min(1000, 2*n_rows),\n",
    "        # Maximum depth of tree\n",
    "        'max_depth': 10\n",
    "    }\n",
    "\n",
    "    # Histogram-Based Gradient Boosting Classifier parameters:\n",
    "    hgbc_params = {\n",
    "        # maximum iterations (number of trees)\n",
    "        \"max_iter\":min(1000, 2*n_rows),\n",
    "        # Boosting learning rate\n",
    "        'learning_rate': 0.1,\n",
    "        # Maximum depth of the individual estimators\n",
    "        'max_depth': 10,\n",
    "        # Minimum number of samples per leaf\n",
    "        'min_samples_leaf': 2,\n",
    "        # Grow trees with max_leaf_nodes in best-first fashion\n",
    "        'max_leaf_nodes': 3,\n",
    "        # L2 regularization term on weights\n",
    "        'l2_regularization': 0.1\n",
    "    }\n",
    "\n",
    "    # CatBoost Classifier parameters:\n",
    "    cb_params = {\n",
    "        # Learning rate\n",
    "        'learning_rate': 0.1,\n",
    "        # Depth of the trees\n",
    "        'depth': min(10, int(n_cols/2)),\n",
    "        # L2 regularization coefficient\n",
    "        'l2_leaf_reg': 1.0,\n",
    "        # The number of trees to fit\n",
    "        'iterations': min(1000, 2*n_rows),\n",
    "        # Subsample ratio of the training instance\n",
    "        'subsample': 0.8,\n",
    "        # Random strength\n",
    "        'random_strength': 5\n",
    "    }\n",
    "\n",
    "    # Logistic Regression parameters:\n",
    "    lr_params = {\n",
    "        # Inverse of regularization strength\n",
    "        'C': 1.0,\n",
    "        # Maximum number of iterations for optimization\n",
    "        'max_iter': 1000,\n",
    "        # Tolerance for stopping criteria\n",
    "        'tol': 1e-4\n",
    "    }\n",
    "\n",
    "    return rf_params, lgbm_params, hgbc_params, cb_params, lr_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b671232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PFI_median_wrap(PFI_folds):\n",
    "    \"\"\"\n",
    "    Computes the median importance across all folds and normalizes the importance values.\n",
    "\n",
    "    Parameters:\n",
    "    PFI_folds (list of feature importance values in folds): List of DataFrames where each DataFrame contains 'Feature' and 'Importance' columns.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with 'Feature' and normalized 'Importance' sorted by importance.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of folds\n",
    "    num_folds = len(PFI_folds)\n",
    "    \n",
    "    # Start with the 'Feature' column from the first DataFrame\n",
    "    merged_df = PFI_folds[0][['Feature']].copy()\n",
    "    \n",
    "    # Loop through each fold and add the 'Importance' column to the DataFrame\n",
    "    for i in range(num_folds):\n",
    "        fold_column = PFI_folds[i][['Importance']].rename(columns={'Importance': f'Importance Fold {i+1}'})\n",
    "        merged_df = merged_df.merge(fold_column, left_index=True, right_index=True)\n",
    "    \n",
    "    # Calculate the median of importance values for each feature\n",
    "    importance_columns = [f'Importance Fold {i+1}' for i in range(num_folds)]\n",
    "    merged_df['Importance'] = merged_df[importance_columns].median(axis=1)\n",
    "    \n",
    "    PFI_median = merged_df.copy()\n",
    "    # # Select only the 'Feature' and 'Importance' columns\n",
    "    # PFI_median = merged_df[['Feature', 'Importance']]\n",
    "    \n",
    "    # Sort the DataFrame by 'Importance' in descending order\n",
    "    PFI_median = PFI_median.sort_values(by='Importance', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    # Normalize the 'Importance' column\n",
    "    PFI_median['Importance'] = minmax_scaler.fit_transform(PFI_median[['Importance']])\n",
    "    \n",
    "    return PFI_median\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "58088bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_PFI(PFI_folds, X, model_name):\n",
    "    \"\"\"\n",
    "    Plot permutation-based feature importances (PFI) from multiple folds using a strip plot.\n",
    "\n",
    "    Parameters:\n",
    "    - PFI_folds (list of DataFrames): List where each DataFrame contains 'Feature' and 'Importance' columns for each fold.\n",
    "    - X (DataFrame): DataFrame used to determine the number of features for plot sizing.\n",
    "    - model_name (str): A string representing the name of the model or experiment, used for naming the output files.\n",
    "\n",
    "    Output:\n",
    "    - Saves the plot with filenames including the model_name parameter and displays it.\n",
    "    \"\"\"\n",
    "    # Combine feature importances from all folds into a single DataFrame\n",
    "    combined_importances = PFI_median_wrap(PFI_folds)\n",
    "    median_importance = combined_importances[['Feature', 'Importance']]\n",
    "    \n",
    "    # Plot boxplot for feature importances\n",
    "    plt.figure(figsize=(5, 0.5 * X.shape[1]))\n",
    "    sns.stripplot(x=\"Importance\", y=\"Feature\", data=pd.concat(PFI_folds, axis=0), order=median_importance['Feature'], jitter=True, alpha=0.5)\n",
    "    plt.title(f\"Fold-wise mean permutation importances for {model_name}\", size=10)\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.grid(linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', labelsize=8) \n",
    "    plt.gca().set_facecolor('white')  \n",
    "    # Display grid lines\n",
    "    plt.grid(which='both', color=\"grey\")\n",
    "    # Modify grid lines\n",
    "    plt.grid(which='minor', alpha=0.1)\n",
    "    plt.grid(which='major', alpha=0.3)\n",
    "    # Add a dotted line at x = 0\n",
    "    plt.axvline(x=0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "    \n",
    "    # Save plot with model_name in the filename\n",
    "    plt.savefig(f\"FI_perm_{model_name}.tif\", bbox_inches='tight')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a0c8f28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_TFI(X, tree_FI, model_name):\n",
    "    \"\"\"\n",
    "    Plots the tree-based feature importances for a given model.\n",
    "\n",
    "    Parameters:\n",
    "    X (pd.DataFrame): The training data used for feature names.\n",
    "    tree_FI (list of pd.Series): List of feature importance scores for tree-based feature importance from each fold.\n",
    "    model_name (str): Name of the model to use in the plot title and filenames.\n",
    "\n",
    "    Returns:\n",
    "    None: Displays and saves the plot.\n",
    "    \"\"\"\n",
    "    # Extract feature names from X\n",
    "    feature_names = X.columns\n",
    "\n",
    "    # Combine feature importances from all folds into a single DataFrame\n",
    "    combined_importances = pd.concat(\n",
    "        [pd.DataFrame({'Feature': feature_names, 'Importance': fold}) for fold in tree_FI],\n",
    "        axis=0, ignore_index=True\n",
    "    )\n",
    "\n",
    "    # Calculate the median importance across folds for each feature\n",
    "    median_importance = combined_importances.groupby(\"Feature\")[\"Importance\"].median().reset_index()\n",
    "\n",
    "    # Sort features by their median importance\n",
    "    median_importance = median_importance.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "    # Plot feature importances\n",
    "    plt.figure(figsize=(5, 0.5 * X_train.shape[1]))\n",
    "    sns.stripplot(x=\"Importance\", y=\"Feature\", data=combined_importances, \n",
    "                  order=median_importance['Feature'], jitter=True, alpha=0.5)\n",
    "    plt.title(f\"Mean tree-based feature importances per fold ({model_name})\", size=10)\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Feature\")\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.grid(linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "    plt.axvline(x=0, color=\"k\", linestyle=\"--\", linewidth=1)\n",
    "    plt.grid(True)\n",
    "    plt.tick_params(axis='both', labelsize=8)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.grid(which='both', color=\"grey\")\n",
    "    plt.grid(which='minor', alpha=0.1)\n",
    "    plt.grid(which='major', alpha=0.3)\n",
    "    \n",
    "    # Save the plot to files\n",
    "    plt.savefig(f\"FI_tree_{model_name}.tif\", bbox_inches='tight')\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f7e0249",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_summary_plot(shap_values, data, model_name):\n",
    "    \"\"\"\n",
    "    Generates and saves a SHAP summary plot based on provided SHAP values and data from cross validation\n",
    "    \n",
    "    Parameters:\n",
    "    - shap_values: concatenated list of SHAP values arrays from different folds\n",
    "    - data: DataFrames (trainset or testset)\n",
    "    - model_name: Name of the model (e.g., \"CB\" for CatBoost)\n",
    "    \n",
    "    Returns:\n",
    "    - None: Saves the plot as a .tif file and displays it.\n",
    "    \"\"\"\n",
    "   \n",
    "    # Create the SHAP summary plot\n",
    "    shap.summary_plot(\n",
    "        shap_values, \n",
    "        data, \n",
    "        color=plt.get_cmap(\"viridis\"), \n",
    "        show=False, \n",
    "        alpha = 0.8, max_display=top_n_f\n",
    "    )\n",
    "    \n",
    "    # Customize the plot appearance\n",
    "    plt.gcf().axes[-1].tick_params(labelsize=8)\n",
    "    plt.grid(linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.yticks(size=8)\n",
    "    plt.xticks(size=8)\n",
    "    plt.xlabel('SHAP value', size=8)\n",
    "    plt.ylabel('feature', size=8)\n",
    "    fig, ax = plt.gcf(), plt.gca()\n",
    "    fig.axes[-1].set_ylabel('feature value', size=8)\n",
    "    plt.xticks(size=8)\n",
    "    plt.grid(True)\n",
    "    plt.gca().tick_params(axis='both', labelsize=8)\n",
    "    plt.gca().set_facecolor('white')\n",
    "    \n",
    "    # Display grid lines\n",
    "    plt.grid(which='both', color=\"grey\")\n",
    "    plt.grid(which='minor', alpha=0.1)\n",
    "    plt.grid(which='major', alpha=0.2)\n",
    "    \n",
    "    # Save and display the plot\n",
    "    plot_filename = f\"SHAP_summary_plot_{model_name}.tif\"\n",
    "    plt.savefig(plot_filename, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if there are categorical features and generate a plot with categories if needed\n",
    "    if 'category' in data.dtypes.values:\n",
    "        feature_names_with_shapvalues = [\n",
    "            f\"{feature}: {round(value, 2)}\"\n",
    "            for feature, value in zip(data.columns, np.mean(np.abs(shap_values), axis=0))\n",
    "        ]\n",
    "        # Call the custom function to create the SHAP summary plot with categories\n",
    "        categorical_shap_plot(\n",
    "            shap_values=shap_values, \n",
    "            data=data,\n",
    "            top_n=min(len(feature_names_with_shapvalues),top_n_f),\n",
    "            jitter=0.1\n",
    "        )\n",
    "        \n",
    "        # Customize the second plot appearance\n",
    "        plt.gca().set_facecolor('white')\n",
    "        plt.grid(which='both', color=\"grey\")\n",
    "        plt.grid(which='minor', alpha=0.2)\n",
    "        plt.grid(which='major', alpha=0.3)\n",
    "        \n",
    "        # Save the plot with categories\n",
    "        plot_filename_with_cats = f\"SHAP_summary_plot_{model_name}_withcats.tif\"\n",
    "        plt.savefig(plot_filename_with_cats, dpi = 300)\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627c767",
   "metadata": {},
   "source": [
    "#### Cross validation\n",
    "\n",
    "This block contains the code to perform cross-validation for all selected binary classification models. The code calculates performance measures for all models, applies hyperparameter tuning and training, and generates visualizations for feature importance using various approaches (e.g., SHAP, feature permutation, tree-based feature importance). Additionally, it produces performance metrics such as ROC and PR curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7bc8ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate evaluation metrics\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred, labels= [False, True])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    # Positive Predictive Value (Precision)\n",
    "    PPV = tp / (tp + fp)\n",
    "    # Negative Predictive Value\n",
    "    NPV = tn / (tn + fn)\n",
    "    # True Positive Rate (Recall)\n",
    "    Sensitivity = tp / (tp + fn)\n",
    "    # True Negative Rate\n",
    "    Specificity = tn / (tn + fp)\n",
    "    Balanced_Accuracy = (Sensitivity + Specificity) / 2 # Balanced Accuracy\n",
    "    MCC = matthews_corrcoef(y_true, y_pred) # Matthews Correlation Coefficient\n",
    "    ROC_AUC = roc_auc_score(y_true, y_pred_proba) # ROC AUC Score\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba, pos_label=True)\n",
    "    PR_AUC = auc(recall, precision) # Precision-Recall AUC Score\n",
    "    Brier_Score = brier_score_loss(y_true, y_pred_proba, pos_label=True) # Brier Score\n",
    "    F1_Score = f1_score(y_true, y_pred) # F1 Score\n",
    "    return {\n",
    "        'PPV': PPV,\n",
    "        'NPV': NPV,\n",
    "        'Sensitivity': Sensitivity,\n",
    "        'Specificity': Specificity,\n",
    "        'Balanced Accuracy': Balanced_Accuracy,\n",
    "        'MCC': MCC,\n",
    "        'ROCAUC': ROC_AUC,\n",
    "        'PRAUC': PR_AUC,\n",
    "        'Brier Score': Brier_Score,\n",
    "        'F1 Score': F1_Score\n",
    "    }\n",
    "# Function to cross-validate the model\n",
    "def cross_validate_model(model_class, X, y, sample_weights=None, n_splits=cv_folds, random_state=SEED, measures=None,\n",
    "                         use_default_threshold=False, **model_params):\n",
    "    n_repeats = n_rep_feature_permutation # number of repetitions for feature permutation\n",
    "    if measures is None:\n",
    "        measures = ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'ROCAUC', 'PRAUC', 'Brier Score', 'F1 Score']\n",
    "    fold_data = [] # to save the data that are split by folds for subsequent analyses\n",
    "    y_fold_data = []\n",
    "    fold_results = pd.DataFrame()\n",
    "    fold_results_plt = pd.DataFrame()\n",
    "    aggregated_thr = np.array([]) # aggregated list of estimated optimal thresholds per fold\n",
    "    aggregated_predictions = np.array([])\n",
    "    aggregated_labels = np.array([])\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "    feature_importance_list = []\n",
    "    treebased_feature_importance_list = []\n",
    "    shap_values_list = []  # To store SHAP values for each fold\n",
    "    missclassified_samples = [] # to store the index of missclassified samples\n",
    "    ########\n",
    "    overlapping_samples = False\n",
    "    test_indices_list = []  # To store test indices of samples in each fold\n",
    "    ########\n",
    "    predictions_proba_fold_list = []\n",
    "    predictions_proba_fold_train_list = []\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Check for overlapping test samples across previous folds\n",
    "        if fold > 1:\n",
    "            current_test_index = set(test_index)\n",
    "            for prev_test_index in test_indices_list:\n",
    "                if not current_test_index.isdisjoint(prev_test_index):\n",
    "                    overlapping_samples = True\n",
    "                    break\n",
    "        # Store test indices of samples in this fold\n",
    "        test_indices_list.append(set(test_index))\n",
    "        #####################\n",
    "        sample_weights_fold = sample_weights[train_index] if sample_weights is not None else None\n",
    "        # Calculate necessary information for adjusting hyperparameters\n",
    "        n_rows = X_train_fold.shape[0]\n",
    "        n_cols = X_train_fold.shape[1]\n",
    "        class_proportion = y_train_fold.mean()  # binary classification\n",
    "        # Adjust hyperparameters based on the training data in this fold\n",
    "        rf_param_dist, lgbm_param_dist, hgbc_param_dist, cb_param_dist, lr_param_dist = adjust_hyperparameters(n_rows, n_cols)\n",
    "        rf_params, lgbm_params, hgbc_params, cb_params, lr_params = set_parameters(n_rows, n_cols, class_proportion)\n",
    "        # Check if the model is a RandomForestClassifier\n",
    "        if model_class == RandomForestClassifier:\n",
    "            # Explicitly set sampling_strategy to 'all'\n",
    "            rf_model = RandomForestClassifier(random_state=SEED, n_jobs=n_cpu_model_training, **rf_params) # , class_weight = 'balanced'\n",
    "            # if hyperparameter tuning should be done or not\n",
    "            if hp_tuning:\n",
    "                random_search = RandomizedSearchCV(\n",
    "                estimator=rf_model, \n",
    "                param_distributions=rf_param_dist, \n",
    "                n_iter=n_iter_hptuning,\n",
    "                scoring= custom_scorer,  \n",
    "                cv=cv_folds_hptuning,\n",
    "                refit=True, \n",
    "                random_state=random_state,\n",
    "                verbose=0, \n",
    "                n_jobs = n_cpu_for_tuning)\n",
    "                # Fit the RandomizedSearchCV object to the data\n",
    "                random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "                # Get the best parameters and best estimator from the random search\n",
    "                best_params = random_search.best_params_\n",
    "                rf_model = RandomForestClassifier(random_state=SEED,n_jobs=n_cpu_model_training, **best_params) # , class_weight = 'balanced' (note: class weight should not be used simultaneously with sample weight, it messes up the sample weighting)\n",
    "            else:\n",
    "                rf_model = RandomForestClassifier(random_state=SEED, n_jobs=n_cpu_model_training, **rf_params) # , class_weight = 'balanced'\n",
    "            # Fit the best estimator on the entire training data\n",
    "            rf_model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = rf_model.predict_proba(X_test_fold)[:, 1]\n",
    "            treebased_feature_importance = rf_model.feature_importances_\n",
    "            treebased_feature_importance_list.append(treebased_feature_importance)\n",
    "            # Use permutation_importance to get feature importances\n",
    "            perm_result = permutation_importance(\n",
    "                rf_model, X_test_fold, y_test_fold, n_repeats=n_repeats, random_state=random_state, n_jobs=n_cpu_model_training, scoring = custom_scorer # \"roc_auc\"\n",
    "            )\n",
    "            # Get feature importances and sort them\n",
    "            feature_importance = perm_result.importances_mean # Mean of feature importance over n_repeats\n",
    "            feature_importance_df = pd.DataFrame(\n",
    "                {\"Feature\": X_train_fold.columns, \"Importance\": feature_importance}\n",
    "            )\n",
    "            feature_importance_df = feature_importance_df.sort_values(\n",
    "                by=\"Importance\", ascending=False\n",
    "            )\n",
    "            # Append to the list\n",
    "            feature_importance_list.append(feature_importance_df)\n",
    "            # Compute SHAP values\n",
    "            explainer = shap.TreeExplainer(rf_model)\n",
    "            shap_values = explainer.shap_values(X_test_fold)\n",
    "            shap_values_list.append(shap_values)\n",
    "            fold_data.append(X_test_fold) # for subsequent SHAP vs feature value analyses\n",
    "            y_fold_data.append(y_test_fold)\n",
    "            \n",
    "            ############\n",
    "            predictions_proba_fold = rf_model.predict_proba(X_test_fold)[:, 1]\n",
    "            predictions_proba_fold_train = rf_model.predict_proba(X_train_fold)[:, 1]\n",
    "            predictions_proba_fold_list.append(predictions_proba_fold)\n",
    "            predictions_proba_fold_train_list.append(predictions_proba_fold_train)\n",
    "            # Check if the model is a QLattice\n",
    "        elif model_class == 'QLattice':\n",
    "            X_train_fold_ql = X_train_fold.copy()\n",
    "            X_train_fold_ql[outcome_var] = y_train_fold.values\n",
    "            if hp_tuning:\n",
    "                best_composite_score = 0\n",
    "                best_parameters = {'n_epochs': 50, 'max_complexity': 10}\n",
    "                def evaluate_params(n_epochs, max_complexity):\n",
    "                    ql = feyn.QLattice(random_seed=random_state)\n",
    "                    models = ql.auto_run(\n",
    "                        data=X_train_fold_ql,\n",
    "                        output_name=outcome_var,\n",
    "                        kind='classification',\n",
    "                        n_epochs=n_epochs,\n",
    "                        stypes=stypes,\n",
    "                        criterion=\"aic\",\n",
    "                        loss_function='binary_cross_entropy',\n",
    "                        max_complexity=max_complexity,\n",
    "                        sample_weights=sample_weights_fold\n",
    "                    )\n",
    "                    best_model = models[0]\n",
    "                    predictions_proba = best_model.predict(X_test_fold)\n",
    "                    QL_composite_score = (roc_auc_score(y_true = y_test_fold, y_score = predictions_proba) +\n",
    "                                          average_precision_score(y_true = y_test_fold, y_score = predictions_proba))/2\n",
    "                    return QL_composite_score, {'n_epochs': n_epochs, 'max_complexity': max_complexity}\n",
    "                results = Parallel(n_jobs=n_cpu_for_tuning, backend='loky')(\n",
    "                    delayed(evaluate_params)(n_epochs, max_complexity)\n",
    "                    for n_epochs in [50, 100]\n",
    "                    for max_complexity in [5, 10]\n",
    "                )\n",
    "                for QL_composite_score, params in results:\n",
    "                    if QL_composite_score > best_composite_score:\n",
    "                        best_composite_score = QL_composite_score\n",
    "                        best_parameters = params\n",
    "                print(\"Best Parameters:\", best_parameters)\n",
    "                print(\"Best composite score:\", best_composite_score)\n",
    "                # Use the best parameters from the grid search\n",
    "                best_n_epochs = best_parameters['n_epochs']\n",
    "                best_max_complexity = best_parameters['max_complexity']\n",
    "            else:\n",
    "                best_n_epochs = 50\n",
    "                best_max_complexity = 10\n",
    "            # Train the final model with the best parameters\n",
    "            ql = feyn.QLattice(random_seed=random_state)\n",
    "            models = ql.auto_run(\n",
    "                data=X_train_fold_ql,\n",
    "                output_name=outcome_var,\n",
    "                kind='classification',\n",
    "                n_epochs=best_n_epochs,\n",
    "                stypes=stypes,\n",
    "                criterion=\"aic\",\n",
    "                loss_function='binary_cross_entropy',\n",
    "                max_complexity=best_max_complexity,\n",
    "                sample_weights=sample_weights_fold\n",
    "            )\n",
    "            model = models[0]\n",
    "            predictions_proba = model.predict(X_test_fold)\n",
    "            # Calculate the baseline custom score = (AUC+PRAUC)/2\n",
    "            baseline_score = combined_metric(y_true = y_test_fold, y_pred_proba=predictions_proba)\n",
    "            # baseline_roc_auc = roc_auc_score(y_test_fold, predictions_proba)\n",
    "            # Initialize an array to store the permutation importances\n",
    "            perm_importances = []\n",
    "            # Iterate over each feature and permute its values\n",
    "            for feature in X_test_fold.columns:\n",
    "                # Permute the feature values\n",
    "                permuted_features = X_test_fold.copy()\n",
    "                permuted_features[feature] = np.random.permutation(permuted_features[feature])\n",
    "                # Make predictions on the entire dataset with permuted feature\n",
    "                permuted_predictions = model.predict(permuted_features)\n",
    "                # Calculate the custom score = (AUC+PRAUC)/2 with permuted feature\n",
    "                permuted_score = combined_metric(y_true=y_test_fold, y_pred_proba=permuted_predictions)\n",
    "                # Calculate permutation importance for the feature\n",
    "                perm_importance = baseline_score - permuted_score\n",
    "                perm_importances.append((feature, perm_importance))\n",
    "            # Sort the permutation importances\n",
    "            perm_importances.sort(key=lambda x: x[1], reverse=True)\n",
    "            # Get feature importances and sort them\n",
    "            feature_importance = [importance for feature, importance in perm_importances]\n",
    "            feature_importance_df = pd.DataFrame(\n",
    "                {\"Feature\": X_test_fold.columns, \"Importance\": feature_importance}\n",
    "            )\n",
    "            feature_importance_df = feature_importance_df.sort_values(\n",
    "                by=\"Importance\", ascending=False\n",
    "            )\n",
    "            # Append to the list\n",
    "            feature_importance_list.append(feature_importance_df)\n",
    "            treebased_feature_importance = []\n",
    "            treebased_feature_importance_list.append(treebased_feature_importance)\n",
    "            predictions_proba_fold = model.predict(X_test_fold)\n",
    "            predictions_proba_fold_train = model.predict(X_train_fold)\n",
    "            predictions_proba_fold_list.append(predictions_proba_fold)\n",
    "            predictions_proba_fold_train_list.append(predictions_proba_fold_train)\n",
    "            # Clear memory from QLattice models\n",
    "            ql = None\n",
    "            models = None\n",
    "        elif model_class == HistGradientBoostingClassifier:\n",
    "            # Create a HistGradientBoostingClassifier instance\n",
    "            hgbc_model = HistGradientBoostingClassifier(random_state=random_state, early_stopping=True, **hgbc_params)\n",
    "            if hp_tuning:\n",
    "                # Create a RandomizedSearchCV instance\n",
    "                random_search = RandomizedSearchCV(\n",
    "                    estimator=hgbc_model, \n",
    "                    param_distributions=hgbc_param_dist, \n",
    "                    n_iter=n_iter_hptuning,\n",
    "                    scoring= custom_scorer, \n",
    "                    cv=cv_folds_hptuning,\n",
    "                    refit=True, \n",
    "                    random_state=random_state,\n",
    "                    verbose=0,\n",
    "                n_jobs = n_cpu_for_tuning)\n",
    "                # Perform the random search on the training data\n",
    "                random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "                # Get the best parameters and best estimator\n",
    "                best_params = random_search.best_params_\n",
    "                hgbc_model = HistGradientBoostingClassifier(random_state=random_state, early_stopping=True, **best_params)\n",
    "            else:\n",
    "                hgbc_model = HistGradientBoostingClassifier(random_state=random_state, early_stopping=True, **hgbc_params)\n",
    "            # model = random_search.best_estimator_\n",
    "            # Fit the best estimator on the entire training data\n",
    "            hgbc_model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = hgbc_model.predict_proba(X_test_fold)[:, 1]\n",
    "            treebased_feature_importance = [] # model.feature_importances_ is not implemented for HistGradientBoostingClassifier\n",
    "            treebased_feature_importance_list.append(treebased_feature_importance)\n",
    "            # feature_importance = model.feature_importances_\n",
    "            # feature_importance_list.append(feature_importance)\n",
    "            perm_result = permutation_importance(\n",
    "                hgbc_model, X_test_fold, y_test_fold, n_repeats=n_repeats, random_state=random_state, n_jobs=n_cpu_model_training, scoring = custom_scorer # \"roc_auc\"\n",
    "            )\n",
    "            # Get feature importances and sort them\n",
    "            feature_importance = perm_result.importances_mean # Mean of feature importance over n_repeats\n",
    "            feature_importance_df = pd.DataFrame(\n",
    "                {\"Feature\": X_train_fold.columns, \"Importance\": feature_importance}\n",
    "            )\n",
    "            feature_importance_df = feature_importance_df.sort_values(\n",
    "                by=\"Importance\", ascending=False\n",
    "            )\n",
    "            # Append to the list\n",
    "            feature_importance_list.append(feature_importance_df)\n",
    "            # Compute SHAP values\n",
    "            explainer = shap.TreeExplainer(hgbc_model)\n",
    "            shap_values = explainer.shap_values(X_test_fold)\n",
    "            shap_values_list.append(shap_values)\n",
    "            fold_data.append(X_test_fold) # for subsequent SHAP vs feature value analyses\n",
    "            y_fold_data.append(y_test_fold)\n",
    "            \n",
    "            predictions_proba_fold = hgbc_model.predict_proba(X_test_fold)[:, 1]\n",
    "            predictions_proba_fold_train = hgbc_model.predict_proba(X_train_fold)[:, 1]\n",
    "            predictions_proba_fold_list.append(predictions_proba_fold)\n",
    "            predictions_proba_fold_train_list.append(predictions_proba_fold_train)\n",
    "        elif model_class == lgb.LGBMClassifier:\n",
    "            # Create a LightGBM instance\n",
    "            if GPU_avail:\n",
    "                lgbm_model = lgb.LGBMClassifier(random_state=random_state, n_jobs=n_cpu_model_training, verbose=-1,device=\"gpu\", **lgbm_params) \n",
    "            else:\n",
    "                lgbm_model = lgb.LGBMClassifier(random_state=random_state, n_jobs=n_cpu_model_training, verbose=-1, **lgbm_params) \n",
    "            if hp_tuning:\n",
    "                random_search = RandomizedSearchCV(\n",
    "                    estimator=lgbm_model, \n",
    "                    param_distributions=lgbm_param_dist, \n",
    "                    n_iter=n_iter_hptuning,\n",
    "                    scoring= custom_scorer, \n",
    "                    cv=cv_folds_hptuning,\n",
    "                    refit=True, \n",
    "                    random_state=random_state,\n",
    "                    verbose=0, \n",
    "                    n_jobs = n_cpu_for_tuning)\n",
    "                # Perform the random search on the training data\n",
    "                random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "                # Get the best parameters and best estimator\n",
    "                best_params = random_search.best_params_\n",
    "                if GPU_avail:\n",
    "                    lgbm_model = lgb.LGBMClassifier(random_state=random_state, n_jobs=n_cpu_model_training, verbose=-1, device=\"gpu\", **best_params) \n",
    "                else:\n",
    "                    lgbm_model = lgb.LGBMClassifier(random_state=random_state, n_jobs=n_cpu_model_training, verbose=-1, **best_params) \n",
    "            # Fit the best estimator on the entire training data\n",
    "            lgbm_model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = lgbm_model.predict_proba(X_test_fold)[:, 1]\n",
    "            treebased_feature_importance = lgbm_model.feature_importances_\n",
    "            treebased_feature_importance_list.append(treebased_feature_importance)\n",
    "            # feature_importance = model.feature_importances_\n",
    "            # feature_importance_list.append(feature_importance)\n",
    "            perm_result = permutation_importance(\n",
    "                lgbm_model, X_test_fold, y_test_fold, n_repeats=n_repeats, random_state=random_state, n_jobs=n_cpu_model_training, scoring = custom_scorer # \"roc_auc\"\n",
    "            )\n",
    "            # Get feature importances and sort them\n",
    "            feature_importance = perm_result.importances_mean # Mean of feature importance over n_repeats\n",
    "            feature_importance_df = pd.DataFrame(\n",
    "                {\"Feature\": X_train_fold.columns, \"Importance\": feature_importance}\n",
    "            )\n",
    "            feature_importance_df = feature_importance_df.sort_values(\n",
    "                by=\"Importance\", ascending=False\n",
    "            )\n",
    "            # Append to the list\n",
    "            feature_importance_list.append(feature_importance_df)\n",
    "            # Compute SHAP values\n",
    "            explainer = shap.TreeExplainer(lgbm_model)\n",
    "            shap_values = explainer.shap_values(X_test_fold)\n",
    "            shap_values_list.append(shap_values)\n",
    "            fold_data.append(X_test_fold) # for subsequent SHAP vs feature value analyses\n",
    "            y_fold_data.append(y_test_fold)\n",
    "            \n",
    "            predictions_proba_fold = lgbm_model.predict_proba(X_test_fold)[:, 1]\n",
    "            predictions_proba_fold_train = lgbm_model.predict_proba(X_train_fold)[:, 1]\n",
    "            predictions_proba_fold_list.append(predictions_proba_fold)\n",
    "            predictions_proba_fold_train_list.append(predictions_proba_fold_train)\n",
    "        elif model_class == cb.CatBoostClassifier: # cb.CatBoostClassifier\n",
    "            # Define the CatBoost classifier\n",
    "            # if GPU_avail:\n",
    "            #     cb_model = cb.CatBoostClassifier(random_state=random_state, cat_features=cat_features, silent=True, task_type=\"GPU\", bootstrap_type = \"No\") # , logging_level='Silent' verbose=0, \n",
    "            # else:\n",
    "            cb_model = cb.CatBoostClassifier(random_state=random_state, cat_features=cat_features, silent=True,**cb_params) # , **cb_params, logging_level='Silent' verbose=0, silent=True,\n",
    "            if hp_tuning:\n",
    "                # Create a RandomizedSearchCV instance\n",
    "                random_search = RandomizedSearchCV(\n",
    "                    estimator=cb_model, \n",
    "                    param_distributions=cb_param_dist, \n",
    "                    n_iter=n_iter_hptuning,\n",
    "                    scoring= custom_scorer, \n",
    "                    cv=cv_folds_hptuning,\n",
    "                    refit=True, \n",
    "                    random_state=random_state,\n",
    "                    verbose=0, \n",
    "                    n_jobs = n_cpu_for_tuning\n",
    "                )\n",
    "                # Perform the random search on the training data\n",
    "                random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "                # Get the best parameters and best estimator\n",
    "                best_params = random_search.best_params_\n",
    "                cb_model = cb.CatBoostClassifier(random_state=random_state, cat_features=cat_features, silent=True,**best_params) # logging_level='Silent', verbose=0, silent=True,\n",
    "            # Fit the best estimator on the entire training data\n",
    "            cb_model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = cb_model.predict_proba(X_test_fold)[:, 1]\n",
    "            treebased_feature_importance = cb_model.feature_importances_\n",
    "            treebased_feature_importance_list.append(treebased_feature_importance)\n",
    "\n",
    "            perm_result = permutation_importance(\n",
    "                cb_model, X_test_fold, y_test_fold, n_repeats=n_repeats, random_state=random_state, n_jobs=n_cpu_model_training, scoring = custom_scorer # \"roc_auc\"\n",
    "            )\n",
    "            # Get feature importances and sort them\n",
    "            feature_importance = perm_result.importances_mean # Mean of feature importance over n_repeats\n",
    "            feature_importance_df = pd.DataFrame(\n",
    "                {\"Feature\": X_train_fold.columns, \"Importance\": feature_importance}\n",
    "            )\n",
    "            feature_importance_df = feature_importance_df.sort_values(\n",
    "                by=\"Importance\", ascending=False\n",
    "            )\n",
    "            # Append to the list\n",
    "            feature_importance_list.append(feature_importance_df)\n",
    "            # Compute SHAP values\n",
    "            explainer = shap.TreeExplainer(cb_model)\n",
    "            shap_values = explainer.shap_values(X_test_fold)\n",
    "            shap_values_list.append(shap_values)\n",
    "            fold_data.append(X_test_fold) # for subsequent SHAP vs feature value analyses\n",
    "            y_fold_data.append(y_test_fold)\n",
    "            \n",
    "            predictions_proba_fold = cb_model.predict_proba(X_test_fold)[:, 1]\n",
    "            predictions_proba_fold_train = cb_model.predict_proba(X_train_fold)[:, 1]\n",
    "            predictions_proba_fold_list.append(predictions_proba_fold)\n",
    "            predictions_proba_fold_train_list.append(predictions_proba_fold_train)\n",
    "\n",
    "        # Check if the specified model class is LogisticRegression (logistic regression regularized on both L1 and L2 terms - elasticnet)\n",
    "        elif model_class == LogisticRegression:\n",
    "            # Define the Logistic Regression classifier (configured as elasticnet)\n",
    "            lr_model = LogisticRegression(penalty='l1', random_state=random_state, solver=\"liblinear\", **lr_params)\n",
    "            if hp_tuning:\n",
    "                # Create a RandomizedSearchCV instance\n",
    "                random_search = RandomizedSearchCV(\n",
    "                    estimator=lr_model, \n",
    "                    param_distributions=lr_param_dist, \n",
    "                    n_iter=n_iter_hptuning,\n",
    "                    scoring= custom_scorer, \n",
    "                    cv=cv_folds_hptuning,\n",
    "                    refit=True, \n",
    "                    random_state=random_state,\n",
    "                    verbose= 0,\n",
    "                    n_jobs=n_cpu_for_tuning\n",
    "                )\n",
    "                # Perform the random search on the training data\n",
    "                random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "                # Get the best parameters and best estimator\n",
    "                best_params = random_search.best_params_\n",
    "                lr_model = LogisticRegression(penalty='l1',random_state=random_state, solver=\"liblinear\", **best_params)\n",
    "            else:\n",
    "                lr_model = LogisticRegression(penalty='l1',random_state=random_state, solver=\"liblinear\", **lr_params)\n",
    "            # Fit the best estimator on the entire training data\n",
    "            lr_model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = lr_model.predict_proba(X_test_fold)[:, 1]\n",
    "            perm_result = permutation_importance(\n",
    "                lr_model, X_test_fold, y_test_fold, n_repeats=n_repeats, random_state=random_state, n_jobs=n_cpu_model_training, scoring = custom_scorer #\"roc_auc\"\n",
    "            )\n",
    "            # Get feature importances and sort them\n",
    "            feature_importance = perm_result.importances_mean # Mean of feature importance over n_repeats\n",
    "            feature_importance_df = pd.DataFrame(\n",
    "                {\"Feature\": X_train_fold.columns, \"Importance\": feature_importance}\n",
    "            )\n",
    "            feature_importance_df = feature_importance_df.sort_values(\n",
    "                by=\"Importance\", ascending=False\n",
    "            )\n",
    "            # Append to the list\n",
    "            feature_importance_list.append(feature_importance_df)\n",
    "            # Compute SHAP values\n",
    "            explainer = shap.LinearExplainer(lr_model, X_train_fold)\n",
    "            shap_values = explainer.shap_values(X_test_fold)\n",
    "            shap_values_list.append(shap_values)\n",
    "            fold_data.append(X_test_fold) # for subsequent SHAP vs feature value analyses\n",
    "            y_fold_data.append(y_test_fold)\n",
    "            \n",
    "            predictions_proba_fold = lr_model.predict_proba(X_test_fold)[:, 1]\n",
    "            predictions_proba_fold_train = lr_model.predict_proba(X_train_fold)[:, 1]\n",
    "            predictions_proba_fold_list.append(predictions_proba_fold)\n",
    "            predictions_proba_fold_train_list.append(predictions_proba_fold_train)\n",
    "        elif model_class == GaussianNB: # Naive Bayes\n",
    "            nb_model = GaussianNB()\n",
    "            nb_model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            predictions_proba = nb_model.predict_proba(X_test_fold)[:, 1]\n",
    "            perm_result = permutation_importance(\n",
    "                nb_model, X_test_fold, y_test_fold, n_repeats=n_repeats, random_state=random_state, n_jobs=n_cpu_model_training, scoring = custom_scorer # \"roc_auc\"\n",
    "            )\n",
    "            # Get feature importances and sort them\n",
    "            feature_importance = perm_result.importances_mean # Mean of feature importance over n_repeats\n",
    "            feature_importance_df = pd.DataFrame(\n",
    "                {\"Feature\": X_train_fold.columns, \"Importance\": feature_importance}\n",
    "            )\n",
    "            feature_importance_df = feature_importance_df.sort_values(\n",
    "                by=\"Importance\", ascending=False\n",
    "            )\n",
    "            # Append to the list\n",
    "            feature_importance_list.append(feature_importance_df)\n",
    "            treebased_feature_importance = [] # empty as it is not defined for Naive Bayes model \n",
    "            treebased_feature_importance_list.append(treebased_feature_importance)\n",
    "            # Compute SHAP values \n",
    "            explainer = shap.Explainer(nb_model.predict_proba, X_train_fold)\n",
    "            shap_values = explainer(X_test_fold)\n",
    "            shap_values_list.append(shap_values)\n",
    "            fold_data.append(X_test_fold) # for subsequent SHAP vs feature value analyses\n",
    "            y_fold_data.append(y_test_fold)\n",
    "            \n",
    "            predictions_proba_fold = nb_model.predict_proba(X_test_fold)[:, 1]\n",
    "            predictions_proba_fold_train = nb_model.predict_proba(X_train_fold)[:, 1]\n",
    "            predictions_proba_fold_list.append(predictions_proba_fold)\n",
    "            predictions_proba_fold_train_list.append(predictions_proba_fold_train)\n",
    "        # Aggregate predictions and labels\n",
    "        aggregated_predictions = np.concatenate((aggregated_predictions, predictions_proba))\n",
    "        aggregated_labels = np.concatenate((aggregated_labels, y_test_fold))\n",
    "    # Other processing for each fold goes here\n",
    "    if overlapping_samples:\n",
    "        print(\"Warning: Overlapping test samples found across folds.\")\n",
    "    \n",
    "    # Initialize plot objects\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 4))\n",
    "    ax1.set_title('ROC Curve')\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax2.set_title('Precision-Recall Curve')\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    # Initialize a list to store thresholds for each fold\n",
    "    thresholds_per_fold = [0.5]\n",
    "    # Calculate and store metrics for each fold\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        X_test_fold = X.iloc[test_index]\n",
    "        y_test_fold = y.iloc[test_index]\n",
    "        predictions_proba_fold = predictions_proba_fold_list[fold - 1]\n",
    "        predictions_proba_fold_train = predictions_proba_fold_train_list[fold - 1]\n",
    "        y_train_fold = y.iloc[train_index]\n",
    "        # # Get predictions for the current fold using the optimal threshold\n",
    "            # Use default threshold if specified\n",
    "        if use_default_threshold:\n",
    "            opt_threshold_fold = 0.5\n",
    "        else: # we use prediction probabilities from the train subsets to estimate the optimal threshold for classification\n",
    "            class_1_probs = predictions_proba_fold_train[y_train_fold == True]\n",
    "            class_0_probs = predictions_proba_fold_train[y_train_fold == False]\n",
    "            median_class_1_probs = np.median(class_1_probs)\n",
    "            median_class_0_probs = np.median(class_0_probs)\n",
    "            # Update threshold based on previous folds\n",
    "            opt_threshold_fold = np.median([threshold for threshold in thresholds_per_fold])\n",
    "            # Append current fold's threshold to the list\n",
    "            threshold = np.mean([median_class_1_probs, median_class_0_probs])\n",
    "            thresholds_per_fold.append(threshold)\n",
    "        predictions_class_fold = np.where(predictions_proba_fold >= opt_threshold_fold, True, False)\n",
    "        ###########\n",
    "        # Find the indices where y_test_fold does not equal predictions_class_fold\n",
    "        missclassified_samples_fold = y_test_fold.index[np.where(y_test_fold != predictions_class_fold)[0]]\n",
    "        missclassified_samples.extend(missclassified_samples_fold.tolist())\n",
    "        ###########\n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_test_fold, predictions_class_fold, predictions_proba_fold)\n",
    "        metrics['Fold'] = fold\n",
    "        # fold_results = fold_results.append(metrics, ignore_index=True)\n",
    "        fold_results = pd.concat([fold_results, pd.DataFrame(metrics, index=[0])], ignore_index=True)\n",
    "        # Compute ROC and PR curve values\n",
    "        fpr, tpr, _ = roc_curve(y_test_fold, predictions_proba_fold, pos_label=True, drop_intermediate=False)\n",
    "        precision, recall, _ = precision_recall_curve(y_test_fold, predictions_proba_fold, pos_label=True)\n",
    "        # Create a DataFrame for the current fold's results\n",
    "        fold_results_df = pd.DataFrame({\n",
    "            'fold': fold,\n",
    "            'fpr': list(fpr) + [None] * (len(recall) - len(fpr)),  # Padding to match lengths\n",
    "            'tpr': list(tpr) + [None] * (len(recall) - len(tpr)),  # Padding to match lengths\n",
    "            'precision': list(precision),\n",
    "            'recall': list(recall)\n",
    "        })\n",
    "        # Append the current fold's results to the existing results DataFrame\n",
    "        fold_results_plt = pd.concat([fold_results_plt, fold_results_df], ignore_index=True)\n",
    "        # Plot ROC and PR curves for the current fold\n",
    "        ax1.plot(fpr, tpr, label=f'Fold {fold}', alpha=0.5)\n",
    "        ax2.plot(recall, precision, label=f'Fold {fold}', alpha=0.5)\n",
    "    # Finalize plots\n",
    "    ax1.legend(loc='lower right')\n",
    "    ax2.legend(loc='lower left')\n",
    "    ax1.set_facecolor('white')\n",
    "    # show both grid lines\n",
    "    ax1.grid(which='both', color = \"grey\")\n",
    "    # modify grid lines:\n",
    "    ax1.grid(which='minor', alpha=0.1)\n",
    "    ax1.grid(which='major', alpha=0.2)\n",
    "    ax1.plot([0, 1], [0, 1], color='black', linestyle='--', linewidth=0.5, label='chance level')\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, 1.1])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=8)\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=8)\n",
    "    ax1.set_title('ROC curve', fontsize=8)\n",
    "    ax1.legend(loc=\"lower right\", fontsize=8)\n",
    "    ax1.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)  \n",
    "    ax2.set_facecolor('white')\n",
    "    # show both grid lines\n",
    "    ax2.grid(which='both', color = \"grey\")\n",
    "    # modify grid lines:\n",
    "    ax2.grid(which='minor', alpha=0.1)\n",
    "    ax2.grid(which='major', alpha=0.2)\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, 1.1])\n",
    "    ax2.set_xlabel('Recall', fontsize=8)\n",
    "    ax2.set_ylabel('Precision', fontsize=8)\n",
    "    ax2.set_title('Precision-Recall curve', fontsize=8)\n",
    "    ax2.legend(loc=\"lower left\", fontsize=8)\n",
    "    ax2.grid(color='grey', linestyle='-', linewidth=0.25, alpha=0.5)  \n",
    "    plt.show()\n",
    "    # Aggregate results across folds\n",
    "    if use_default_threshold:\n",
    "        opt_threshold = 0.5\n",
    "    else:\n",
    "        # Drop the first entry of thresholds_per_fold that is 0.5\n",
    "        thresholds_per_fold = thresholds_per_fold[1:]\n",
    "        opt_threshold = np.median(thresholds_per_fold)\n",
    "    aggregated_results = {metric: np.nanmean(fold_results[metric].values).round(2) for metric in measures}\n",
    "    aggregated_results_sd = {metric: np.nanstd(fold_results[metric].values).round(2) for metric in measures}\n",
    "    # Combining mean and standard deviation\n",
    "    combined_results = {metric: f\"{mean} ± {sd}\" for metric, mean in aggregated_results.items() for _, sd in aggregated_results_sd.items() if metric == _}\n",
    "    # Creating a DataFrame for tabular display\n",
    "    results_table = pd.DataFrame(list(combined_results.items()), columns=['Metric', 'Result'])\n",
    "    # Displaying the results\n",
    "    print(\"Aggregated Results:\")\n",
    "    print(results_table.to_string(index=False))\n",
    "    return fold_results, results_table, opt_threshold, feature_importance_list, treebased_feature_importance_list, shap_values_list, fold_results_plt, fold_data, missclassified_samples, y_fold_data, predictions_proba_fold_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94760cb",
   "metadata": {},
   "source": [
    "##### SHAP summary plot for when the model uses categorical features\n",
    "\n",
    "This function resolves the issue of not showing the levels of categorical features on the SHAP summary plot from shap package in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "456b7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_shap_plot(shap_values, data, top_n=10, jitter=0.1, **kwargs):\n",
    "    # Ensure data and shap_values are consistent\n",
    "    assert shap_values.shape[1] == data.shape[1], \"Mismatch between shap_values and data\"\n",
    "\n",
    "    feature_names = data.columns\n",
    "\n",
    "    # Calculate the mean absolute SHAP values to rank feature importance\n",
    "    mean_shap_values = np.mean(np.abs(shap_values), axis=0)\n",
    "    sorted_indices = np.argsort(mean_shap_values)[::-1][:top_n]\n",
    "\n",
    "    top_n_features = feature_names[sorted_indices]\n",
    "\n",
    "    # Create an empty dictionary to store categorical features and their categories\n",
    "    cat_features = {}\n",
    "\n",
    "    # Identify and store categories for categorical features\n",
    "    for feature in top_n_features:\n",
    "        if data[feature].dtype.name in ['category', 'object']:\n",
    "            categories = data[feature].unique().tolist()\n",
    "            cat_features[feature] = categories\n",
    "\n",
    "    # Extract all unique categories for the top n features\n",
    "    unique_categories = set().union(*cat_features.values())\n",
    "    num_categories = len(unique_categories)\n",
    "    cmap_grey = plt.get_cmap('Set1')\n",
    "    category_colors = cmap_grey(np.linspace(0, 1, num_categories))\n",
    "\n",
    "    # Create a dictionary to map each category to a distinct marker\n",
    "    category_marker_dictionary = {}\n",
    "    category_markers = ['s', 'D', 'P', 'X', '^', 'v', '<', '>', 'H', 'h', '+', 'x', 'p', 'd', '1', '2', '3', '4', '|', '_', ',', '.', '8']\n",
    "\n",
    "    for category in unique_categories:\n",
    "        if category_markers:\n",
    "            category_marker_dictionary[category] = category_markers.pop(0)\n",
    "        else:\n",
    "            print(\"Warning: Not enough markers available.\")\n",
    "            break\n",
    "\n",
    "    # Calculate the height based on the number of rows\n",
    "    height = round(np.max([10, np.log(top_n)]))\n",
    "    max_height = 65535 / 72  # Convert pixels to inches\n",
    "    if height > max_height:\n",
    "        height = max_height\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, height))\n",
    "    cmap = plt.get_cmap('bwr')\n",
    "\n",
    "    displayed_categories = set()\n",
    "    legend_handles = []\n",
    "    legend_labels = []\n",
    "\n",
    "    for i, idx in enumerate(sorted_indices):\n",
    "        feature_shap_values = shap_values[:, idx]\n",
    "        feature_values = data.iloc[:, idx]\n",
    "        ax.axhline(i+1, linestyle='--', color='gray', linewidth=0.5)\n",
    "\n",
    "        if feature_names[idx] in cat_features:\n",
    "            # Handle categorical features\n",
    "            for j, category in enumerate(unique_categories):\n",
    "                mask = (data[feature_names[idx]] == category)\n",
    "                if np.sum(mask) > 0:\n",
    "                    jitter_values = jitter * (np.random.randn(np.sum(mask)) - 0.5)\n",
    "                    ax.scatter(feature_shap_values[mask],\n",
    "                               [top_n - i] * np.sum(mask) + jitter_values,\n",
    "                               facecolors=category_colors[j % num_categories],\n",
    "                               edgecolors='grey',\n",
    "                               marker=category_marker_dictionary[category], \n",
    "                               s=15,\n",
    "                               alpha=0.7, linewidths=0.5)\n",
    "\n",
    "                    if category not in displayed_categories:\n",
    "                        legend_handles.append(plt.Line2D([0], [0], \n",
    "                                                         marker=category_marker_dictionary[category], \n",
    "                                                         color='white',\n",
    "                                                         markerfacecolor=category_colors[j % num_categories], \n",
    "                                                         markeredgecolor='grey', \n",
    "                                                         markersize=5))\n",
    "                        legend_labels.append(category)\n",
    "                        displayed_categories.add(category)\n",
    "        else:\n",
    "            # Handle numerical features\n",
    "            numeric_values = np.array(feature_values, dtype=float)\n",
    "            missing_mask = np.isnan(numeric_values)  # Handle missing values\n",
    "\n",
    "            # Plot missing values in grey\n",
    "            if np.any(missing_mask):\n",
    "                jitter_values_missing = jitter * (np.random.randn(np.sum(missing_mask)) - 0.5)\n",
    "                ax.scatter(feature_shap_values[missing_mask], \n",
    "                        [top_n - i] * np.sum(missing_mask) + jitter_values_missing,\n",
    "                        c='grey', \n",
    "                        marker='x', \n",
    "                        edgecolors='grey',\n",
    "                        alpha=0.7,\n",
    "                        label='Missing', s=15,\n",
    "                       linewidths=0.5)\n",
    "            \n",
    "            normalized_values = QuantileTransformer(output_distribution='uniform').fit_transform(numeric_values.reshape(-1, 1)).flatten()\n",
    "            jitter_values = jitter * (np.random.randn(len(feature_shap_values)) - 0.5)\n",
    "            ax.scatter(feature_shap_values, \n",
    "                       [top_n - i] * len(feature_shap_values) + jitter_values,\n",
    "                       c=normalized_values,\n",
    "                       cmap=cmap,\n",
    "                       marker=\"o\",\n",
    "                       edgecolors='grey',\n",
    "                       alpha=0.7,\n",
    "                       s=15,\n",
    "                       linewidths=0.5)\n",
    "\n",
    "\n",
    "    # Set y-axis ticks and labels\n",
    "    ax.set_yticks(range(1, top_n + 1))\n",
    "    ax.set_yticklabels([feature_names[idx] for idx in sorted_indices[::-1]], rotation='horizontal', fontsize=8)\n",
    "\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel('SHAP values', fontsize=8)\n",
    "\n",
    "    # Add colorbar for numerical features\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "    sm.set_array([])\n",
    "    cbar = plt.colorbar(sm, ax=ax, orientation='vertical')\n",
    "    cbar.ax.tick_params(axis='both', labelsize=8)\n",
    "\n",
    "    # Add legend for categorical features\n",
    "    if legend_handles:\n",
    "        ax.legend(legend_handles, legend_labels, loc='lower right', fontsize=8)\n",
    "\n",
    "    # Add a midline\n",
    "    plt.axvline(0, linestyle='--', color='gray', alpha=0.5)\n",
    "    cbar.set_label(label='Feature value', size=8)\n",
    "\n",
    "    plt.rcParams.update({'font.size': 8})\n",
    "    plt.grid(linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "    ax.tick_params(axis='x', labelsize=8)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ffed74",
   "metadata": {},
   "source": [
    "##### QLattice model\n",
    "\n",
    "The QLattice, integrated into the Feyn Python library, represents a cutting-edge approach to supervised machine learning known as symbolic regression. It specializes in identifying the most suitable mathematical models to describe complex datasets. Through an iterative process of training, the QLattice prioritizes simplicity while maintaining high performance.\n",
    "\n",
    "More information: https://docs.abzu.ai/docs/guides/getting_started/qlattice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e38062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"QLattice_mdl\" in models_to_include:\n",
    "    fold_results_QLattice, aggregated_results_QLattice, opt_threshold_QLattice, FI_QLattice, treeFI_QLattice, shap_QLattice, fold_results_plt_Qlattice, _, missclassified_samples_QLattice, y_fold, pp_fold_QLattice = cross_validate_model(model_class='QLattice',\n",
    "                                                                                                                X=X_train_imputed,\n",
    "                                                                                                                y=y_train,\n",
    "                                                                                                                sample_weights=sample_weights,\n",
    "                                                                                                                random_state=SEED,\n",
    "                                                                                                                use_default_threshold=use_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8fd18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"QLattice_mdl\" in models_to_include:\n",
    "    # plot permutation-based feature importance\n",
    "    plot_PFI(PFI_folds=FI_QLattice, X=X_train, model_name=\"QLattice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1642fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"QLattice_mdl\" in models_to_include:\n",
    "    if export_missclassified:\n",
    "        misclassified_ids = mydata_backup.loc[missclassified_samples_QLattice, 'ID']\n",
    "        \n",
    "        misclassified_ids_df = pd.DataFrame(misclassified_ids.tolist(), columns=['Misclassified_IDs'])\n",
    "        misclassified_ids_df.to_excel('misclassified_ids_QLattice.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2183998",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d608a52",
   "metadata": {},
   "source": [
    "##### Gaussian Naive Bayes\n",
    "\n",
    "Gaussian Naive Bayes (GaussianNB) is a classification algorithm implemented in Python's scikit-learn library. It assumes that the likelihood of features follows a Gaussian distribution. The algorithm estimates parameters using maximum likelihood. In practice, GaussianNB is commonly used for classification tasks when dealing with continuous data.\n",
    "\n",
    "Read more here: https://scikit-learn.org/stable/modules/naive_bayes.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc302b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NaiveBayes_mdl\" in models_to_include:\n",
    "    fold_results_NB, aggregated_results_NB, opt_threshold_NB, fi_NB, treeFI_NB, shap_NB, fold_results_plt_NB, fold_data_NB, missclassified_samples_NB, y_fold, pp_fold_NB = cross_validate_model(model_class=GaussianNB,\n",
    "                                                                                                X=X_train_OHE,\n",
    "                                                                                                y=y_train,\n",
    "                                                                                                sample_weights=sample_weights,\n",
    "                                                                                                random_state=SEED,\n",
    "                                                                                                use_default_threshold=use_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9052cb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NaiveBayes_mdl\" in models_to_include:\n",
    "    if export_missclassified:\n",
    "        misclassified_ids = mydata_backup.loc[missclassified_samples_NB, 'ID']\n",
    "        \n",
    "        misclassified_ids_df = pd.DataFrame(misclassified_ids.tolist(), columns=['Misclassified_IDs'])\n",
    "        misclassified_ids_df.to_excel('misclassified_ids_NB.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38706699",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NaiveBayes_mdl\" in models_to_include:\n",
    "    # plot permutation-based feature importance\n",
    "    plot_PFI(PFI_folds=fi_NB, X=X_train_OHE, model_name=\"NB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fecb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NaiveBayes_mdl\" in models_to_include:\n",
    "    shap_values = np.concatenate([fold.values for fold in shap_NB], axis=0)\n",
    "    # Concatenate SHAP values and DataFrames\n",
    "    all_columns = fold_data_NB[0].columns\n",
    "    fold_data_all = pd.concat([fold for fold in fold_data_NB], axis=0)\n",
    "    fold_data_all.columns = all_columns\n",
    "    # SHAP summary plot based on the cross validation results\n",
    "    shap_summary_plot(shap_values=shap_values[:,:,1], data=fold_data_all, model_name=\"NB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63814d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"NaiveBayes_mdl\" in models_to_include:\n",
    "    \n",
    "    PFI_median = PFI_median_wrap(fi_NB)\n",
    "    PFI_median = PFI_median[['Feature', 'Importance']]\n",
    "    \n",
    "    # Create a DataFrame with SHAP values for positive class predictions\n",
    "    shap_df_positive = pd.DataFrame(shap_values[:,:,1], columns=fold_data_NB[0].columns)\n",
    "\n",
    "    # Calculate the median absolute SHAP value across folds for each feature\n",
    "    median_abs_shap_positive = shap_df_positive.abs().median()\n",
    "\n",
    "    # Sort features by their median absolute SHAP value\n",
    "    sorted_features_positive = median_abs_shap_positive.sort_values(ascending=False).index\n",
    "\n",
    "    # Take absolute values of SHAP dataframe\n",
    "    # Reorder SHAP dataframe based on sorted features\n",
    "    shap_df_sorted_positive = shap_df_positive[sorted_features_positive].abs()\n",
    "    # Take absolute values of SHAP dataframe\n",
    "    shap_df_sorted_positive_T = shap_df_sorted_positive.T\n",
    "\n",
    "    # Calculate the median importance across samples\n",
    "    SHAPFI_median = shap_df_sorted_positive_T.median(axis=1)\n",
    "    SHAPFI_median = pd.DataFrame({'Feature': SHAPFI_median.index, 'Importance': SHAPFI_median.values})\n",
    "    # normalization\n",
    "    SHAPFI_median['Importance'] = minmax_scaler.fit_transform(SHAPFI_median[['Importance']])\n",
    "\n",
    "    # Merge PFI_median, SHAPFI_median, and TFI_median dataframes by \"Feature\"\n",
    "    FI_merged_df = PFI_median.merge(SHAPFI_median, on=\"Feature\", how='outer', suffixes=('_PFI', '_SHAP'))\n",
    "\n",
    "    # Take the mean of importance values across different methods\n",
    "    FI_merged_df['Normalized_Mean_Importance'] = FI_merged_df[['Importance_PFI', 'Importance_SHAP']].mean(axis=1)\n",
    "\n",
    "    # Sort features by their mean importance\n",
    "    FI_merged_df = FI_merged_df.sort_values(by=\"Normalized_Mean_Importance\", ascending=False)\n",
    "    print(FI_merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033cc593",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LogisticRegression_mdl\" in models_to_include:\n",
    "    fold_results_LR, aggregated_results_LR, opt_threshold_LR, FI_LR, treeFI_LR, shap_LR, fold_results_plt_LR, fold_data_LR, missclassified_samples_LR, y_fold, pp_fold_LR = cross_validate_model(model_class=LogisticRegression,\n",
    "                                                                                                X=X_train_OHE,\n",
    "                                                                                                y=y_train,\n",
    "                                                                                                sample_weights=sample_weights,\n",
    "                                                                                                random_state=SEED,\n",
    "                                                                                                use_default_threshold=use_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e49ce200",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LogisticRegression_mdl\" in models_to_include:\n",
    "    if export_missclassified:\n",
    "        misclassified_ids = mydata_backup.loc[missclassified_samples_LR, 'ID']\n",
    "        \n",
    "        misclassified_ids_df = pd.DataFrame(misclassified_ids.tolist(), columns=['Misclassified_IDs'])\n",
    "        misclassified_ids_df.to_excel('misclassified_ids_LR.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c74c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LogisticRegression_mdl\" in models_to_include:\n",
    "    # plot permutation-based feature importance\n",
    "    plot_PFI(PFI_folds=FI_LR, X=X_train_OHE, model_name=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66aa576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LogisticRegression_mdl\" in models_to_include:\n",
    "    # Concatenate SHAP values and DataFrames\n",
    "    all_columns = fold_data_LR[0].columns\n",
    "    fold_data_all = pd.concat([fold for fold in fold_data_LR], axis=0)\n",
    "    fold_data_all.columns = all_columns\n",
    "    shap_values = np.concatenate([fold for fold in shap_LR], axis=0)\n",
    "    # SHAP summary plot based on the cross validation results\n",
    "    shap_summary_plot(shap_values=shap_values, data=fold_data_all, model_name=\"LR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a3d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LogisticRegression_mdl\" in models_to_include:\n",
    "    \n",
    "    # permutation-based feature importance\n",
    "    PFI_median = PFI_median_wrap(FI_LR)\n",
    "    PFI_median = PFI_median[['Feature', 'Importance']]\n",
    "    # Create a DataFrame with SHAP values for positive class predictions\n",
    "    shap_df_positive = pd.DataFrame(shap_values, columns=fold_data_LR[0].columns)\n",
    "\n",
    "    # Calculate the median absolute SHAP value across folds for each feature\n",
    "    median_abs_shap_positive = shap_df_positive.abs().median()\n",
    "\n",
    "    # Sort features by their median absolute SHAP value\n",
    "    sorted_features_positive = median_abs_shap_positive.sort_values(ascending=False).index\n",
    "\n",
    "    # Take absolute values of SHAP dataframe\n",
    "    # Reorder SHAP dataframe based on sorted features\n",
    "    shap_df_sorted_positive = shap_df_positive[sorted_features_positive].abs()\n",
    "    # Take absolute values of SHAP dataframe\n",
    "    shap_df_sorted_positive_T = shap_df_sorted_positive.T\n",
    "\n",
    "    # Calculate the median importance across samples\n",
    "    SHAPFI_median = shap_df_sorted_positive_T.median(axis=1)\n",
    "    SHAPFI_median = pd.DataFrame({'Feature': SHAPFI_median.index, 'Importance': SHAPFI_median.values})\n",
    "    # normalization\n",
    "    SHAPFI_median['Importance'] = minmax_scaler.fit_transform(SHAPFI_median[['Importance']])\n",
    "\n",
    "    # Merge PFI_median, SHAPFI_median, and TFI_median dataframes by \"Feature\"\n",
    "    FI_merged_df = PFI_median.merge(SHAPFI_median, on=\"Feature\", how='outer', suffixes=('_PFI', '_SHAP'))\n",
    "\n",
    "    # Take the mean of importance values across different methods\n",
    "    FI_merged_df['Normalized_Mean_Importance'] = FI_merged_df[['Importance_PFI', 'Importance_SHAP']].mean(axis=1)\n",
    "\n",
    "    # Sort features by their mean importance\n",
    "    FI_merged_df = FI_merged_df.sort_values(by=\"Normalized_Mean_Importance\", ascending=False)\n",
    "    print(FI_merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426f5009",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier (RF)\n",
    "\n",
    "The `RandomForestClassifier`, part of the `sklearn.ensemble` module in scikit-learn, is a versatile and powerful tool for classification tasks. It operates as a meta estimator that fits multiple decision tree classifiers on various sub-samples of the dataset, using averaging to enhance predictive accuracy and mitigate overfitting. By default, the classifier uses bootstrap sampling (`bootstrap=True`), and each tree is built using a random subset of features (`max_features='sqrt'`).\n",
    "\n",
    "Key parameters include:\n",
    "- `n_estimators`: Number of trees in the forest.\n",
    "- `criterion`: Function to measure the quality of a split (`'gini'` or `'entropy'`).\n",
    "- `max_depth`: Maximum depth of the trees.\n",
    "- `min_samples_split`: Minimum number of samples required to split an internal node.\n",
    "- `min_samples_leaf`: Minimum number of samples required to be at a leaf node.\n",
    "- `class_weight`: Adjusts weights inversely proportional to class frequencies to handle imbalanced datasets.\n",
    "\n",
    "The `RandomForestClassifier` is highly customizable, allowing for fine-tuning to suit specific datasets and classification challenges. It provides robust performance, especially in scenarios where feature interactions are complex or when the dataset contains a mix of categorical and numerical features.\n",
    "\n",
    "Read more here: [scikit-learn RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f524539",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"RandomForest_mdl\" in models_to_include:\n",
    "    fold_results_rf, aggregated_results_rf, opt_threshold_rf, FI_rf, treeFI_rf, shap_rf, fold_results_plt_rf, fold_data_rf, missclassified_samples_rf, y_fold, pp_fold_rf = cross_validate_model(model_class=RandomForestClassifier,\n",
    "                                                                                                    X = X_train_OHE,\n",
    "                                                                                                    y = y_train,\n",
    "                                                                                                    sample_weights = sample_weights,\n",
    "                                                                                                    random_state = SEED,\n",
    "                                                                                                    use_default_threshold=use_default_threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "430a0e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"RandomForest_mdl\" in models_to_include:\n",
    "    if export_missclassified:\n",
    "        misclassified_ids = mydata_backup.loc[missclassified_samples_rf, 'ID']\n",
    "        \n",
    "        misclassified_ids_df = pd.DataFrame(misclassified_ids.tolist(), columns=['Misclassified_IDs'])\n",
    "        misclassified_ids_df.to_excel('misclassified_ids_rf.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9fa48df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"RandomForest_mdl\" in models_to_include:\n",
    "    # plot permutation-based feature importance\n",
    "    plot_PFI(PFI_folds=FI_rf, X=X_train_OHE, model_name=\"RF\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66eaf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"RandomForest_mdl\" in models_to_include:\n",
    "    # plot tree-based feature importance\n",
    "    plot_TFI(X=X_train_OHE, tree_FI=treeFI_rf, model_name=\"RF\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b266a20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"RandomForest_mdl\" in models_to_include:\n",
    "    # Concatenate SHAP values and DataFrames\n",
    "    shap_values = np.concatenate([fold[:,:,1] for fold in shap_rf], axis=0)\n",
    "    all_columns = fold_data_rf[0].columns\n",
    "    fold_data_all = pd.concat([fold for fold in fold_data_rf], axis=0)\n",
    "    fold_data_all.columns = all_columns\n",
    "    # SHAP summary plot based on the cross validation results\n",
    "    shap_summary_plot(shap_values=shap_values, data=fold_data_all, model_name=\"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc3ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"RandomForest_mdl\" in models_to_include:\n",
    "    \n",
    "    # permutation-based feature importance\n",
    "    PFI_median = PFI_median_wrap(FI_rf)\n",
    "    PFI_median = PFI_median[['Feature', 'Importance']]\n",
    "    \n",
    "    # Extract feature names from X_train_OHE\n",
    "    feature_names = X_train_OHE.columns\n",
    "    \n",
    "    # # Create a DataFrame with SHAP values for positive class predictions\n",
    "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
    "\n",
    "    shap_df_sorted_positive_T = shap_df.T\n",
    "    \n",
    "    # Calculate the median importance across samples\n",
    "    SHAPFI_median = shap_df_sorted_positive_T.abs().median(axis=1)\n",
    "    SHAPFI_median = pd.DataFrame({'Feature': SHAPFI_median.index, 'Importance': SHAPFI_median.values})\n",
    "    # normalization\n",
    "    SHAPFI_median['Importance'] = minmax_scaler.fit_transform(SHAPFI_median[['Importance']])\n",
    "\n",
    "    # Combine feature importances from all folds into a single DataFrame TFI: tree-based feature importance\n",
    "    TFI = pd.concat([pd.DataFrame({'Feature': feature_names, 'Importance': fold}) for fold in treeFI_rf], axis=0, ignore_index=True)\n",
    "\n",
    "    # Calculate the median importance across folds for each feature\n",
    "    TFI_median = TFI.groupby(\"Feature\")[\"Importance\"].median().reset_index()\n",
    "\n",
    "    # Sort features by their median importance\n",
    "    TFI_median = TFI_median.sort_values(by=\"Importance\", ascending=False)\n",
    "    TFI_median['Importance'] = minmax_scaler.fit_transform(TFI_median[['Importance']])\n",
    "    # Rename the 'Importance' column in TFI_median to 'Importance_TFI'\n",
    "    TFI_median = TFI_median.rename(columns={'Importance': 'Importance_TFI'})\n",
    "    # Merge PFI_median, SHAPFI_median, and TFI_median dataframes by \"Feature\"\n",
    "    FI_merged_df = PFI_median.merge(SHAPFI_median, on=\"Feature\", how='outer', suffixes=('_PFI', '_SHAP')).merge(TFI_median, on=\"Feature\", how='outer')\n",
    "\n",
    "    # Take the mean of importance values across different methods\n",
    "    FI_merged_df['Normalized_Mean_Importance'] = FI_merged_df[['Importance_PFI', 'Importance_SHAP', 'Importance_TFI']].mean(axis=1)\n",
    "\n",
    "    # Sort features by their mean importance\n",
    "    FI_merged_df = FI_merged_df.sort_values(by=\"Normalized_Mean_Importance\", ascending=False)\n",
    "    print(FI_merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb4d5f",
   "metadata": {},
   "source": [
    "##### Histogram-based Gradient Boosting Classification Tree (HGBC)\n",
    "\n",
    "The HistGradientBoostingClassifier, part of the scikit-learn library, offers a histogram-based approach to gradient boosting for classification tasks. Notably, it exhibits significantly faster performance on large datasets (with n_samples >= 10,000) compared to the traditional GradientBoostingClassifier. The implementation of HistGradientBoostingClassifier is inspired by LightGBM and offers various parameters for customization, such as learning rate, maximum depth of trees, and early stopping criteria. This classifier is an excellent choice for classification tasks with large datasets, providing both speed and accuracy.\n",
    "\n",
    "Read more here: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e440fdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HistGBC_mdl\" in models_to_include:\n",
    "    fold_results_HGBC, aggregated_results_HGBC, opt_threshold_HGBC, FI_HGBC, treeFI_HGBC, shap_HGBC, fold_results_plt_HGBC, fold_data_HGBC, missclassified_samples_HGBC, y_fold, pp_fold_HGBC = cross_validate_model(model_class=HistGradientBoostingClassifier,\n",
    "                                                                                                        X = X_train_OHE,\n",
    "                                                                                                        y = y_train,\n",
    "                                                                                                        sample_weights = sample_weights,\n",
    "                                                                                                        random_state = SEED,\n",
    "                                                                                                    use_default_threshold=use_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "fdcb0b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HistGBC_mdl\" in models_to_include:\n",
    "    if export_missclassified:\n",
    "        misclassified_ids = mydata_backup.loc[missclassified_samples_HGBC, 'ID']\n",
    "        \n",
    "        misclassified_ids_df = pd.DataFrame(misclassified_ids.tolist(), columns=['Misclassified_IDs'])\n",
    "        misclassified_ids_df.to_excel('misclassified_ids_HGBC.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbd5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HistGBC_mdl\" in models_to_include:\n",
    "    # plot permutation-based feature importance\n",
    "    plot_PFI(PFI_folds=FI_HGBC, X=X_train_OHE, model_name=\"HGBC\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889dd72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HistGBC_mdl\" in models_to_include:\n",
    "    shap_values = np.concatenate([fold for fold in shap_HGBC], axis=0)\n",
    "    all_columns = fold_data_HGBC[0].columns\n",
    "    fold_data_all = pd.concat([fold for fold in fold_data_HGBC], axis=0)\n",
    "    fold_data_all.columns = all_columns\n",
    "    # SHAP summary plot based on the cross validation results\n",
    "    shap_summary_plot(shap_values=shap_values, data=fold_data_all, model_name=\"HGBC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231333f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"HistGBC_mdl\" in models_to_include:\n",
    "    \n",
    "    # permutation-based feature importance\n",
    "    PFI_median = PFI_median_wrap(FI_HGBC)\n",
    "    PFI_median = PFI_median[['Feature', 'Importance']]\n",
    "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
    "\n",
    "    shap_df_sorted_positive_T = shap_df.T\n",
    "    \n",
    "    # Calculate the median importance across samples\n",
    "    SHAPFI_median = shap_df_sorted_positive_T.abs().median(axis=1)\n",
    "    SHAPFI_median = pd.DataFrame({'Feature': SHAPFI_median.index, 'Importance': SHAPFI_median.values})\n",
    "    # normalization\n",
    "    SHAPFI_median['Importance'] = minmax_scaler.fit_transform(SHAPFI_median[['Importance']])\n",
    "\n",
    "    # Merge PFI_median, SHAPFI_median, and TFI_median dataframes by \"Feature\"\n",
    "    FI_merged_df = PFI_median.merge(SHAPFI_median, on=\"Feature\", how='outer', suffixes=('_PFI', '_SHAP'))\n",
    "\n",
    "    # Take the mean of importance values across different methods\n",
    "    FI_merged_df['Normalized_Mean_Importance'] = FI_merged_df[['Importance_PFI', 'Importance_SHAP']].mean(axis=1)\n",
    "\n",
    "    # Sort features by their mean importance\n",
    "    FI_merged_df = FI_merged_df.sort_values(by=\"Normalized_Mean_Importance\", ascending=False)\n",
    "    print(FI_merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280fe75",
   "metadata": {},
   "source": [
    "##### Light gradient-boosting machine (LightGBM)\n",
    "\n",
    "LightGBM represents an open-source, distributed, and high-performance gradient boosting framework, engineered by Microsoft, to tackle machine learning challenges with precision and efficiency. It operates on decision trees, finely tuned to optimize model efficiency while minimizing memory consumption. A key innovation is the Gradient-based One-Side Sampling (GOSS) method, which intelligently retains instances with significant gradients during training, thereby optimizing memory usage and training duration. Additionally, LightGBM employs histogram-based algorithms for rapid and resource-efficient tree construction. These advanced techniques, alongside optimizations such as leaf-wise tree growth and streamlined data storage formats, collectively contribute to LightGBM's remarkable efficiency and competitive edge in the realm of gradient boosting frameworks.\n",
    "\n",
    "Read more here: https://lightgbm.readthedocs.io/en/stable/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a95296",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LightGBM_mdl\" in models_to_include:\n",
    "    fold_results_LGBM, aggregated_results_LGBM, opt_threshold_LGBM, FI_LGBM, treeFI_LGBM, shap_LGBM, fold_results_plt_LGBM, fold_data_LGBM, missclassified_samples_LGBM, y_fold, pp_fold_LGBM = cross_validate_model(model_class=lgb.LGBMClassifier,\n",
    "                                                                                                        X = X_train,\n",
    "                                                                                                        y = y_train,\n",
    "                                                                                                        sample_weights = sample_weights,\n",
    "                                                                                                        random_state = SEED,\n",
    "                                                                                                        use_default_threshold=use_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "60561442",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LightGBM_mdl\" in models_to_include:\n",
    "    if export_missclassified:\n",
    "        misclassified_ids = mydata_backup.loc[missclassified_samples_LGBM, 'ID']\n",
    "        \n",
    "        misclassified_ids_df = pd.DataFrame(misclassified_ids.tolist(), columns=['Misclassified_IDs'])\n",
    "        misclassified_ids_df.to_excel('misclassified_ids_LGBM.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642e3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LightGBM_mdl\" in models_to_include:\n",
    "    # plot permutation-based feature importance\n",
    "    plot_PFI(PFI_folds=FI_LGBM, X=X_train, model_name=\"LGBM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c68a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LightGBM_mdl\" in models_to_include:\n",
    "    # plot tree-based feature importance\n",
    "    plot_TFI(X=X_train, tree_FI=treeFI_LGBM, model_name=\"LGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c4faa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LightGBM_mdl\" in models_to_include:\n",
    "    # each fold contains two arrays: one for the SHAP values of the negative class predictions (index 0) and one for the SHAP values of the positive class predictions (index 1). \n",
    "    # Therefore, to extract the arrays for the positive class predictions, you should use index 1.\n",
    "    shap_values = np.concatenate([fold for fold in shap_LGBM], axis=0)\n",
    "    all_columns = fold_data_LGBM[0].columns\n",
    "    fold_data_all = pd.concat([fold for fold in fold_data_LGBM], axis=0)\n",
    "    fold_data_all.columns = all_columns\n",
    "    # SHAP summary plot based on the cross validation results\n",
    "    shap_summary_plot(shap_values=shap_values, data=fold_data_all, model_name=\"LGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba2bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"LightGBM_mdl\" in models_to_include:\n",
    "    \n",
    "    # permutation-based feature importance\n",
    "    PFI_median = PFI_median_wrap(FI_LGBM)\n",
    "    PFI_median = PFI_median[['Feature', 'Importance']]\n",
    "    shap_values_positive = np.concatenate([fold[1] for fold in shap_LGBM], axis=0)\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
    "\n",
    "    shap_df_sorted_positive_T = shap_df.T\n",
    "    \n",
    "    # Calculate the median importance across samples\n",
    "    SHAPFI_median = shap_df_sorted_positive_T.abs().median(axis=1)\n",
    "    SHAPFI_median = pd.DataFrame({'Feature': SHAPFI_median.index, 'Importance': SHAPFI_median.values})\n",
    "    # normalization\n",
    "    SHAPFI_median['Importance'] = minmax_scaler.fit_transform(SHAPFI_median[['Importance']])\n",
    "\n",
    "    # Extract feature names from X_train\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Combine feature importances from all folds into a single DataFrame TFI: tree-based feature importance\n",
    "    TFI = pd.concat([pd.DataFrame({'Feature': feature_names, 'Importance': fold}) for fold in treeFI_LGBM], axis=0, ignore_index=True)\n",
    "\n",
    "    # Calculate the median importance across folds for each feature\n",
    "    TFI_median = TFI.groupby(\"Feature\")[\"Importance\"].median().reset_index()\n",
    "\n",
    "    # Sort features by their median importance\n",
    "    TFI_median = TFI_median.sort_values(by=\"Importance\", ascending=False)\n",
    "    TFI_median['Importance'] = minmax_scaler.fit_transform(TFI_median[['Importance']])\n",
    "    # Rename the 'Importance' column in TFI_median to 'Importance_TFI'\n",
    "    TFI_median = TFI_median.rename(columns={'Importance': 'Importance_TFI'})\n",
    "    # Merge PFI_median, SHAPFI_median, and TFI_median dataframes by \"Feature\"\n",
    "    FI_merged_df = PFI_median.merge(SHAPFI_median, on=\"Feature\", how='outer', suffixes=('_PFI', '_SHAP')).merge(TFI_median, on=\"Feature\", how='outer')\n",
    "\n",
    "    # Take the mean of importance values across different methods\n",
    "    FI_merged_df['Normalized_Mean_Importance'] = FI_merged_df[['Importance_PFI', 'Importance_SHAP', 'Importance_TFI']].mean(axis=1)\n",
    "\n",
    "    # Sort features by their mean importance\n",
    "    FI_merged_df = FI_merged_df.sort_values(by=\"Normalized_Mean_Importance\", ascending=False)\n",
    "    print(FI_merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb4d87e",
   "metadata": {},
   "source": [
    "##### Categorical boosting (CATBoost)\n",
    "\n",
    "CatBoost is a supervised machine learning method utilized for classification and regression tasks, particularly useful for handling categorical data without the need for extensive preprocessing. Employing gradient boosting, CatBoost iteratively constructs decision trees to refine predictions, achieving enhanced accuracy over time. Notably, CatBoost employs ordered encoding to effectively handle categorical features, utilizing target statistics from all rows to inform encoding decisions. Additionally, it introduces symmetric trees, ensuring uniformity in split conditions at each depth level. Compared to similar methods like XGBoost, CatBoost have often demonstrates superior performance across datasets of varying sizes, retaining key features such as cross-validation, regularization, and support for missing values.\n",
    "\n",
    "Read more here: https://catboost.ai/docs/features/categorical-features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8ca3d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = list(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048905b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94e0631",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CatBoost_mdl\" in models_to_include:\n",
    "    fold_results_CB, aggregated_results_CB, opt_threshold_CB, FI_CB, treeFI_CB, shap_CB, fold_results_plt_CB, fold_data_CB, missclassified_samples_CB, y_fold, pp_fold_CB = cross_validate_model(model_class= cb.CatBoostClassifier,\n",
    "                                                                                                X = X_train,\n",
    "                                                                                                y = y_train,\n",
    "                                                                                                sample_weights = sample_weights,\n",
    "                                                                                                random_state = SEED,\n",
    "                                                                                                cat_features = cat_features,\n",
    "                                                                                                use_default_threshold = use_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "ce560055",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CatBoost_mdl\" in models_to_include:\n",
    "    if export_missclassified:\n",
    "        misclassified_ids = mydata_backup.loc[missclassified_samples_CB, 'ID']\n",
    "        \n",
    "        misclassified_ids_df = pd.DataFrame(misclassified_ids.tolist(), columns=['Misclassified_IDs'])\n",
    "        misclassified_ids_df.to_excel('misclassified_ids_CB.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd069a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CatBoost_mdl\" in models_to_include:\n",
    "    # plot permutation-based feature importance\n",
    "    plot_PFI(PFI_folds=FI_CB, X=X_train, model_name=\"CB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7160e7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CatBoost_mdl\" in models_to_include:\n",
    "    # plot tree-based feature importance\n",
    "    plot_TFI(X=X_train, tree_FI=treeFI_CB, model_name=\"CB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65400239",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CatBoost_mdl\" in models_to_include:\n",
    "    shap_values = np.concatenate([fold for fold in shap_CB], axis=0)\n",
    "    all_columns = fold_data_CB[0].columns\n",
    "    fold_data_all = pd.concat([fold for fold in fold_data_CB], axis=0)\n",
    "    fold_data_all.columns = all_columns\n",
    "    # SHAP summary plot based on the cross validation results\n",
    "    shap_summary_plot(shap_values=shap_values, data=fold_data_all, model_name=\"CB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5462eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"CatBoost_mdl\" in models_to_include:\n",
    "    \n",
    "    # permutation-based feature importance\n",
    "    PFI_median = PFI_median_wrap(FI_CB)\n",
    "    PFI_median = PFI_median[['Feature', 'Importance']]\n",
    "    shap_values_positive = np.concatenate([fold for fold in shap_CB], axis=0)\n",
    "    feature_names = X_train.columns\n",
    "    shap_df = pd.DataFrame(shap_values, columns=feature_names)\n",
    "\n",
    "    shap_df_sorted_positive_T = shap_df.T\n",
    "    \n",
    "    # Calculate the median importance across samples\n",
    "    SHAPFI_median = shap_df_sorted_positive_T.abs().median(axis=1)\n",
    "    SHAPFI_median = pd.DataFrame({'Feature': SHAPFI_median.index, 'Importance': SHAPFI_median.values})\n",
    "    # normalization\n",
    "    SHAPFI_median['Importance'] = minmax_scaler.fit_transform(SHAPFI_median[['Importance']])\n",
    "\n",
    "    # Extract feature names from X_train\n",
    "    feature_names = X_train.columns\n",
    "\n",
    "    # Combine feature importances from all folds into a single DataFrame TFI: tree-based feature importance\n",
    "    TFI = pd.concat([pd.DataFrame({'Feature': feature_names, 'Importance': fold}) for fold in treeFI_CB], axis=0, ignore_index=True)\n",
    "\n",
    "    # Calculate the median importance across folds for each feature\n",
    "    TFI_median = TFI.groupby(\"Feature\")[\"Importance\"].median().reset_index()\n",
    "\n",
    "    # Sort features by their median importance\n",
    "    TFI_median = TFI_median.sort_values(by=\"Importance\", ascending=False)\n",
    "    TFI_median['Importance'] = minmax_scaler.fit_transform(TFI_median[['Importance']])\n",
    "    # Rename the 'Importance' column in TFI_median to 'Importance_TFI'\n",
    "    TFI_median = TFI_median.rename(columns={'Importance': 'Importance_TFI'})\n",
    "    # Merge PFI_median, SHAPFI_median, and TFI_median dataframes by \"Feature\"\n",
    "    FI_merged_df = PFI_median.merge(SHAPFI_median, on=\"Feature\", how='outer', suffixes=('_PFI', '_SHAP')).merge(TFI_median, on=\"Feature\", how='outer')\n",
    "\n",
    "    # Take the mean of importance values across different methods\n",
    "    FI_merged_df['Normalized_Mean_Importance'] = FI_merged_df[['Importance_PFI', 'Importance_SHAP', 'Importance_TFI']].mean(axis=1)\n",
    "\n",
    "    # Sort features by their mean importance\n",
    "    FI_merged_df = FI_merged_df.sort_values(by=\"Normalized_Mean_Importance\", ascending=False)\n",
    "    print(FI_merged_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "620c3e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fold_all = pd.concat([fold for fold in y_fold], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4f9523",
   "metadata": {},
   "source": [
    "##### summary of the cross validation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a165cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the available models and their corresponding DataFrames\n",
    "models_dict = {}\n",
    "if \"QLattice_mdl\" in models_to_include:\n",
    "    models_dict[\"QLattice_mdl\"] = aggregated_results_QLattice\n",
    "if \"RandomForest_mdl\" in models_to_include:\n",
    "    models_dict[\"RandomForest_mdl\"] = aggregated_results_rf\n",
    "if \"LightGBM_mdl\" in models_to_include:\n",
    "    models_dict[\"LightGBM_mdl\"] = aggregated_results_LGBM\n",
    "if \"NaiveBayes_mdl\" in models_to_include:\n",
    "    models_dict[\"NaiveBayes_mdl\"] = aggregated_results_NB\n",
    "if \"CatBoost_mdl\" in models_to_include:\n",
    "    models_dict[\"CatBoost_mdl\"] = aggregated_results_CB\n",
    "if \"LogisticRegression_mdl\" in models_to_include:\n",
    "    models_dict[\"LogisticRegression_mdl\"] = aggregated_results_LR\n",
    "if \"HistGBC_mdl\" in models_to_include:\n",
    "    models_dict[\"HistGBC_mdl\"] = aggregated_results_HGBC\n",
    "\n",
    "# Initialize an empty list to store selected models' DataFrames\n",
    "selected_models = []\n",
    "\n",
    "# Select the DataFrames based on user's choice\n",
    "for model_name in models_to_include:\n",
    "    if model_name in models_dict:\n",
    "        selected_models.append(models_dict[model_name])\n",
    "\n",
    "# Set 'Metric' as the index for each model's DataFrame\n",
    "for model in selected_models:\n",
    "    model.set_index('Metric', inplace=True)\n",
    "\n",
    "# Concatenate the DataFrames along the columns\n",
    "aggregated_results_all = pd.concat(selected_models, axis=1)\n",
    "\n",
    "# Set the column names based on the selected models\n",
    "aggregated_results_all.columns = models_to_include\n",
    "\n",
    "# Display the results\n",
    "print(aggregated_results_all)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "aggregated_results_all.to_excel('aggregated_results_all.xlsx', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d44a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract mean values from strings\n",
    "def extract_mean(value):\n",
    "    mean = re.search(r'(\\d+\\.\\d+)', value)\n",
    "    if mean:\n",
    "        return float(mean.group())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Extracting mean values from the DataFrame\n",
    "mean_values = aggregated_results_all.applymap(extract_mean)\n",
    "\n",
    "# Calculate mean values for MCC, ROCAUC, and PRAUC for each model\n",
    "mean_values_per_model = mean_values.T.groupby(level=0).mean()\n",
    "\n",
    "# Calculate the average of MCC, ROCAUC, and PRAUC for each model\n",
    "mean_values_per_model['MRPAvg'] = mean_values_per_model[['MCC', 'ROCAUC', 'PRAUC']].mean(axis=1)\n",
    "\n",
    "print(mean_values_per_model)\n",
    "\n",
    "# Find the model with the highest average of MCC, ROCAUC, and PRAUC (termed as MRPavg)\n",
    "best_model = mean_values_per_model['MRPAvg'].idxmax()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c421a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map model abbreviations to full names\n",
    "model_names = {\n",
    "    'RandomForest_mdl': 'rf',\n",
    "    'HistGBC_mdl': 'HGBC',\n",
    "    'LogisticRegression_mdl': 'LR',\n",
    "    'CatBoost_mdl': 'CB',\n",
    "    'NaiveBayes_mdl': 'NB',\n",
    "    'LightGBM_mdl': 'LGBM',\n",
    "    'QLattice_mdl' : 'QLattice'\n",
    "}\n",
    "\n",
    "# Get the full name of the best model\n",
    "best_model_name = model_names.get(best_model)\n",
    "\n",
    "# Print the best model\n",
    "print(f\"Model with the highest average of MCC, ROCAUC, and PRAUC: {best_model_name} ({best_model})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5a912",
   "metadata": {},
   "source": [
    "##### Statistical test to compare the performance of the models on cross validation\n",
    "\n",
    "Note: this is done only for AUC but can be extended for other measures.\n",
    "\n",
    "Using the Kruskal-Wallis test allows you to compare the mean AUC values of multiple models without relying on the assumptions of normality and homogeneity of variances. It provides a robust nonparametric approach to assess whether there are significant differences between the models in terms of their performance.\n",
    "\n",
    "The Kruskal-Wallis test is a nonparametric equivalent of the ANOVA test and is suitable when the assumptions of normality and homogeneity of variances are not met.\n",
    "\n",
    "Here's an outline of the steps to perform a Kruskal-Wallis test:\n",
    "\n",
    "Null Hypothesis (H0): The mean AUC values of all models are equal.\n",
    "Alternative Hypothesis (HA): At least one mean AUC value is significantly different from the others.\n",
    "\n",
    "Collect the mean AUC values of each model obtained from cross-validation.\n",
    "\n",
    "Perform a Kruskal-Wallis test, which tests for differences in the distribution of a continuous variable (AUC) among multiple groups (models).\n",
    "\n",
    "Calculate the test statistic (H-statistic) and obtain the corresponding p-value.\n",
    "\n",
    "Interpret the results:\n",
    "\n",
    "If the p-value is less than a predetermined significance level (e.g., 0.05), reject the null hypothesis. It suggests that at least one model's mean AUC value is significantly different from the others.\n",
    "If the p-value is greater than the significance level, fail to reject the null hypothesis. It indicates that there is no significant difference between the mean AUC values of the models.\n",
    "If the null hypothesis is rejected (i.e., significant differences exist), you can perform post-hoc tests to determine which specific models are significantly different from each other. Common post-hoc tests for nonparametric data include the Dunn test or the Bonferroni correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d3392",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map model names to their corresponding AUC values\n",
    "# Define the available models and their corresponding fold results\n",
    "model_auc_dict = {}\n",
    "if \"QLattice_mdl\" in models_to_include:\n",
    "    model_auc_dict['QLattice_mdl'] = fold_results_QLattice['ROCAUC'].values\n",
    "if \"RandomForest_mdl\" in models_to_include:\n",
    "    model_auc_dict['RandomForest_mdl'] = fold_results_rf['ROCAUC'].values\n",
    "if \"LightGBM_mdl\" in models_to_include:\n",
    "    model_auc_dict['LightGBM_mdl'] = fold_results_LGBM['ROCAUC'].values\n",
    "if \"NaiveBayes_mdl\" in models_to_include:\n",
    "    model_auc_dict['NaiveBayes_mdl'] = fold_results_NB['ROCAUC'].values\n",
    "if \"CatBoost_mdl\" in models_to_include:\n",
    "    model_auc_dict['CatBoost_mdl'] = fold_results_CB['ROCAUC'].values\n",
    "if \"LogisticRegression_mdl\" in models_to_include:\n",
    "    model_auc_dict['LogisticRegression_mdl'] = fold_results_LR['ROCAUC'].values\n",
    "if \"HistGBC_mdl\" in models_to_include:\n",
    "    model_auc_dict['HistGBC_mdl'] = fold_results_HGBC['ROCAUC'].values\n",
    "\n",
    "# Initialize an empty list to store selected AUC values\n",
    "selected_auc_values = []\n",
    "\n",
    "# Select the AUC values based on user's choice\n",
    "for model_name in models_to_include:\n",
    "    if model_name in model_auc_dict:\n",
    "        selected_auc_values.append(model_auc_dict[model_name])\n",
    "\n",
    "# Perform Kruskal-Wallis test\n",
    "statistic, p_value = kruskal(*selected_auc_values)\n",
    "\n",
    "# Interpret the results\n",
    "alpha = 0.05  # Significance level\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"At least one model's mean AUC value is significantly different from the others.\")\n",
    "else:\n",
    "    print(\"No significant difference between the mean AUC values of the models.\")\n",
    "\n",
    "print(f\"Kruskal-Wallis test statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d39cff",
   "metadata": {},
   "source": [
    "#### Model Uncertainty Reduction (MUR)\n",
    "\n",
    "The following code chunk identifies a margin around the prediction probability threshold and SHAP percentile to filter out samples where the predicted probabilities and SHAP values fall within a predefined uncertainty margin. This margin is determined through a grid search over a limited search space for SHAP percentile values and prediction probability margins in binary classification models. The approach ensures that the number of discarded samples does not exceed a specified maximum percentage, thereby balancing the trade-off between model uncertainty reduction and sample retention. This trade-off involves maintaining a high number of samples while ensuring the model has high certainty in its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5292dc68",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_uncertainty_reduction:\n",
    "    # Initialize an empty list to store selected models' DataFrames\n",
    "    selected_models = []\n",
    "\n",
    "    # Calculate SHAP values for the positive class\n",
    "    positive_class_index = 1\n",
    "    \n",
    "    # Define the probability margin around the threshold\n",
    "    # probability_threshold = 0.5\n",
    "    margin_grid = [0.01, 0.02, 0.05, 0.1] # Margin from the prediction probability threshold\n",
    "    SHAP_percentile_grid = [1, 2, 5, 10, 20] # Percentile of absolute SHAP values\n",
    "    max_sample_loss_perc = 20 # Maximum percentage of samples that can be discarded\n",
    "\n",
    "    aggregated_CV_results_filtered_mdl = pd.DataFrame()\n",
    "    # Iterate over the combinations of margin and SHAP_percentile\n",
    "    for margin in margin_grid:\n",
    "        for SHAP_percentile in SHAP_percentile_grid:\n",
    "\n",
    "            for selected_model in models_to_include:\n",
    "                if selected_model==\"HistGBC_mdl\":\n",
    "                    fold_data_all_OHE = pd.concat([fold for fold in fold_data_HGBC], axis=0)\n",
    "                    shap_values = np.concatenate([fold for fold in shap_HGBC], axis=0)\n",
    "                    predicted_probabilities = np.concatenate([fold for fold in pp_fold_HGBC], axis=0)\n",
    "                    shap_sum = shap_values.sum(axis=1)\n",
    "                    shap_sum_abs = np.abs(shap_sum)\n",
    "                    SHAP_thr_HGBC = np.percentile(shap_sum_abs, SHAP_percentile)\n",
    "\n",
    "                    X_train_filtered_shap = fold_data_all_OHE[(shap_sum_abs > SHAP_thr_HGBC) & \n",
    "                                                        ((predicted_probabilities < (opt_threshold_HGBC - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_HGBC + margin)))]\n",
    "                    y_train_filtered_shap = y_fold_all[(shap_sum_abs > SHAP_thr_HGBC) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_HGBC - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_HGBC + margin)))]\n",
    "                    pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_HGBC) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_HGBC - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_HGBC + margin)))]\n",
    "                    pc_filtered = np.where(pp_filtered >= opt_threshold_HGBC, True, False)\n",
    "                        \n",
    "                elif selected_model==\"RandomForest_mdl\":\n",
    "                    fold_data_all_OHE = pd.concat([fold for fold in fold_data_rf], axis=0)\n",
    "                    shap_values = np.concatenate([fold[1] for fold in shap_rf], axis=0)\n",
    "                    predicted_probabilities = np.concatenate([fold for fold in pp_fold_rf], axis=0)\n",
    "                    shap_sum = shap_values.sum(axis=1)\n",
    "                    shap_sum_abs = np.abs(shap_sum)\n",
    "                    SHAP_thr_rf = np.percentile(shap_sum_abs, SHAP_percentile)\n",
    "\n",
    "                    X_train_filtered_shap = fold_data_all_OHE[(shap_sum_abs > SHAP_thr_rf) & \n",
    "                                                        ((predicted_probabilities < (opt_threshold_rf - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_rf + margin)))]\n",
    "                    y_train_filtered_shap = y_fold_all[(shap_sum_abs > SHAP_thr_rf) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_rf - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_rf + margin)))]\n",
    "                    pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_rf) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_rf - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_rf + margin)))]\n",
    "                    pc_filtered = np.where(pp_filtered >= opt_threshold_rf, True, False)\n",
    "                    \n",
    "                elif selected_model==\"CatBoost_mdl\":\n",
    "                    fold_data_all = pd.concat([fold for fold in fold_data_CB], axis=0)\n",
    "                    shap_values = np.concatenate([fold for fold in shap_CB], axis=0)\n",
    "                    shap_sum = shap_values.sum(axis=1)\n",
    "                    shap_sum_abs = np.abs(shap_sum)\n",
    "                    SHAP_thr_CB = np.percentile(shap_sum_abs, SHAP_percentile)\n",
    "                    predicted_probabilities = np.concatenate([fold for fold in pp_fold_CB], axis=0)\n",
    "\n",
    "                    X_train_filtered_shap = fold_data_all[(shap_sum_abs > SHAP_thr_CB) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_CB - margin)) | \n",
    "                                                    (predicted_probabilities > (opt_threshold_CB + margin)))]\n",
    "                    y_train_filtered_shap = y_fold_all[(shap_sum_abs > SHAP_thr_CB) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_CB - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_CB + margin)))]\n",
    "                    pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_CB) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_CB - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_CB + margin)))]\n",
    "                    pc_filtered = np.where(pp_filtered >= opt_threshold_CB, True, False)\n",
    "                    \n",
    "                elif selected_model==\"LightGBM_mdl\":\n",
    "                    fold_data_all = pd.concat([fold for fold in fold_data_LGBM], axis=0)\n",
    "                    shap_values = np.concatenate([fold[1] for fold in shap_LGBM], axis=0)\n",
    "                    shap_sum = shap_values.sum(axis=1)\n",
    "                    shap_sum_abs = np.abs(shap_sum)\n",
    "                    SHAP_thr_LGBM = np.percentile(shap_sum_abs, SHAP_percentile)\n",
    "                    predicted_probabilities = np.concatenate([fold for fold in pp_fold_LGBM], axis=0)\n",
    "                    X_train_filtered_shap = fold_data_all[(shap_sum_abs > SHAP_thr_LGBM) & \n",
    "                                                        ((predicted_probabilities < (opt_threshold_LGBM - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_LGBM + margin)))]\n",
    "                    y_train_filtered_shap = y_fold_all[(shap_sum_abs > SHAP_thr_LGBM) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_LGBM - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_LGBM + margin)))]\n",
    "                    pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_LGBM) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_LGBM - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_LGBM + margin)))]\n",
    "                    pc_filtered = np.where(pp_filtered >= opt_threshold_LGBM, True, False)\n",
    "                    \n",
    "                elif selected_model==\"LogisticRegression_mdl\":\n",
    "                    fold_data_all_OHE = pd.concat([fold for fold in fold_data_LR], axis=0)\n",
    "                    shap_values = np.concatenate([fold for fold in shap_LR], axis=0)\n",
    "                    predicted_probabilities = np.concatenate([fold for fold in pp_fold_LR], axis=0)\n",
    "                    shap_sum = shap_values.sum(axis=1)\n",
    "                    shap_sum_abs = np.abs(shap_sum)\n",
    "                    SHAP_thr_LR = np.percentile(shap_sum_abs, SHAP_percentile)\n",
    "\n",
    "                    X_train_filtered_shap = fold_data_all_OHE[(shap_sum_abs > SHAP_thr_LR) & \n",
    "                                                        ((predicted_probabilities < (opt_threshold_LR - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_LR + margin)))]\n",
    "                    y_train_filtered_shap = y_fold_all[(shap_sum_abs > SHAP_thr_LR) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_LR - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_LR + margin)))]\n",
    "                    pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_LR) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_LR - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_LR + margin)))]\n",
    "                    pc_filtered = np.where(pp_filtered >= opt_threshold_LR, True, False)                   \n",
    "                   \n",
    "                elif selected_model==\"NaiveBayes_mdl\":\n",
    "                    fold_data_all_OHE = pd.concat([fold for fold in fold_data_NB], axis=0)\n",
    "                    shap_values = np.concatenate([fold.values for fold in shap_NB], axis=0)\n",
    "                    predicted_probabilities = np.concatenate([fold for fold in pp_fold_NB], axis=0)\n",
    "                    shap_sum = shap_values[:,:,1].sum(axis=1)\n",
    "                    shap_sum_abs = np.abs(shap_sum)\n",
    "                    SHAP_thr_NB = np.percentile(shap_sum_abs, SHAP_percentile)\n",
    "\n",
    "                    X_train_filtered_shap = fold_data_all_OHE[(shap_sum_abs > SHAP_thr_NB) & \n",
    "                                                        ((predicted_probabilities < (opt_threshold_NB - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_NB + margin)))]\n",
    "                    y_train_filtered_shap = y_fold_all[(shap_sum_abs > SHAP_thr_NB) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_NB - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_NB + margin)))]\n",
    "                    pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_NB) & \n",
    "                                                    ((predicted_probabilities < (opt_threshold_NB - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_NB + margin)))]\n",
    "                    pc_filtered = np.where(pp_filtered >= opt_threshold_NB, True, False)\n",
    "                    \n",
    "                elif selected_model==\"QLattice_mdl\":\n",
    "                    fold_data_all_OHE = pd.concat([fold for fold in fold_data_QLattice], axis=0)\n",
    "                    predicted_probabilities = np.concatenate([fold for fold in pp_fold_QLattice], axis=0)\n",
    "                    X_train_filtered_shap = fold_data_all_OHE[((predicted_probabilities < (opt_threshold_QLattice - margin)) | \n",
    "                                                    (predicted_probabilities > (opt_threshold_QLattice + margin)))]\n",
    "                    y_train_filtered_shap = y_fold_all[((predicted_probabilities < (opt_threshold_QLattice - margin)) | \n",
    "                                                        (predicted_probabilities > (opt_threshold_QLattice + margin)))]\n",
    "                    pp_filtered = predicted_probabilities[((predicted_probabilities < (opt_threshold_QLattice - margin)) | (predicted_probabilities > (opt_threshold_QLattice + margin)))]\n",
    "                    pc_filtered = np.where(pp_filtered >= opt_threshold_QLattice, True, False)\n",
    "\n",
    "                CV_results_filtered_mdl = calculate_metrics(y_true = y_train_filtered_shap, y_pred = pc_filtered, y_pred_proba = pp_filtered)\n",
    "                CV_results_filtered_mdl = pd.DataFrame(CV_results_filtered_mdl, index=[0])\n",
    "                # Add the model name to the results\n",
    "                CV_results_filtered_mdl['Model'] = selected_model\n",
    "                CV_results_filtered_mdl['Margin'] = margin\n",
    "                CV_results_filtered_mdl['SHAP_percentile'] = SHAP_percentile\n",
    "                CV_results_filtered_mdl['Sample_size'] = len(y_train_filtered_shap)\n",
    "                CV_results_filtered_mdl['Sample_loss'] = np.round(100-(len(y_train_filtered_shap)/len(y_fold_all))*100,2)\n",
    "                # Append results\n",
    "                aggregated_CV_results_filtered_mdl = aggregated_CV_results_filtered_mdl.append(CV_results_filtered_mdl, ignore_index=True)\n",
    "                # Save the results to an Excel file\n",
    "                \n",
    "    # Save the aggregated results to an Excel file\n",
    "    aggregated_CV_results_filtered_mdl.to_excel('agg_results.xlsx', index=False)\n",
    "    # Define a filter for sample loss\n",
    "    sample_loss_filter = aggregated_CV_results_filtered_mdl[\"Sample_loss\"] < max_sample_loss_perc\n",
    "\n",
    "    # Find the maximum values for ROCAUC, PRAUC, and MCC under the sample loss filter\n",
    "    max_rocauc = aggregated_CV_results_filtered_mdl.loc[sample_loss_filter, \"ROCAUC\"].max()\n",
    "    max_prauc = aggregated_CV_results_filtered_mdl.loc[sample_loss_filter, \"PRAUC\"].max()\n",
    "    max_mcc = aggregated_CV_results_filtered_mdl.loc[sample_loss_filter, \"MCC\"].max()\n",
    "\n",
    "    # Filter the DataFrame based on the maximum values and sample loss filter\n",
    "    best_combination = aggregated_CV_results_filtered_mdl.loc[\n",
    "        sample_loss_filter &\n",
    "        (aggregated_CV_results_filtered_mdl[\"ROCAUC\"] == max_rocauc) &\n",
    "        (aggregated_CV_results_filtered_mdl[\"PRAUC\"] == max_prauc) &\n",
    "        (aggregated_CV_results_filtered_mdl[\"MCC\"] == max_mcc)\n",
    "    ]\n",
    "    # Filter the DataFrame based on sample loss\n",
    "    filtered_df = aggregated_CV_results_filtered_mdl.loc[sample_loss_filter]\n",
    "\n",
    "    # Filter the DataFrame based on the maximum values and sample loss filter\n",
    "    best_combination = filtered_df[\n",
    "        (filtered_df[\"ROCAUC\"] == max_rocauc) &\n",
    "        (filtered_df[\"PRAUC\"] == max_prauc) &\n",
    "        (filtered_df[\"MCC\"] == max_mcc)\n",
    "    ]\n",
    "    # Check if best_combination is empty\n",
    "    if best_combination.empty:\n",
    "        print(\"No exact match found. Finding the closest combination.\")\n",
    "\n",
    "        # Define a closeness metric: using inverse of distance to the maximum values\n",
    "        filtered_df['Closeness'] = (\n",
    "            np.abs(filtered_df[\"ROCAUC\"] - max_rocauc) +\n",
    "            np.abs(filtered_df[\"PRAUC\"] - max_prauc) +\n",
    "            np.abs(filtered_df[\"MCC\"] - max_mcc)\n",
    "        )\n",
    "\n",
    "        # Find the row with the smallest closeness value\n",
    "        closest_combination = filtered_df.loc[filtered_df['Closeness'].idxmin()]\n",
    "\n",
    "        print(\"Closest combination found:\")\n",
    "        print(closest_combination)\n",
    "        best_combination = closest_combination\n",
    "    else:\n",
    "        print(\"Best combination found:\")\n",
    "        print(best_combination)\n",
    "        best_combination = best_combination.sort_values(by=[\"Sample_loss\", \"Margin\", \"SHAP_percentile\"]).iloc[0]\n",
    "    \n",
    "\n",
    "    best_margin = best_combination[\"Margin\"]\n",
    "    best_SHAP_percentile = best_combination[\"SHAP_percentile\"]\n",
    "    best_model = best_combination[\"Model\"]\n",
    "    print(f\"The best margin is {best_margin} and the best SHAP_percentile is {best_SHAP_percentile} and the best model is {best_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "94271b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_uncertainty_reduction:\n",
    "    best_combination_agg_perf_df = aggregated_CV_results_filtered_mdl.loc[(aggregated_CV_results_filtered_mdl[\"Margin\"] == best_margin) & (aggregated_CV_results_filtered_mdl[\"SHAP_percentile\"] == best_SHAP_percentile)]\n",
    "    # List of columns to be rounded\n",
    "    columns_to_round = [\n",
    "        \"PPV\", \"NPV\", \"Sensitivity\", \"Specificity\", \n",
    "        \"Balanced Accuracy\", \"MCC\", \"ROCAUC\", \n",
    "        \"PRAUC\", \"Brier Score\", \"F1 Score\"\n",
    "    ]\n",
    "\n",
    "    # Round the selected columns to 2 decimal places\n",
    "    best_combination_agg_perf_df[columns_to_round] = best_combination_agg_perf_df[columns_to_round].round(2)\n",
    "\n",
    "    print(best_combination_agg_perf_df)\n",
    "    # Save the aggregated results to an Excel file\n",
    "    best_combination_agg_perf_df.to_excel('best_combination_agg_perf_df.xlsx', index=False)\n",
    "    \n",
    "    # reassign the best model after applying the MUR approach\n",
    "    selected_model = best_model\n",
    "    \n",
    "    # Get the full name of the best model\n",
    "    best_model_name = model_names.get(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f0872",
   "metadata": {},
   "source": [
    "#### Stopping if there is no data split\n",
    "\n",
    "If data split is not done then the following code should stop the pipeline here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e4aa66be",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not data_split:\n",
    "    class IpyExit(SystemExit):\n",
    "        \"\"\"Exit Exception for IPython.\"\"\"\n",
    "        def __init__(self):\n",
    "            sys.stderr = StringIO()\n",
    "\n",
    "        def __del__(self):\n",
    "            sys.stderr.close()\n",
    "            sys.stderr = sys.__stderr__\n",
    "\n",
    "    def ipy_exit():\n",
    "        raise IpyExit\n",
    "\n",
    "    if get_ipython():    # If running in IPython (e.g., Jupyter)\n",
    "        exit = ipy_exit\n",
    "    else:\n",
    "        exit = sys.exit\n",
    "\n",
    "    exit()  # Stop the execution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce49bcf",
   "metadata": {},
   "source": [
    "### Prediction block\n",
    "\n",
    "The following blocks are for the case that there is an independent dataset that can be used to validate a trained model (external validation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cc52b82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if hp_tuning:\n",
    "            \n",
    "        best_composite_score = 0\n",
    "        best_parameters = {'n_epochs': 50, 'max_complexity': 10}\n",
    "    \n",
    "        def evaluate_params(n_epochs, max_complexity):\n",
    "            ql = feyn.QLattice(random_seed=SEED)\n",
    "            models = ql.auto_run(\n",
    "                data=mydata_imputed_nocv,\n",
    "                output_name=outcome_var,\n",
    "                kind='classification',\n",
    "                n_epochs=n_epochs,\n",
    "                stypes=stypes,\n",
    "                criterion=\"aic\",\n",
    "                loss_function='binary_cross_entropy',\n",
    "                max_complexity=max_complexity,\n",
    "                sample_weights=sample_weights\n",
    "            )\n",
    "            best_model = models[0]\n",
    "            \n",
    "            predictions_proba = best_model.predict(mydata_imputed_nocv)\n",
    "            QL_composite_score = (roc_auc_score(y_true = y_train, y_score = predictions_proba) + average_precision_score(y_true = y_train, y_score = predictions_proba))/2\n",
    "            print(QL_composite_score)\n",
    "            return QL_composite_score, {'n_epochs': n_epochs, 'max_complexity': max_complexity}\n",
    "    \n",
    "        results = Parallel(n_jobs=n_cpu_for_tuning, backend='loky')(\n",
    "            delayed(evaluate_params)(n_epochs, max_complexity)\n",
    "            for n_epochs in [50, 100]\n",
    "            for max_complexity in [5, 10]\n",
    "        )\n",
    "    \n",
    "        for QL_composite_score, params in results:\n",
    "            if QL_composite_score > best_composite_score:\n",
    "                best_composite_score = QL_composite_score\n",
    "                best_parameters = params\n",
    "        \n",
    "        print(\"Best Parameters:\", best_parameters)\n",
    "        print(\"Best composite score:\", best_composite_score)\n",
    "        # Use the best parameters from the grid search\n",
    "        best_n_epochs = best_parameters['n_epochs']\n",
    "        best_max_complexity = best_parameters['max_complexity']\n",
    "    else:\n",
    "        best_n_epochs = 50\n",
    "        best_max_complexity = 10\n",
    "            \n",
    "    # Train the final model with the best parameters\n",
    "    ql = feyn.QLattice(random_seed=SEED)\n",
    "    models = ql.auto_run(\n",
    "        data=mydata_imputed_nocv,\n",
    "        output_name=outcome_var,\n",
    "        kind='classification',\n",
    "        n_epochs=best_n_epochs,\n",
    "        stypes=stypes,\n",
    "        criterion=\"aic\",\n",
    "        loss_function='binary_cross_entropy',\n",
    "        max_complexity=best_max_complexity,\n",
    "        sample_weights=sample_weights\n",
    "    )\n",
    "\n",
    "    best_model = models[0]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "85d27d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        best_model.plot_signal(mydata_imputed_nocv,corr_func='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "43dcd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        best_model.plot_signal(testset_imputed,corr_func='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a089f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        best_model.plot_signal(mydata_imputed_nocv,corr_func='mutual_information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "8b39bbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        best_model.plot_signal(testset_imputed,corr_func='mutual_information')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1930a909",
   "metadata": {},
   "source": [
    "#### QLattice model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1befcae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        results_df_QLattice, missclassified_samples = evaluate_and_plot_model(model = best_model,\n",
    "                                                    threshold = opt_threshold_QLattice,\n",
    "                                                    testset = testset_imputed,\n",
    "                                                    y_test = y_test,\n",
    "                                                    filename= f'ROC_CM_QLattice.{fig_file_format}')\n",
    "        if external_val:\n",
    "            # Reorder extval_data_imputed columns to match testset_imputed\n",
    "            extval_data_imputed = extval_data_imputed[testset_imputed.columns]\n",
    "            results_df_QLattice_extval, missclassified_samples_external = evaluate_and_plot_model(model = best_model,\n",
    "                                                        threshold = opt_threshold_QLattice,\n",
    "                                                        testset = extval_data_imputed,\n",
    "                                                        y_test = y_extval_data,\n",
    "                                                        filename= f'ROC_CM_QLattice_extval.{fig_file_format}')\n",
    "    \n",
    "        if export_missclassified: # extend the code if you have external validation set and want to check this \n",
    "            misclassified_ids = mydata_backup.loc[missclassified_samples, 'ID']\n",
    "            \n",
    "            misclassified_ids_df.to_excel('testset_misclassified_ids_QLattice.xlsx', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "fb381d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        init_printing()\n",
    "        display(best_model.plot(mydata_imputed_nocv, testset_imputed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "cfe74f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        # feature selected by the model\n",
    "        print(best_model.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "12b357db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribution of model predicted probabilities for each class\n",
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        best_model.plot_probability_scores(testset_imputed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f12fa0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model representation as a closed-form expression\n",
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        init_printing()\n",
    "        sympy_model = best_model.sympify(symbolic_lr=True, include_weights=True)\n",
    "\n",
    "        display(sympy_model.as_expr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "dacec6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"QLattice\":\n",
    "    pass\n",
    "else:\n",
    "    if \"QLattice_mdl\" in models_to_include:\n",
    "        # Save a model to a file\n",
    "        best_model.save('QLattice_model.json')\n",
    "        \n",
    "        # to load the model use the following script\n",
    "        # from feyn import Model\n",
    "        # model = Model.load('QLattice_model.json')\n",
    "        # prediction = model.predict(testset_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0848a3c",
   "metadata": {},
   "source": [
    "#### Test dummy models\n",
    "\n",
    "See how dummy models (models that are not trained on the data) perform. This is done to estimate the performance level of dummy models as compared with the models that are trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0bb57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Train Dummy Classifier\n",
    "dummy_classifier = DummyClassifier(strategy='most_frequent')  # you can choose different strategies based on your requirements\n",
    "dummy_classifier.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n",
    "\n",
    "results_df_dummy, missclassified_samples = evaluate_and_plot_model(model = dummy_classifier,\n",
    "                                        threshold = 0.5,\n",
    "                                        testset = X_test_OHE,\n",
    "                                        y_test = y_test,\n",
    "                                        filename= f'ROC_CM_dummy_most_frequent.{fig_file_format}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f118af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Dummy Classifier with 'stratified' strategy\n",
    "dummy_classifier = DummyClassifier(strategy='stratified')\n",
    "dummy_classifier.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n",
    "\n",
    "results_df_dummy, missclassified_samples = evaluate_and_plot_model(model = dummy_classifier,\n",
    "                                        threshold = 0.5,\n",
    "                                        testset = X_test_OHE,\n",
    "                                        y_test = y_test,\n",
    "                                        filename= f'ROC_CM_dummy_stratified.{fig_file_format}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ee4ba",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "4b48de7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"NB\":\n",
    "    pass\n",
    "else:\n",
    "    if \"NaiveBayes_mdl\" in models_to_include:\n",
    "        # Train Naive Bayes\n",
    "        nb_classifier = GaussianNB()\n",
    "        nb_classifier.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n",
    "\n",
    "        results_df_NB, missclassified_samples = evaluate_and_plot_model(model = nb_classifier,\n",
    "                                                threshold = opt_threshold_NB,\n",
    "                                                testset = X_test_OHE,\n",
    "                                                y_test = y_test,\n",
    "                                                filename= f'ROC_CM_NB.{fig_file_format}')\n",
    "        \n",
    "        if external_val:\n",
    "            # Reorder X_extval_data_OHE columns to match X_test_OHE\n",
    "            X_extval_data_OHE = X_extval_data_OHE[X_test_OHE.columns]\n",
    "            results_df_NB_extval, missclassified_samples_external = evaluate_and_plot_model(model = nb_classifier,\n",
    "                                                    threshold = opt_threshold_NB,\n",
    "                                                    testset = X_extval_data_OHE,\n",
    "                                                    y_test = y_extval_data,\n",
    "                                                    filename= f'ROC_CM_NB_extval.{fig_file_format}')\n",
    "    \n",
    "        if export_missclassified: # extend the code if you have external validation set and want to check this \n",
    "            misclassified_ids = mydata_backup.loc[missclassified_samples, 'ID']\n",
    "            \n",
    "            misclassified_ids_df.to_excel('testset_misclassified_ids_NB.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "febc36d7",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "8fc77b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"LR\":\n",
    "    pass\n",
    "else:\n",
    "    if \"LogisticRegression_mdl\" in models_to_include:\n",
    "        # Calculate necessary information for adjusting hyperparameters\n",
    "        n_rows = X_train_OHE_nocv.shape[0]\n",
    "        n_cols = X_train_OHE_nocv.shape[1]\n",
    "        class_proportion = y_train.mean()  # binary classification\n",
    "        rf_params, lgbm_params, hgbc_params, cb_params, lr_params = set_parameters(n_rows, n_cols, class_proportion)\n",
    "        # Create a Logistic Regression instance\n",
    "        lr = LogisticRegression(penalty='l1',random_state=SEED, solver=\"liblinear\", **lr_params)\n",
    "        if hp_tuning:\n",
    "        \n",
    "            # Adjust hyperparameters based on the training data in this fold\n",
    "            rf_param_dist, lgbm_param_dist, hgbc_param_dist, cb_param_dist, lr_param_dist = adjust_hyperparameters(n_rows, n_cols)\n",
    "            # Create a RandomizedSearchCV instance\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=lr,\n",
    "                param_distributions=lr_param_dist,\n",
    "                n_iter=n_iter_hptuning,\n",
    "                scoring= custom_scorer, \n",
    "                cv=cv_folds_hptuning,\n",
    "                refit=True, \n",
    "                random_state=SEED,\n",
    "                verbose=0,\n",
    "                n_jobs=n_cpu_for_tuning\n",
    "            )\n",
    "\n",
    "            # Perform the random search on the training data\n",
    "            random_search.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            # Get the best parameters and best estimator\n",
    "            best_params = random_search.best_params_\n",
    "            lr = LogisticRegression(penalty='l1',random_state=SEED, solver=\"liblinear\", **best_params)\n",
    "        else:\n",
    "            lr = LogisticRegression(penalty='l1',random_state=SEED, solver=\"liblinear\", **lr_params)\n",
    "\n",
    "        # Fit the best estimator on the entire training data\n",
    "        lr.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "0c2e90d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"LR\":\n",
    "    pass\n",
    "else:\n",
    "    if \"LogisticRegression_mdl\" in models_to_include:\n",
    "        results_df_LR, missclassified_samples = evaluate_and_plot_model(model = lr,\n",
    "                                                threshold = opt_threshold_LR,\n",
    "                                                testset = X_test_OHE,\n",
    "                                                y_test = y_test,\n",
    "                                                filename= f'ROC_CM_LR.{fig_file_format}')\n",
    "        if external_val:\n",
    "            # Reorder X_extval_data_OHE columns to match X_test_OHE\n",
    "            X_extval_data_OHE = X_extval_data_OHE[X_test_OHE.columns]\n",
    "            results_df_LR_extval, missclassified_samples_external = evaluate_and_plot_model(model = lr,\n",
    "                                                threshold = opt_threshold_LR,\n",
    "                                                testset = X_extval_data_OHE,\n",
    "                                                y_test = y_extval_data,\n",
    "                                                filename= f'ROC_CM_LR_extval.{fig_file_format}')\n",
    "            \n",
    "        if export_missclassified: # extend the code if you have external validation set and want to check this \n",
    "            misclassified_ids = mydata_backup.loc[missclassified_samples, 'ID']\n",
    "            \n",
    "            misclassified_ids_df.to_excel('testset_misclassified_ids_LR.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09279756",
   "metadata": {},
   "source": [
    "#### HGBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a9a26ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"HGBC\":\n",
    "    pass\n",
    "else:\n",
    "    if \"HistGBC_mdl\" in models_to_include:\n",
    "        # Calculate necessary information for adjusting hyperparameters\n",
    "        n_rows = X_train_OHE_nocv.shape[0]\n",
    "        n_cols = X_train_OHE_nocv.shape[1]\n",
    "        class_proportion = y_train.mean()  # binary classification\n",
    "        rf_params, lgbm_params, hgbc_params, cb_params, lr_params = set_parameters(n_rows, n_cols, class_proportion)\n",
    "        # Create a HistGradientBoostingClassifier instance\n",
    "        HGBC = HistGradientBoostingClassifier(random_state=SEED, early_stopping=True, **hgbc_params)\n",
    "        if hp_tuning:\n",
    "        \n",
    "            # Adjust hyperparameters based on the training data in this fold\n",
    "            rf_param_dist, lgbm_param_dist, hgbc_param_dist, cb_param_dist, lr_param_dist = adjust_hyperparameters(n_rows, n_cols)\n",
    "            # Create a RandomizedSearchCV instance\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=HGBC, \n",
    "                param_distributions=hgbc_param_dist, \n",
    "                n_iter=n_iter_hptuning,\n",
    "                scoring= custom_scorer, \n",
    "                cv=cv_folds_hptuning,\n",
    "                refit=True, \n",
    "                random_state=SEED,\n",
    "                verbose=0,\n",
    "                n_jobs = n_cpu_for_tuning)\n",
    "\n",
    "            # Perform the random search on the training data\n",
    "            random_search.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            # Get the best parameters and best estimator\n",
    "            best_params = random_search.best_params_\n",
    "            HGBC = HistGradientBoostingClassifier(random_state=SEED, early_stopping=True, **best_params)\n",
    "        else:\n",
    "            HGBC = HistGradientBoostingClassifier(random_state=SEED, early_stopping=True, **hgbc_params)\n",
    "\n",
    "        # Fit the best estimator on the entire training data\n",
    "        HGBC.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dec63aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"HGBC\":\n",
    "    pass\n",
    "else:\n",
    "    if \"HistGBC_mdl\" in models_to_include:\n",
    "        results_df_HGBC, missclassified_samples = evaluate_and_plot_model(model = HGBC,\n",
    "                                                threshold = opt_threshold_HGBC,\n",
    "                                                testset = X_test_OHE,\n",
    "                                                y_test = y_test,\n",
    "                                                filename= f'ROC_CM_HGBC.{fig_file_format}')\n",
    "        \n",
    "        if external_val:\n",
    "            # Reorder X_extval_data_OHE columns to match X_test_OHE\n",
    "            X_extval_data_OHE = X_extval_data_OHE[X_test_OHE.columns]\n",
    "            results_df_HGBC_extval, missclassified_samples_external = evaluate_and_plot_model(model = HGBC,\n",
    "                                                threshold = opt_threshold_HGBC,\n",
    "                                                testset = X_extval_data_OHE,\n",
    "                                                y_test = y_extval_data,\n",
    "                                                filename= f'ROC_CM_HGBC_extval.{fig_file_format}')\n",
    "    \n",
    "        if export_missclassified: # extend the code if you have external validation set and want to check this \n",
    "            misclassified_ids = mydata_backup.loc[missclassified_samples, 'ID']\n",
    "            \n",
    "            misclassified_ids_df.to_excel('testset_misclassified_ids_HGBC.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2a02a",
   "metadata": {},
   "source": [
    "#### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "1357fc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"rf\":\n",
    "    pass\n",
    "else:\n",
    "    if \"RandomForest_mdl\" in models_to_include:\n",
    "        # Calculate necessary information for adjusting hyperparameters\n",
    "        n_rows = X_train_OHE_nocv.shape[0]\n",
    "        n_cols = X_train_OHE_nocv.shape[1]\n",
    "        class_proportion = y_train.mean()  # binary classification\n",
    "        rf_params, lgbm_params, hgbc_params, cb_params, lr_params = set_parameters(n_rows, n_cols, class_proportion)\n",
    "        rf = RandomForestClassifier(random_state=SEED, n_jobs=n_cpu_model_training, **rf_params) # , class_weight= \"balanced\"\n",
    "        # rf = RandomForestClassifier(random_state=SEED, sampling_strategy='all', n_jobs=n_cpu_model_training, **rf_params)\n",
    "        if hp_tuning:      \n",
    "            # Adjust hyperparameters based on the training data in this fold\n",
    "            rf_param_dist, lgbm_param_dist, hgbc_param_dist, cb_param_dist, lr_param_dist = adjust_hyperparameters(n_rows, n_cols)\n",
    "            # Create RandomizedSearchCV object with balanced accuracy as the scoring metric\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=rf, \n",
    "                param_distributions=rf_param_dist, \n",
    "                n_iter=n_iter_hptuning,\n",
    "                scoring= custom_scorer, \n",
    "                cv=cv_folds_hptuning,\n",
    "                refit=True, \n",
    "                random_state=SEED,\n",
    "                verbose=0,n_jobs = n_cpu_for_tuning)\n",
    "\n",
    "            # Fit the RandomizedSearchCV object to the data\n",
    "            random_search.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            # Get the best parameters and best estimator from the random search\n",
    "            best_params = random_search.best_params_\n",
    "\n",
    "            # Reinitialize a new rf model with the best parameters\n",
    "            rf = RandomForestClassifier(random_state=SEED, class_weight= \"balanced\",n_jobs=n_cpu_model_training, **best_params)\n",
    "            # Print the best parameters\n",
    "            print(\"Best Parameters:\", best_params)\n",
    "        else:\n",
    "            rf = RandomForestClassifier(random_state=SEED, class_weight= \"balanced\",n_jobs=n_cpu_model_training, **rf_params)\n",
    "            # Print the best parameters\n",
    "            print(\"Best Parameters:\", best_params)\n",
    "        # Train the new model on the training data\n",
    "        rf.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "25120e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"rf\":\n",
    "    pass\n",
    "else:\n",
    "    if \"RandomForest_mdl\" in models_to_include:\n",
    "        results_df_rf, missclassified_samples = evaluate_and_plot_model(model = rf, threshold = opt_threshold_rf, testset = X_test_OHE, y_test = y_test, filename= f'ROC_CM_rf.{fig_file_format}')\n",
    "        if external_val:\n",
    "            # Reorder X_extval_data_OHE columns to match X_test_OHE\n",
    "            X_extval_data_OHE = X_extval_data_OHE[X_test_OHE.columns]\n",
    "            results_df_rf_extval, missclassified_samples_external = evaluate_and_plot_model(model = rf,\n",
    "                                                            threshold = opt_threshold_rf,\n",
    "                                                            testset = X_extval_data_OHE,\n",
    "                                                            y_test = y_extval_data,\n",
    "                                                            filename= f'ROC_CM_rf_extval.{fig_file_format}')\n",
    "\n",
    "        if export_missclassified: # extend the code if you have external validation set and want to check this \n",
    "            misclassified_ids = mydata_backup.loc[missclassified_samples, 'ID']\n",
    "            \n",
    "            misclassified_ids_df.to_excel('testset_misclassified_ids_rf.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f651f7",
   "metadata": {},
   "source": [
    "#### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d365faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"LGBM\":\n",
    "    pass\n",
    "else:\n",
    "    if \"LightGBM_mdl\" in models_to_include:\n",
    "        # Calculate necessary information for adjusting hyperparameters\n",
    "        n_rows = X_train.shape[0]\n",
    "        n_cols = X_train.shape[1]\n",
    "        class_proportion = y_train.mean()  # binary classification\n",
    "        rf_params, lgbm_params, hgbc_params, cb_params, lr_params = set_parameters(n_rows, n_cols, class_proportion)\n",
    "        print(lgbm_params)\n",
    "        # Define the classifier\n",
    "        if GPU_avail:\n",
    "            lgbm = lgb.LGBMClassifier(random_state=SEED, n_jobs=n_cpu_model_training, verbose=-1, device=\"gpu\", **lgbm_params) \n",
    "        else:\n",
    "            lgbm = lgb.LGBMClassifier(random_state=SEED, n_jobs=n_cpu_model_training, verbose=-1, **lgbm_params) \n",
    "\n",
    "        if hp_tuning:\n",
    "            \n",
    "            # Adjust hyperparameters based on the training data in this fold\n",
    "            rf_param_dist, lgbm_param_dist, hgbc_param_dist, cb_param_dist, lr_param_dist = adjust_hyperparameters(n_rows, n_cols)\n",
    "            \n",
    "            # Define the search strategy and scoring metric\n",
    "            # Create a RandomizedSearchCV instance\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=lgbm, \n",
    "                param_distributions=lgbm_param_dist, \n",
    "                n_iter=n_iter_hptuning,\n",
    "                scoring= custom_scorer, \n",
    "                cv=cv_folds_hptuning,\n",
    "                refit=True, \n",
    "                random_state=SEED,\n",
    "                verbose=0,\n",
    "                n_jobs = n_cpu_for_tuning)\n",
    "\n",
    "            # Perform the random search on the data\n",
    "            random_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            # Get the best parameters and best model\n",
    "            best_params = random_search.best_params_\n",
    "\n",
    "            # Print the best parameters\n",
    "            print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "            # Reinitialize a new lgbm model with the best parameters\n",
    "            if GPU_avail:\n",
    "                lgbm = lgb.LGBMClassifier(random_state=SEED, n_jobs=n_cpu_model_training, verbose=-1, device=\"gpu\", **best_params) \n",
    "            else:\n",
    "                lgbm = lgb.LGBMClassifier(random_state=SEED, n_jobs=n_cpu_model_training, verbose=-1, **best_params) \n",
    "\n",
    "        # Train the new model on the training data\n",
    "        lgbm.fit(X_train, y_train, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a318fbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"LGBM\":\n",
    "    pass\n",
    "else:\n",
    "    if \"LightGBM_mdl\" in models_to_include:\n",
    "        results_df_LGBM, missclassified_samples = evaluate_and_plot_model(model = lgbm,\n",
    "                                                threshold = opt_threshold_LGBM,\n",
    "                                                testset = X_test,\n",
    "                                                y_test = y_test,\n",
    "                                                filename= f'ROC_CM_LGBM.{fig_file_format}')\n",
    "        if external_val:\n",
    "            # Reorder X_extval_data columns to match X_test\n",
    "            X_extval_data = X_extval_data[X_test.columns]\n",
    "            results_df_LGBM_extval, missclassified_samples_external = evaluate_and_plot_model(model = lgbm,\n",
    "                                            threshold = opt_threshold_LGBM,\n",
    "                                            testset = X_extval_data,\n",
    "                                            y_test = y_extval_data,\n",
    "                                            filename= f'ROC_CM_LGBM_extval.{fig_file_format}')\n",
    "        \n",
    "        if export_missclassified: # extend the code if you have external validation set and want to check this \n",
    "            misclassified_ids = mydata_backup.loc[missclassified_samples, 'ID']\n",
    "            \n",
    "            misclassified_ids_df.to_excel('testset_misclassified_ids_LGBM.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8de8a5",
   "metadata": {},
   "source": [
    "#### CatBoost\n",
    "\n",
    "It may generate some unimportant warning messages about that can be ignored and cleaned up after running the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5f64de70",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"CB\":\n",
    "    pass\n",
    "else:\n",
    "    if \"CatBoost_mdl\" in models_to_include:\n",
    "        # Calculate necessary information for adjusting hyperparameters\n",
    "        n_rows = X_train.shape[0]\n",
    "        n_cols = X_train.shape[1]\n",
    "        class_proportion = y_train.mean()  # binary classification\n",
    "        rf_params, lgbm_params, hgbc_params, cb_params, lr_params = set_parameters(n_rows, n_cols, class_proportion)\n",
    "        if hp_tuning:\n",
    "            # Adjust hyperparameters based on the training data in this fold\n",
    "            rf_param_dist, lgbm_param_dist, hgbc_param_dist, cb_param_dist, lr_param_dist = adjust_hyperparameters(n_rows, n_cols)\n",
    "            catb = cb.CatBoostClassifier(random_state=SEED, cat_features=cat_features, silent=True) # , logging_level='Silent' verbose=0, \n",
    "\n",
    "            # Perform random search\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=catb, \n",
    "                param_distributions=cb_param_dist, \n",
    "                n_iter=n_iter_hptuning,\n",
    "                scoring= custom_scorer, \n",
    "                cv=cv_folds_hptuning,\n",
    "                refit=True, \n",
    "                random_state=SEED,\n",
    "                verbose=0,\n",
    "                n_jobs=n_cpu_for_tuning\n",
    "            )\n",
    "\n",
    "            # Fit the random search on your data\n",
    "            random_search.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            # Get the best parameters and best model\n",
    "            best_params = random_search.best_params_\n",
    "            \n",
    "            catb = cb.CatBoostClassifier(random_state=SEED, cat_features=cat_features, silent=True, **best_params) \n",
    "        else:\n",
    "            catb = cb.CatBoostClassifier(random_state=SEED, cat_features=cat_features, silent=True, **cb_params) \n",
    "            \n",
    "        # Train the new model on the training data\n",
    "        catb.fit(X_train, y_train, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "4f975f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if test_only_best_cvmodel and best_model_name != \"CB\":\n",
    "    pass\n",
    "else:\n",
    "    if \"CatBoost_mdl\" in models_to_include:\n",
    "        results_df_CB, missclassified_samples = evaluate_and_plot_model(model = catb,\n",
    "                                                threshold = opt_threshold_CB,\n",
    "                                                testset = X_test,\n",
    "                                                y_test = y_test,\n",
    "                                                filename= f'ROC_CM_CB.{fig_file_format}')\n",
    "        \n",
    "        if external_val:\n",
    "            # Reorder X_extval_data columns to match X_test\n",
    "            X_extval_data = X_extval_data[X_test.columns]\n",
    "            results_df_CB_extval, missclassified_samples_external = evaluate_and_plot_model(model = catb,\n",
    "                                            threshold = opt_threshold_CB,\n",
    "                                            testset = X_extval_data,\n",
    "                                            y_test = y_extval_data,\n",
    "                                            filename= f'ROC_CM_CB_extval.{fig_file_format}')\n",
    "        \n",
    "        if export_missclassified: # extend the code if you have external validation set and want to check this \n",
    "            misclassified_ids = mydata_backup.loc[missclassified_samples, 'ID']\n",
    "            \n",
    "            misclassified_ids_df.to_excel('testset_misclassified_ids_CB.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8832fa6c",
   "metadata": {},
   "source": [
    "#### Model interpretation for the best performing model\n",
    "\n",
    "The best performing model is chosen based on the performance of the models on cross validation as the model with the highest mean of MCC, AUC, and PRAUC. This model may not necessarily have the best performance on the test set, especially if the models perform closely similar on the cross validation. Since most of the data is used in cross validation, the model that is chosen based on that is prefered to the best performing model based only on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "16bdd406",
   "metadata": {},
   "outputs": [],
   "source": [
    "if best_model_name != \"QLattice\":\n",
    "    if test_only_best_cvmodel:\n",
    "        if best_model_name == \"rf\":\n",
    "            selected_model = rf\n",
    "        elif best_model_name == \"LGBM\":\n",
    "            selected_model = lgbm\n",
    "        elif best_model_name == \"NB\":\n",
    "            selected_model = nb_classifier\n",
    "        elif best_model_name == \"CB\":\n",
    "            selected_model = catb\n",
    "        elif best_model_name == \"LR\":\n",
    "            selected_model = lr\n",
    "        elif best_model_name == \"HGBC\":\n",
    "            selected_model = HGBC\n",
    "    else:\n",
    "        selected_model = model_dictionary[best_model_name]\n",
    "else:\n",
    "    skip_block = True\n",
    "    \n",
    "    # raise Exception(\"QLattice is already explained - Stopping code execution\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c4c1eb",
   "metadata": {},
   "source": [
    "##### SHAP values association with predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d1ca0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # Calculate SHAP values for the positive class\n",
    "    positive_class_index = 1 \n",
    "\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier)):\n",
    "        explainer = shap.TreeExplainer(selected_model)\n",
    "        shap_values = explainer.shap_values(X_test_OHE)\n",
    "        if isinstance(selected_model, RandomForestClassifier):\n",
    "            # shap_values = shap_values[positive_class_index]\n",
    "            shap_values = shap_values[:,:,1]\n",
    "    elif isinstance(selected_model, cb.CatBoostClassifier):\n",
    "        explainer = shap.TreeExplainer(selected_model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "    elif isinstance(selected_model, LogisticRegression):\n",
    "        explainer = shap.LinearExplainer(selected_model, X_train_OHE_nocv)\n",
    "        shap_values = explainer.shap_values(X_test_OHE)\n",
    "    elif isinstance(selected_model, GaussianNB):  \n",
    "        explainer = shap.Explainer(selected_model.predict_proba, X_train_OHE_nocv)\n",
    "        shap_values = explainer(X_test_OHE)\n",
    "        shap_values = shap_values.values\n",
    "        shap_values = shap_values[:, :, 1]\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(selected_model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        # shap_values = shap_values[positive_class_index]\n",
    "\n",
    "    # Calculate the sum of SHAP values for each sample\n",
    "    shap_sum = shap_values.sum(axis=1)\n",
    "\n",
    "    # Get the predicted probabilities of the model\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier)):\n",
    "        predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "    elif isinstance(selected_model, cb.CatBoostClassifier):\n",
    "        predicted_probabilities = selected_model.predict_proba(X_test)[:, positive_class_index]\n",
    "    elif isinstance(selected_model, (LogisticRegression, GaussianNB)):\n",
    "        predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "    else:\n",
    "        predicted_probabilities = selected_model.predict_proba(X_test)[:, positive_class_index]\n",
    "\n",
    "    # Plot the SHAP sum against the predicted probabilities\n",
    "    plt.scatter(shap_sum, predicted_probabilities)\n",
    "\n",
    "    plt.xlabel('sum of SHAP values')\n",
    "    plt.ylabel('predicted probability')\n",
    "    plt.title('sum of SHAP values vs. predicted probability', size=10)\n",
    "    plt.grid(linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.grid(True)\n",
    "    plt.gca().tick_params(axis='both', labelsize=8) \n",
    "    plt.gca().set_facecolor('white')\n",
    "    # display grid lines\n",
    "    plt.grid(which='both', color=\"grey\")\n",
    "    plt.grid(which='minor', alpha=0.1)\n",
    "    plt.grid(which='major', alpha=0.2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "127364e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    if model_uncertainty_reduction:\n",
    "\n",
    "        # Calculate SHAP values for the positive class\n",
    "        positive_class_index = 1 \n",
    "        if isinstance(selected_model, HistGradientBoostingClassifier):\n",
    "            explainer = shap.TreeExplainer(selected_model)\n",
    "            shap_values = explainer.shap_values(X_test_OHE)\n",
    "            predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "            shap_sum = shap_values.sum(axis=1)\n",
    "            shap_sum_abs = np.abs(shap_sum)\n",
    "            SHAP_thr_HGBC = np.percentile(shap_sum_abs, best_SHAP_percentile)\n",
    "            opt_threshold_selectedmodel = opt_threshold_HGBC\n",
    "\n",
    "            X_test_filtered_shap = X_test_OHE[(shap_sum_abs > SHAP_thr_HGBC) & \n",
    "                                                ((predicted_probabilities < (opt_threshold_HGBC - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_HGBC + best_margin)))]\n",
    "            y_test_filtered_shap = y_test[(shap_sum_abs > SHAP_thr_HGBC) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_HGBC - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_HGBC + best_margin)))]\n",
    "            pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_HGBC) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_HGBC - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_HGBC + best_margin)))]\n",
    "            pc_filtered = np.where(pp_filtered >= opt_threshold_HGBC, True, False)\n",
    "            \n",
    "        elif isinstance(selected_model, RandomForestClassifier):\n",
    "            explainer = shap.TreeExplainer(selected_model)\n",
    "            shap_values = explainer.shap_values(X_test_OHE)\n",
    "            shap_values = shap_values[:,:,1]\n",
    "            predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "            shap_sum = shap_values.sum(axis=1)\n",
    "            shap_sum_abs = np.abs(shap_sum)\n",
    "            SHAP_thr_rf = np.percentile(shap_sum_abs, best_SHAP_percentile)\n",
    "            opt_threshold_selectedmodel = opt_threshold_rf\n",
    "            X_test_filtered_shap = X_test_OHE[(shap_sum_abs > SHAP_thr_rf) & \n",
    "                                                ((predicted_probabilities < (opt_threshold_rf - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_rf + best_margin)))]\n",
    "            y_test_filtered_shap = y_test[(shap_sum_abs > SHAP_thr_rf) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_rf - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_rf + best_margin)))]\n",
    "            pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_rf) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_rf - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_rf + best_margin)))]\n",
    "            pc_filtered = np.where(pp_filtered >= opt_threshold_rf, True, False)\n",
    "            \n",
    "        elif isinstance(selected_model, cb.CatBoostClassifier):\n",
    "            explainer = shap.TreeExplainer(selected_model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            shap_sum = shap_values.sum(axis=1)\n",
    "            shap_sum_abs = np.abs(shap_sum)\n",
    "            SHAP_thr_CB = np.percentile(shap_sum_abs, best_SHAP_percentile)\n",
    "            opt_threshold_selectedmodel = opt_threshold_CB\n",
    "            predicted_probabilities = selected_model.predict_proba(X_test)[:, positive_class_index]\n",
    "            X_test_filtered_shap = X_test[(shap_sum_abs > SHAP_thr_CB) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_CB - best_margin)) | \n",
    "                                            (predicted_probabilities > (opt_threshold_CB + best_margin)))]\n",
    "            y_test_filtered_shap = y_test[(shap_sum_abs > SHAP_thr_CB) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_CB - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_CB + best_margin)))]\n",
    "            pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_CB) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_CB - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_CB + best_margin)))]\n",
    "            pc_filtered = np.where(pp_filtered >= opt_threshold_CB, True, False)\n",
    "            \n",
    "        elif isinstance(selected_model, LogisticRegression):\n",
    "            opt_threshold_selectedmodel = opt_threshold_LR\n",
    "            explainer = shap.LinearExplainer(selected_model, X_train_OHE_nocv)\n",
    "            shap_values = explainer.shap_values(X_test_OHE)\n",
    "            predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "            shap_sum = shap_values.sum(axis=1)\n",
    "            shap_sum_abs = np.abs(shap_sum)\n",
    "            SHAP_thr_LR = np.percentile(shap_sum_abs, best_SHAP_percentile)\n",
    "\n",
    "            X_test_filtered_shap = X_test_OHE[(shap_sum_abs > SHAP_thr_LR) & \n",
    "                                                ((predicted_probabilities < (opt_threshold_LR - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_LR + best_margin)))]\n",
    "            y_test_filtered_shap = y_test[(shap_sum_abs > SHAP_thr_LR) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_LR - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_LR + best_margin)))]\n",
    "            pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_LR) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_LR - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_LR + best_margin)))]\n",
    "            pc_filtered = np.where(pp_filtered >= opt_threshold_LR, True, False)\n",
    "            \n",
    "        elif isinstance(selected_model, GaussianNB):  \n",
    "            explainer = shap.Explainer(selected_model.predict_proba, X_train_OHE_nocv)\n",
    "            opt_threshold_selectedmodel = opt_threshold_NB\n",
    "            shap_values = explainer(X_test_OHE)  \n",
    "            shap_values = shap_values.values[:,:,1]\n",
    "            predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "            shap_sum = shap_values.sum(axis=1)\n",
    "            shap_sum_abs = np.abs(shap_sum)\n",
    "            SHAP_thr_NB = np.percentile(shap_sum_abs, best_SHAP_percentile)\n",
    "\n",
    "            X_test_filtered_shap = X_test_OHE[(shap_sum_abs > SHAP_thr_NB) & \n",
    "                                                ((predicted_probabilities < (opt_threshold_NB - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_NB + best_margin)))]\n",
    "            y_test_filtered_shap = y_test[(shap_sum_abs > SHAP_thr_NB) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_NB - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_NB + best_margin)))]\n",
    "            pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_NB) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_NB - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_NB + best_margin)))]\n",
    "            pc_filtered = np.where(pp_filtered >= opt_threshold_NB, True, False)\n",
    "            \n",
    "        else: # LGBM\n",
    "            opt_threshold_selectedmodel = opt_threshold_LGBM\n",
    "            explainer = shap.TreeExplainer(selected_model)\n",
    "            shap_values = explainer.shap_values(X_test)\n",
    "            # shap_values = shap_values[positive_class_index]\n",
    "            shap_sum = shap_values.sum(axis=1)\n",
    "            shap_sum_abs = np.abs(shap_sum)\n",
    "            SHAP_thr_LGBM = np.percentile(shap_sum_abs, best_SHAP_percentile)\n",
    "            predicted_probabilities = selected_model.predict_proba(X_test)[:, positive_class_index]\n",
    "            X_test_filtered_shap = X_test[(shap_sum_abs > SHAP_thr_LGBM) & \n",
    "                                                ((predicted_probabilities < (opt_threshold_LGBM - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_LGBM + best_margin)))]\n",
    "            y_test_filtered_shap = y_test[(shap_sum_abs > SHAP_thr_LGBM) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_LGBM - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_LGBM + best_margin)))]\n",
    "            pp_filtered = predicted_probabilities[(shap_sum_abs > SHAP_thr_LGBM) & \n",
    "                                            ((predicted_probabilities < (opt_threshold_LGBM - best_margin)) | \n",
    "                                                (predicted_probabilities > (opt_threshold_LGBM + best_margin)))]\n",
    "            pc_filtered = np.where(pp_filtered >= opt_threshold_LGBM, True, False)\n",
    "\n",
    "        # Plot the SHAP sum against the predicted probabilities\n",
    "        plt.scatter(shap_sum_abs, predicted_probabilities)\n",
    "        print(np.round(len(X_test_filtered_shap)/len(X_test),2))\n",
    "        plt.xlabel('Sum of absolute SHAP values')\n",
    "        plt.ylabel('Predicted probability')\n",
    "        plt.title('Sum of absolute SHAP values vs. Predicted probability', size=10)\n",
    "        plt.grid(linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "        plt.rcParams['font.family'] = 'sans-serif'\n",
    "        plt.rcParams['font.size'] = 8\n",
    "        plt.grid(True)\n",
    "        plt.gca().tick_params(axis='both', labelsize=8) \n",
    "        plt.gca().set_facecolor('white')\n",
    "        # display grid lines\n",
    "        plt.grid(which='both', color=\"grey\")\n",
    "        plt.grid(which='minor', alpha=0.1)\n",
    "        plt.grid(which='major', alpha=0.2)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "edb257fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    if model_uncertainty_reduction:\n",
    "        results_df_selected_model, missclassified_samples_selected_model = evaluate_and_plot_model(model = selected_model,\n",
    "                                                        threshold = opt_threshold_selectedmodel,\n",
    "                                                        testset = X_test_filtered_shap,\n",
    "                                                        y_test = y_test_filtered_shap,\n",
    "                                                        filename= f'ROC_CM_selected_model.{fig_file_format}')\n",
    "        \n",
    "        if isinstance(selected_model, (LogisticRegression, GaussianNB, HistGradientBoostingClassifier, RandomForestClassifier)): # then we discard samples that will have uncertain predictions\n",
    "            X_test_OHE = X_test_filtered_shap\n",
    "        else:\n",
    "            X_test = X_test_filtered_shap\n",
    "            \n",
    "        y_test = y_test_filtered_shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37fa0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # Calculate SHAP values for the positive class\n",
    "    positive_class_index = 1 \n",
    "\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier)):\n",
    "        explainer = shap.TreeExplainer(selected_model)\n",
    "        shap_values = explainer.shap_values(X_test_OHE)\n",
    "        if isinstance(selected_model, RandomForestClassifier):\n",
    "            shap_values = shap_values[:,:,1]\n",
    "    elif isinstance(selected_model, cb.CatBoostClassifier):\n",
    "        explainer = shap.TreeExplainer(selected_model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "    elif isinstance(selected_model, LogisticRegression):\n",
    "        explainer = shap.LinearExplainer(selected_model, X_train_OHE_nocv)\n",
    "        shap_values = explainer.shap_values(X_test_OHE)\n",
    "    elif isinstance(selected_model, GaussianNB):  \n",
    "        explainer = shap.Explainer(selected_model.predict_proba, X_train_OHE_nocv)\n",
    "        shap_values = explainer(X_test_OHE)  \n",
    "        shap_values = shap_values.values[:,:,1]\n",
    "    else:\n",
    "        explainer = shap.TreeExplainer(selected_model)\n",
    "        shap_values = explainer.shap_values(X_test)\n",
    "        # shap_values = shap_values[positive_class_index]\n",
    "\n",
    "    # Calculate the sum of SHAP values for each sample\n",
    "    shap_sum = shap_values.sum(axis=1)\n",
    "\n",
    "    # Get the predicted probabilities of the model\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier)):\n",
    "        predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "    elif isinstance(selected_model, cb.CatBoostClassifier):\n",
    "        predicted_probabilities = selected_model.predict_proba(X_test)[:, positive_class_index]\n",
    "    elif isinstance(selected_model, (LogisticRegression, GaussianNB)):\n",
    "        predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "    else:\n",
    "        predicted_probabilities = selected_model.predict_proba(X_test)[:, positive_class_index]\n",
    "\n",
    "    # Plot the SHAP sum against the predicted probabilities\n",
    "    plt.scatter(shap_sum, predicted_probabilities)\n",
    "\n",
    "    plt.xlabel('sum of SHAP values')\n",
    "    plt.ylabel('predicted probability')\n",
    "    plt.title('sum of SHAP values vs. predicted probability', size=10)\n",
    "    plt.grid(linestyle=':', linewidth=0.5, alpha=0.5)\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.size'] = 8\n",
    "    plt.grid(True)\n",
    "    plt.gca().tick_params(axis='both', labelsize=8) \n",
    "    plt.gca().set_facecolor('white')\n",
    "    # display grid lines\n",
    "    plt.grid(which='both', color=\"grey\")\n",
    "    plt.grid(which='minor', alpha=0.1)\n",
    "    plt.grid(which='major', alpha=0.2)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e9e8d0",
   "metadata": {},
   "source": [
    "##### Interpret the model based on SHAP analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807eccd",
   "metadata": {},
   "source": [
    "##### SHAP summary plot\n",
    "\n",
    "Note: the plot cannot show categorical features in color codes and thus they are plotted in grey (not mistaken with missing values)\n",
    "In the case of having categorical features, two SHAP plots are displayed, once with categories shown and once using the original SHAP plot that does not show the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf057a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # Determine which features to use based on the model type\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        feature_names_with_shapvalues = [\n",
    "            data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "            for feature, value in zip(X_test_OHE.columns, np.mean(np.abs(shap_values), axis=0)) \n",
    "        ]\n",
    "    else:\n",
    "        feature_names_with_shapvalues = [\n",
    "            data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "            for feature, value in zip(X_test.columns, np.mean(np.abs(shap_values), axis=0)) \n",
    "        ]\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        shap_summary_plot(shap_values=shap_values, data=X_test_OHE, model_name = f'finalmodel_{type(selected_model).__name__}')\n",
    "    else:\n",
    "        shap_summary_plot(shap_values=shap_values, data=X_test, model_name = f'finalmodel_{type(selected_model).__name__}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d98bdbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    if external_val:\n",
    "        # Calculate SHAP values for the positive class\n",
    "        positive_class_index = 1 \n",
    "\n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier)):\n",
    "            explainer = shap.TreeExplainer(selected_model)\n",
    "            shap_values_extval = explainer.shap_values(X_extval_OHE)\n",
    "            if isinstance(selected_model, RandomForestClassifier):\n",
    "                shap_values = shap_values[:,:,1]\n",
    "        elif isinstance(selected_model, cb.CatBoostClassifier):\n",
    "            explainer = shap.TreeExplainer(selected_model)\n",
    "            shap_values_extval = explainer.shap_values(X_extval)\n",
    "        elif isinstance(selected_model, LogisticRegression):\n",
    "            explainer = shap.LinearExplainer(selected_model, X_train_OHE_nocv)\n",
    "            shap_values_extval = explainer.shap_values(X_extval_OHE)\n",
    "        elif isinstance(selected_model, GaussianNB):  \n",
    "            explainer = shap.Explainer(selected_model.predict_proba, X_train_OHE_nocv)\n",
    "            shap_values_extval = explainer(X_extval_OHE)  \n",
    "            shap_values_extval = shap_values_extval.values[:,:,1]\n",
    "        else:\n",
    "            explainer = shap.TreeExplainer(selected_model)\n",
    "            shap_values_extval = explainer.shap_values(X_extval)\n",
    "            \n",
    "        # Determine which features to use based on the model type\n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "            feature_names_with_shapvalues = [\n",
    "                data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "                for feature, value in zip(X_extval_OHE.columns, np.mean(np.abs(shap_values_extval), axis=0)) \n",
    "            ]\n",
    "        else:\n",
    "            feature_names_with_shapvalues = [\n",
    "                data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "                for feature, value in zip(X_test.columns, np.mean(np.abs(shap_values_extval), axis=0)) \n",
    "            ]\n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "            shap_summary_plot(shap_values=shap_values_extval, data=X_extval_OHE, model_name = f'finalmodel_{type(selected_model).__name__}')\n",
    "        else:\n",
    "            shap_summary_plot(shap_values=shap_values_extval, data=X_extval, model_name = f'finalmodel_{type(selected_model).__name__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3db210c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "\n",
    "        predictions = selected_model.predict_proba(X_test_OHE)\n",
    "        predictions = predictions[:, 1]\n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier)):\n",
    "            y_pred = [True if x >= opt_threshold_HGBC else False for x in predictions]\n",
    "        elif isinstance(selected_model, (RandomForestClassifier)):\n",
    "            y_pred = [True if x >= opt_threshold_rf else False for x in predictions]\n",
    "        elif isinstance(selected_model, (LogisticRegression)):\n",
    "            y_pred = [True if x >= opt_threshold_LR else False for x in predictions]\n",
    "        else:\n",
    "            y_pred = [True if x >= opt_threshold_NB else False for x in predictions]\n",
    "        \n",
    "        misclassified = y_pred != y_test\n",
    "    elif isinstance(selected_model, (cb.CatBoostClassifier)):\n",
    "        predictions = selected_model.predict_proba(X_test)\n",
    "        predictions = predictions[:, 1]\n",
    "        y_pred = [True if x >= opt_threshold_CB else False for x in predictions]\n",
    "        misclassified = y_pred != y_test\n",
    "    elif isinstance(selected_model, (lgb.LGBMClassifier)):\n",
    "        predictions = selected_model.predict_proba(X_test)\n",
    "        predictions = predictions[:, 1]\n",
    "        y_pred = [True if x >= opt_threshold_LGBM else False for x in predictions]\n",
    "        misclassified = y_pred != y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d49e9b",
   "metadata": {},
   "source": [
    "##### Model interpretation only based on correctly classified samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6427d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    shap_values_CorrectClassified = shap_values[misclassified == False]\n",
    "    # Retrieve feature names from the data dictionary\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier,LogisticRegression, GaussianNB)):\n",
    "        X_test_CorrectClassified_OHE = X_test_OHE[misclassified == False]\n",
    "    else:\n",
    "        X_test_CorrectClassified = X_test[misclassified == False]\n",
    "        \n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        shap_summary_plot(shap_values=shap_values_CorrectClassified, data=X_test_CorrectClassified_OHE, model_name = f'finalmodel_{type(selected_model).__name__}')\n",
    "    else:\n",
    "        shap_summary_plot(shap_values=shap_values_CorrectClassified, data=X_test_CorrectClassified, model_name = f'finalmodel_{type(selected_model).__name__}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35215be5",
   "metadata": {},
   "source": [
    "##### Feature interactions based on SHAP method\n",
    "\n",
    "The below code generates a heatmap visualization representing the interaction between features using SHAP (SHapley Additive exPlanations) values. First, it computes the sum of absolute SHAP values for each pair of features, averaging them over all samples. These interaction scores are stored in an interaction matrix. Next, the interaction matrix is converted into a dataframe for easier plotting, with features as both rows and columns. A mask is created to hide the upper triangle of the heatmap, to eliminate redundant information. Finally, the heatmap is plotted using Seaborn, with feature names on both axes and interaction scores as annotations, providing a visual representation of feature interactions in the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6361710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # Separate SHAP values and y_test into two classes\n",
    "    class_0_indices = np.where(y_test == False)[0]\n",
    "    class_1_indices = np.where(y_test == True)[0]\n",
    "\n",
    "    shap_values_class_0 = shap_values[class_0_indices]\n",
    "    shap_values_class_1 = shap_values[class_1_indices]\n",
    "\n",
    "    # Get the number of features\n",
    "    num_samples, num_features = shap_values.shape\n",
    "\n",
    "    # Initialize matrices to store the median, min, and max SHAP values for each pair of features\n",
    "    interaction_matrix_median = np.zeros((num_features, num_features))\n",
    "    interaction_matrix_min = np.zeros((num_features, num_features))\n",
    "    interaction_matrix_max = np.zeros((num_features, num_features))\n",
    "\n",
    "    # Calculate the median, min, and max SHAP values for each pair of features\n",
    "    for i in range(num_features):\n",
    "        for j in range(i, num_features):\n",
    "            pairwise_shap_values = shap_values[:, i] + shap_values[:, j]\n",
    "            interaction_matrix_median[i, j] = np.median(pairwise_shap_values)\n",
    "            interaction_matrix_median[j, i] = interaction_matrix_median[i, j]\n",
    "            interaction_matrix_min[i, j] = np.min(pairwise_shap_values)\n",
    "            interaction_matrix_min[j, i] = interaction_matrix_min[i, j]\n",
    "            interaction_matrix_max[i, j] = np.max(pairwise_shap_values)\n",
    "            interaction_matrix_max[j, i] = interaction_matrix_max[i, j]\n",
    "\n",
    "    # Select appropriate test dataset based on the model type\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        df = X_test_OHE.copy()\n",
    "    else:\n",
    "        df = X_test.copy()\n",
    "\n",
    "    # Create DataFrames from the interaction matrices for easier plotting\n",
    "    interaction_df_median = pd.DataFrame(interaction_matrix_median, index=df.columns, columns=df.columns)\n",
    "    interaction_df_min = pd.DataFrame(interaction_matrix_min, index=df.columns, columns=df.columns)\n",
    "    interaction_df_max = pd.DataFrame(interaction_matrix_max, index=df.columns, columns=df.columns)\n",
    "\n",
    "    # Create a mask for the upper triangle excluding the diagonal\n",
    "    mask = np.triu(np.ones_like(interaction_df_median, dtype=bool))\n",
    "\n",
    "    height = round(np.max([10, np.log(interaction_df_median.shape[0])])) \n",
    "    # Ensure height does not exceed the maximum allowed dimension\n",
    "    max_height = 20000 / 72  # Convert pixels to inches\n",
    "    if height > max_height:\n",
    "        height = max_height\n",
    "\n",
    "    # Plot heatmaps for median, min, and max interactions\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, height*3))\n",
    "\n",
    "    sns.heatmap(interaction_df_median, annot=True, mask=mask, cmap='coolwarm', linewidths=0.5, fmt=\".1f\", annot_kws={\"size\": 8}, ax=axs[0])\n",
    "    axs[0].set_title('Feature interaction heatmap based on median SHAP values')\n",
    "\n",
    "    sns.heatmap(interaction_df_min, annot=True, mask=mask, cmap='coolwarm', linewidths=0.5, fmt=\".1f\", annot_kws={\"size\": 8}, ax=axs[1])\n",
    "    axs[1].set_title('Feature interaction heatmap based on minimum SHAP values')\n",
    "\n",
    "    sns.heatmap(interaction_df_max, annot=True, mask=mask, cmap='coolwarm', linewidths=0.5, fmt=\".1f\", annot_kws={\"size\": 8}, ax=axs[2])\n",
    "    axs[2].set_title('Feature interaction heatmap based on maximum SHAP values')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('f_interaction_heatmap.tif', bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Create a box plot to show the distribution of interactions for each feature pair\n",
    "    interaction_values = []\n",
    "    feature_pairs = []\n",
    "\n",
    "    for i in range(num_features):\n",
    "        for j in range(i, num_features):\n",
    "            pairwise_shap_values = shap_values[:, i] + shap_values[:, j]\n",
    "            interaction_values.extend(pairwise_shap_values)\n",
    "            feature_pairs.extend([f\"{df.columns[i]} & {df.columns[j]}\"] * num_samples)\n",
    "\n",
    "    interaction_df = pd.DataFrame({'Feature Pair': feature_pairs, 'Interaction Value': interaction_values})\n",
    "\n",
    "    # Calculate median SHAP values for each feature pair\n",
    "    median_values = interaction_df.groupby('Feature Pair')['Interaction Value'].median().sort_values(ascending=False)\n",
    "\n",
    "    # Determine the cutoff values for the top 10% and lowest 10%\n",
    "    top_10_percent_threshold = median_values.quantile(0.90)\n",
    "    bottom_10_percent_threshold = median_values.quantile(0.10)\n",
    "\n",
    "    # Identify the feature pairs within the top 10% and lowest 10%\n",
    "    top_10_percent_pairs = median_values[median_values >= top_10_percent_threshold].index\n",
    "    bottom_10_percent_pairs = median_values[median_values <= bottom_10_percent_threshold].index\n",
    "\n",
    "    # Combine the top and bottom feature pairs\n",
    "    selected_pairs = top_10_percent_pairs.append(bottom_10_percent_pairs)\n",
    "\n",
    "    # Filter the DataFrame to include only the selected feature pairs\n",
    "    interaction_df_filtered = interaction_df[interaction_df['Feature Pair'].isin(selected_pairs)]\n",
    "\n",
    "    # Determine figure size based on the number of variables\n",
    "    height = round(np.max([10, np.log(interaction_df_filtered.shape[0])])) \n",
    "\n",
    "    # Ensure height does not exceed the maximum allowed dimension\n",
    "    max_height = 20000 / 72  # Convert pixels to inches\n",
    "    if height > max_height:\n",
    "        height = max_height\n",
    "\n",
    "    # Plot the box plot with the filtered data\n",
    "    plt.figure(figsize=(height, 5))\n",
    "    sns.boxplot(x='Feature Pair', y='Interaction Value', data=interaction_df_filtered, order=selected_pairs)\n",
    "    # Enable autoscaling\n",
    "    plt.autoscale(enable=True, axis='both')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.title('Box plot of feature interaction values for top and bottom 10% median SHAP values')\n",
    "    plt.savefig('f_interaction_filtered_bplot.tif', dpi = 300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa5ca2",
   "metadata": {},
   "source": [
    "The following script generates several plots to visualize the feature interactions based on SHAP (SHapley Additive exPlanations) values for different classes in a binary classification problem. The specific plots created are:\n",
    "\n",
    "1. **Heatmaps of Feature Interactions:**\n",
    "   - **Median SHAP values**: A heatmap showing the median SHAP interaction values between each pair of features.\n",
    "   - **Minimum SHAP values**: A heatmap displaying the minimum SHAP interaction values for each pair of features.\n",
    "   - **Maximum SHAP values**: A heatmap depicting the maximum SHAP interaction values for each pair of features.\n",
    "   \n",
    "   These heatmaps are created separately for each class (`False` and `True`). The upper triangle of the heatmap (excluding the diagonal) is masked to avoid redundant information.\n",
    "\n",
    "2. **Box Plots of Feature Interactions:**\n",
    "   - **Top and Bottom 10% feature pairs**: A box plot highlighting the feature pairs that fall within the top 10% and bottom 10% of median SHAP interaction values. This helps identify the most and least significant interactions.\n",
    "\n",
    "Each type of plot is generated for both classes, resulting in comprehensive visualizations that facilitate the understanding of how different features interact and contribute to the model's predictions. The plots are saved as TIFF files for further analysis and presentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cecaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # Define the classes\n",
    "    classes = [False, True]\n",
    "\n",
    "    for current_class in classes:\n",
    "        # Get the indices for the current class\n",
    "        class_indices = np.where(y_test == current_class)[0]\n",
    "        \n",
    "        # Extract SHAP values for the current class\n",
    "        shap_values_class = shap_values[class_indices]\n",
    "        \n",
    "        # Get the number of features\n",
    "        num_samples, num_features = shap_values_class.shape\n",
    "\n",
    "        # Initialize matrices to store the median, min, and max SHAP values for each pair of features\n",
    "        interaction_matrix_median = np.zeros((num_features, num_features))\n",
    "        interaction_matrix_min = np.zeros((num_features, num_features))\n",
    "        interaction_matrix_max = np.zeros((num_features, num_features))\n",
    "\n",
    "        # Calculate the median, min, and max SHAP values for each pair of features\n",
    "        for i in range(num_features):\n",
    "            for j in range(i, num_features):\n",
    "                pairwise_shap_values = shap_values_class[:, i] + shap_values_class[:, j]\n",
    "                interaction_matrix_median[i, j] = np.median(pairwise_shap_values)\n",
    "                interaction_matrix_median[j, i] = interaction_matrix_median[i, j]\n",
    "                interaction_matrix_min[i, j] = np.min(pairwise_shap_values)\n",
    "                interaction_matrix_min[j, i] = interaction_matrix_min[i, j]\n",
    "                interaction_matrix_max[i, j] = np.max(pairwise_shap_values)\n",
    "                interaction_matrix_max[j, i] = interaction_matrix_max[i, j]\n",
    "\n",
    "        # Select appropriate test dataset based on the model type\n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "            df = X_test_OHE.copy()\n",
    "        else:\n",
    "            df = X_test.copy()\n",
    "\n",
    "        # Create DataFrames from the interaction matrices for easier plotting\n",
    "        interaction_df_median = pd.DataFrame(interaction_matrix_median, index=df.columns, columns=df.columns)\n",
    "        interaction_df_min = pd.DataFrame(interaction_matrix_min, index=df.columns, columns=df.columns)\n",
    "        interaction_df_max = pd.DataFrame(interaction_matrix_max, index=df.columns, columns=df.columns)\n",
    "\n",
    "        # Create a mask for the upper triangle excluding the diagonal\n",
    "        mask = np.triu(np.ones_like(interaction_df_median, dtype=bool))\n",
    "        height = round(np.max([10, np.log(interaction_df_median.shape[0])])) \n",
    "        # Ensure height does not exceed the maximum allowed dimension\n",
    "        max_height = 20000 / 72  # Convert pixels to inches\n",
    "        if height > max_height:\n",
    "            height = max_height\n",
    "\n",
    "        # Plot heatmaps for median, min, and max interactions\n",
    "        fig, axs = plt.subplots(3, 1, figsize=(10, height*3))\n",
    "\n",
    "        sns.heatmap(interaction_df_median, annot=True, mask=mask, cmap='coolwarm', linewidths=0.5, fmt=\".1f\", annot_kws={\"size\": 8}, ax=axs[0])\n",
    "        axs[0].set_title(f'Feature interaction heatmap based on median SHAP values for class {int(current_class)}')\n",
    "\n",
    "        sns.heatmap(interaction_df_min, annot=True, mask=mask, cmap='coolwarm', linewidths=0.5, fmt=\".1f\", annot_kws={\"size\": 8}, ax=axs[1])\n",
    "        axs[1].set_title(f'Feature interaction heatmap based on minimum SHAP values for class {int(current_class)}')\n",
    "\n",
    "        sns.heatmap(interaction_df_max, annot=True, mask=mask, cmap='coolwarm', linewidths=0.5, fmt=\".1f\", annot_kws={\"size\": 8}, ax=axs[2])\n",
    "        axs[2].set_title(f'Feature interaction heatmap based on maximum SHAP values for class {int(current_class)}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'f_interaction_heatmap_{current_class}.tif', bbox_inches='tight')\n",
    "        # plt.show()\n",
    "\n",
    "        # Create a box plot to show the distribution of interactions for each feature pair\n",
    "        interaction_values = []\n",
    "        feature_pairs = []\n",
    "\n",
    "        for i in range(num_features):\n",
    "            for j in range(i, num_features):\n",
    "                pairwise_shap_values = shap_values_class[:, i] + shap_values_class[:, j]\n",
    "                interaction_values.extend(pairwise_shap_values)\n",
    "                feature_pairs.extend([f\"{df.columns[i]} & {df.columns[j]}\"] * num_samples)\n",
    "\n",
    "        interaction_df = pd.DataFrame({'Feature Pair': feature_pairs, 'Interaction Value': interaction_values})\n",
    "        \n",
    "        # Remove feature pairs where a feature interacts with itself\n",
    "        interaction_df = interaction_df[~interaction_df['Feature Pair'].apply(lambda x: x.split(' & ')[0] == x.split(' & ')[1])]\n",
    "\n",
    "        # Calculate median SHAP values for each feature pair\n",
    "        median_values = interaction_df.groupby('Feature Pair')['Interaction Value'].median().sort_values(ascending=False)\n",
    "\n",
    "        # Determine the cutoff values for the top 10% and lowest 10%\n",
    "        top_10_percent_threshold = median_values.quantile(0.90)\n",
    "        bottom_10_percent_threshold = median_values.quantile(0.10)\n",
    "\n",
    "        # Identify the feature pairs within the top 10% and lowest 10%\n",
    "        top_10_percent_pairs = median_values[median_values >= top_10_percent_threshold].index\n",
    "        bottom_10_percent_pairs = median_values[median_values <= bottom_10_percent_threshold].index\n",
    "\n",
    "        # Combine the top and bottom feature pairs\n",
    "        selected_pairs = top_10_percent_pairs.append(bottom_10_percent_pairs)\n",
    "\n",
    "        # Filter the DataFrame to include only the selected feature pairs\n",
    "        interaction_df_filtered = interaction_df[interaction_df['Feature Pair'].isin(selected_pairs)]\n",
    "\n",
    "        # Determine figure size based on the number of variables\n",
    "        height = round(np.max([10, np.log(interaction_df_filtered.shape[0])])) \n",
    "\n",
    "        # Ensure height does not exceed the maximum allowed dimension\n",
    "        max_height = 20000 / 72  # Convert pixels to inches\n",
    "        if height > max_height:\n",
    "            height = max_height\n",
    "\n",
    "        # Plot the box plot with the filtered data\n",
    "        plt.figure(figsize=(height, 5))\n",
    "        sns.boxplot(x='Feature Pair', y='Interaction Value', data=interaction_df_filtered, order=selected_pairs)\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.title(f'Box plot of feature interaction values for top and bottom 10% median SHAP values for class {int(current_class)}')\n",
    "        plt.savefig(f'f_interaction_filtered_bplot_{current_class}.tif', dpi = 300)\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b3a220",
   "metadata": {},
   "source": [
    "In this context, \"interaction\" refers to the combined effect of two features on the model's prediction, as measured by their SHAP values. SHAP (SHapley Additive exPlanations) values provide a way to interpret the contribution of each feature to the prediction of a machine learning model. Here, the interaction between two features is quantified by combining their SHAP values and assessing how these combined values influence the prediction.\n",
    "\n",
    "Specifically:\n",
    "\n",
    "1. **Pairwise SHAP Values**: The interaction between two features \\( i \\) and \\( j \\) is evaluated by summing their individual SHAP values for each sample. This summed value represents the joint contribution of both features to the prediction for that sample.\n",
    "\n",
    "2. **Interaction Metrics**:\n",
    "   - **Median Interaction**: The median of the pairwise SHAP values across all samples for a given class.\n",
    "   - **Minimum Interaction**: The minimum of the pairwise SHAP values across all samples for a given class.\n",
    "   - **Maximum Interaction**: The maximum of the pairwise SHAP values across all samples for a given class.\n",
    "\n",
    "These metrics are calculated for each pair of features, resulting in matrices that summarize the interactions. The script then visualizes these interactions through heatmaps and box plots to provide insights into how pairs of features work together to influence the model's predictions for different classes.\n",
    "\n",
    "### Plots and Their Representations:\n",
    "\n",
    "1. **Heatmaps**:\n",
    "   - **Median SHAP Interaction Heatmap**: Shows the median combined effect of feature pairs.\n",
    "   - **Minimum SHAP Interaction Heatmap**: Shows the smallest combined effect of feature pairs.\n",
    "   - **Maximum SHAP Interaction Heatmap**: Shows the largest combined effect of feature pairs.\n",
    "\n",
    "2. **Box Plots**:\n",
    "   - **Top and Bottom 10% Feature Pairs Box Plot**: Highlights the feature pairs with the most and least significant interactions, based on the median SHAP values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afa3726",
   "metadata": {},
   "source": [
    "##### Feature interactions based on feature permutation method for feature pairs\n",
    "\n",
    "this code provides insight into the interaction effects between pairs of features in the machine learning model, helping identify which combinations of features contribute significantly to the model's performance.\n",
    "\n",
    "- The `permute_feature_pairs` function calculates the permutation importances for pairs of features.\n",
    "- It converts the binary target variable to numeric format and calculates the baseline score using ROC AUC.\n",
    "- For each pair of features, it shuffles their values multiple times and computes the change in ROC AUC compared to the baseline. The average change in ROC AUC is stored as the importance score for that feature pair.\n",
    "\n",
    "- It generates all possible pairs of features from the input feature set.\n",
    "- It computes the permutation importances for pairs of features using the defined function.\n",
    "- The results are stored in a DataFrame, where each row represents a feature pair along with its importance score.\n",
    "- The DataFrame is sorted based on importance in descending order and printed to display the importance of feature pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "18b542eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # Function to permute pairs of features\n",
    "    if find_interacting_feature_permutation:\n",
    "        def permute_feature_pairs(model, X, y, pairs, n_repeats, random_state, scoring, n_jobs):\n",
    "            y = y.replace({class_1: 1, class_0: 0})\n",
    "            baseline_score = roc_auc_score(y, model.predict_proba(X)[:, 1])\n",
    "            importance_dict = {}\n",
    "\n",
    "            for feature1, feature2 in pairs:\n",
    "                importances = []\n",
    "                for _ in range(n_repeats):\n",
    "                    X_permuted = X.copy()\n",
    "                    X_permuted[[feature1, feature2]] = X_permuted[[feature1, feature2]].sample(frac=1, random_state=random_state).values\n",
    "                    permuted_score = roc_auc_score(y, model.predict_proba(X_permuted)[:, 1])\n",
    "                    importances.append(baseline_score - permuted_score)\n",
    "                \n",
    "                importance_dict[(feature1, feature2)] = sum(importances) / n_repeats  # Average importance over n_repeats\n",
    "            \n",
    "            return importance_dict\n",
    "\n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier,LogisticRegression, GaussianNB)):\n",
    "            df = X_test_OHE.copy()\n",
    "        else:\n",
    "            df = X_test.copy()\n",
    "\n",
    "        # Define the features\n",
    "        features = df.columns.tolist()\n",
    "        pairs = list(itertools.combinations(features, 2))\n",
    "\n",
    "        # Calculate the permutation importances for pairs of features\n",
    "        pair_importances = permute_feature_pairs(\n",
    "            selected_model, df, y_test, pairs, n_repeats = n_rep_feature_permutation, random_state = SEED, scoring = custom_scorer, n_jobs=n_cpu_model_training\n",
    "        )\n",
    "\n",
    "        # Create a DataFrame to store the results\n",
    "        pair_importance_df = pd.DataFrame(\n",
    "            {\"feature pair\": [f\"{pair[0]} + {pair[1]}\" for pair in pair_importances.keys()],\n",
    "            \"importance\": pair_importances.values()}\n",
    "        )\n",
    "\n",
    "        # Sort by importance\n",
    "        pair_importance_df = pair_importance_df.sort_values(by=\"importance\", ascending=False)\n",
    "        print(pair_importance_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6b7cd",
   "metadata": {},
   "source": [
    "##### SHAP decision plot \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51559ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # Plot the SHAP decision plot with only significant features\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier)):\n",
    "        shap.decision_plot(explainer.expected_value, \n",
    "                        shap_values,\n",
    "                        X_test_OHE,\n",
    "                        alpha=0.5, \n",
    "                        feature_names=feature_names_with_shapvalues,\n",
    "                        link='logit',\n",
    "                        highlight=misclassified,\n",
    "                        ignore_warnings=True,\n",
    "                        feature_order = None)\n",
    "    elif isinstance(selected_model, (RandomForestClassifier)):\n",
    "        shap.decision_plot(explainer.expected_value[positive_class_index], \n",
    "                        shap_values,\n",
    "                        X_test_OHE,\n",
    "                        alpha=0.5, \n",
    "                        feature_names=feature_names_with_shapvalues,\n",
    "                        link='logit',\n",
    "                        highlight=misclassified,\n",
    "                        ignore_warnings=True,\n",
    "                        feature_order = None)\n",
    "    elif isinstance(selected_model, (LogisticRegression)):\n",
    "        shap.decision_plot(explainer.expected_value, \n",
    "                        shap_values,\n",
    "                        X_test_OHE,\n",
    "                        alpha=0.5, \n",
    "                        feature_names=feature_names_with_shapvalues,\n",
    "                        link='logit',\n",
    "                        highlight=misclassified,\n",
    "                        ignore_warnings=True,\n",
    "                        feature_order = None)\n",
    "    elif isinstance(selected_model, (cb.CatBoostClassifier)):\n",
    "        shap.decision_plot(explainer.expected_value, \n",
    "                        shap_values,\n",
    "                        X_test,\n",
    "                        alpha=0.5, \n",
    "                        feature_names=feature_names_with_shapvalues,\n",
    "                        link='logit',\n",
    "                        highlight=misclassified,\n",
    "                        ignore_warnings=True,\n",
    "                        feature_order = None)\n",
    "    elif isinstance(selected_model, (GaussianNB)):\n",
    "        shap_values_NB = explainer(X_test_OHE)\n",
    "        shap.decision_plot(base_value = shap_values_NB.base_values[1][1],\n",
    "                        shap_values = shap_values,\n",
    "                        features = X_test_OHE,\n",
    "                        alpha=0.5, \n",
    "                        feature_names=feature_names_with_shapvalues,\n",
    "                        link='logit',\n",
    "                        highlight=misclassified, \n",
    "                        ignore_warnings=True,\n",
    "                        feature_order = None)\n",
    "    else:\n",
    "        shap.decision_plot(explainer.expected_value, \n",
    "                        shap_values,\n",
    "                        X_test,\n",
    "                        alpha=0.5, \n",
    "                        feature_names=feature_names_with_shapvalues,\n",
    "                        link='logit',\n",
    "                        highlight=misclassified, \n",
    "                        ignore_warnings=True,\n",
    "                        feature_order = None)\n",
    "\n",
    "    plt.gca().set_facecolor('white')  \n",
    "    # display grid lines\n",
    "    plt.grid(which='both', color=\"grey\")\n",
    "    # modify grid lines\n",
    "    plt.grid(which='minor', alpha=0.2)\n",
    "    plt.grid(which='major', alpha=0.3)\n",
    "    plt.savefig('shap_decision_allfeats_plot.tif', bbox_inches='tight')\n",
    "\n",
    "    # Close the extra figure\n",
    "    plt.close()\n",
    "    # show the plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a76815",
   "metadata": {},
   "source": [
    "##### SHAP decision plot description\n",
    "\n",
    "The SHAP decision plot centers around the `explainer.expected_value` on the x-axis, with colored lines representing predictions for each observation. Moving upwards, these lines intersect the x-axis at the prediction specific to each observation, depicted in varying colors on a gradient scale. The plot integrates SHAP values for each feature, illustrating their contributions to the overall prediction relative to the model's baseline value. At the plot's bottom, observations converge at `explainer.expected_value`.\n",
    "\n",
    "1. **Demonstrating feature effects:**\n",
    "   - Visualizes the impact of multiple features on predictions and their individual contributions.\n",
    "\n",
    "2. **Revealing interaction effects:**\n",
    "   - shows how interactions between features influence predictions by incorporating SHAP values.\n",
    "\n",
    "3. **Exploring feature effects across values:**\n",
    "   - Enables exploration of feature effects by showcasing prediction variations across different feature values.\n",
    "\n",
    "4. **Identifying outliers:**\n",
    "   - Enables outlier detection by pinpointing observations deviating significantly from expected values or prediction trends.\n",
    "\n",
    "5. **Understanding prediction paths:**\n",
    "   - Facilitates the identification of common prediction patterns, offering insight into model behavior.\n",
    "\n",
    "6. **Model comparison:**\n",
    "   - Allows comparing predictions across multiple models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8efec86",
   "metadata": {},
   "source": [
    "##### SHAP dependence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    sns.set(style=\"ticks\")\n",
    "\n",
    "    # Set the background color to white\n",
    "    sns.set_style(\"white\")\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "    # Compute median absolute SHAP values for each feature\n",
    "    median_abs_shap_values = np.median(np.abs(shap_values), axis=0)\n",
    "\n",
    "    # Sort features by median absolute SHAP values in descending order\n",
    "    sorted_features = np.argsort(median_abs_shap_values)[::-1]\n",
    "\n",
    "    # Calculate the number of features to plot\n",
    "    # num_features_to_plot = min(np.sum(median_abs_shap_values > 0), top_n_f)\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        num_features_to_plot = min(top_n_f,X_test_OHE.shape[1])\n",
    "    else:\n",
    "        num_features_to_plot = min(top_n_f,X_test.shape[1])\n",
    "\n",
    "    # Set the number of columns for subplots\n",
    "    num_cols = 4\n",
    "\n",
    "    # Calculate the number of rows for subplots\n",
    "    num_rows = int(np.ceil(num_features_to_plot / num_cols))\n",
    "\n",
    "    # Initialize a subplot\n",
    "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "    axs = axs.ravel()\n",
    "\n",
    "    # Track the current subplot index\n",
    "    current_subplot = 0\n",
    "\n",
    "    # Iterate over the top features\n",
    "    for feature in sorted_features[:num_features_to_plot]:\n",
    "        # Get feature name\n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "            feature_name = X_test_OHE.columns[feature]\n",
    "            x = X_test_OHE.iloc[:, feature]\n",
    "        else:\n",
    "            feature_name = X_test.columns[feature]\n",
    "            if X_test[feature_name].dtype.name == 'category':\n",
    "                # Convert categorical feature to numerical using LabelEncoder\n",
    "                encoder = LabelEncoder()\n",
    "                X_test_encoded = X_test.copy()\n",
    "                X_test_encoded[feature_name] = encoder.fit_transform(X_test[feature_name])\n",
    "                x = X_test_encoded.iloc[:, feature].astype(float)\n",
    "            else:\n",
    "                x = X_test.iloc[:, feature].astype(float)\n",
    "        \n",
    "        # Handle missing values in feature values and SHAP values\n",
    "        mask_x = ~pd.isnull(x)\n",
    "        mask_shap = ~np.isnan(shap_values[:, feature])\n",
    "        mask = mask_x & mask_shap\n",
    "        \n",
    "        x_filtered = x[mask]\n",
    "        shap_values_filtered = shap_values[:, feature][mask]\n",
    "        predictions_filtered = predictions[mask]\n",
    "        misclassified_filtered = misclassified[mask]\n",
    "        \n",
    "        # Check if all x values are identical\n",
    "        if len(np.unique(x_filtered)) == 1:\n",
    "            print(f\"Skipped feature {feature_name} because all x values are identical.\")\n",
    "            continue\n",
    "        \n",
    "        # Calculate Spearman correlation coefficient and p-value\n",
    "        correlation, p_value = spearmanr(x_filtered, shap_values_filtered, nan_policy='omit')\n",
    "        \n",
    "        # Create scatter plot in the current subplot\n",
    "        scatter = axs[current_subplot].scatter(x_filtered, shap_values_filtered, c=predictions_filtered, cmap='viridis', alpha=0.7, s=50)\n",
    "        axs[current_subplot].set_xlabel(feature_name)\n",
    "        axs[current_subplot].set_ylabel(\"SHAP Value\")\n",
    "        \n",
    "        # Add correlation line\n",
    "        slope, intercept, r_value, p_value_corr, std_err = linregress(x_filtered, shap_values_filtered)\n",
    "        axs[current_subplot].plot(x_filtered, slope * x_filtered + intercept, color='red')\n",
    "        \n",
    "        # Mark misclassified samples with 'x'\n",
    "        axs[current_subplot].scatter(x_filtered[misclassified_filtered], shap_values_filtered[misclassified_filtered], marker=\"X\", color='red', alpha=0.5, s=50)\n",
    "        \n",
    "        # Customize colorbar\n",
    "        cbar = plt.colorbar(scatter, ax=axs[current_subplot])\n",
    "        cbar.set_label(\"Predicted Probability\")\n",
    "        \n",
    "        # Check if correlation is statistically significant\n",
    "        if not np.isnan(correlation) and not np.isnan(p_value_corr):\n",
    "            _, p_value_corr_test = ttest_rel(x_filtered, shap_values_filtered)\n",
    "            p_value_text = f\"p < 0.05\" if p_value_corr_test < 0.05 else f\"p = {p_value_corr_test:.2f}\"\n",
    "            axs[current_subplot].set_title(f\"{feature_name} vs. SHAP Value\\nSpearman Correlation: {correlation:.2f}, {p_value_text}\")\n",
    "        else:\n",
    "            axs[current_subplot].set_title(f\"{feature_name} vs. SHAP Value\\nCorrelation: N/A\")\n",
    "        \n",
    "        # Increment the current subplot index\n",
    "        current_subplot += 1\n",
    "\n",
    "    # Hide any remaining empty subplots\n",
    "    for i in range(current_subplot, num_rows * num_cols):\n",
    "        axs[i].axis('off')\n",
    "\n",
    "    # Adjust the spacing between subplots\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('shap_dependence_plot.tif', bbox_inches='tight')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a077611",
   "metadata": {},
   "source": [
    "##### SHAP clustering\n",
    "\n",
    "In the context of precision medicine, SHAP clustering serves to uncover patient subgroups with distinct patterns, leading to differential model behavior. These subgroups often manifest as low or high-risk clusters, with a potential third cluster exhibiting less decisive model behavior. Identifying these clusters aids in profiling patient subgroups, offering valuable insights for model application to new patients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fc1ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    def find_feature_clusters(X, shap_values, selected_model, data_dictionary, top_n_f):\n",
    "        # Create a DataFrame for SHAP values with feature names as columns\n",
    "        shap_df = pd.DataFrame(shap_values, columns=X.columns)\n",
    "\n",
    "        # Plot clustermap for both rows and columns\n",
    "        cluster_grid = sns.clustermap(shap_df)\n",
    "        plt.title('Clustermap for Features and Instances')\n",
    "        plt.show()\n",
    "        \n",
    "        # Perform hierarchical clustering on columns (features)\n",
    "        col_clusters = AgglomerativeClustering(n_clusters=3).fit_predict(shap_values.T)\n",
    "\n",
    "        # Group columns into clusters\n",
    "        features_in_clusters = {i: [] for i in range(3)}\n",
    "\n",
    "        for i, cluster_idx in enumerate(col_clusters):\n",
    "            features_in_clusters[cluster_idx].append(X.columns[i])\n",
    "\n",
    "        # Get top N clusters for features\n",
    "        top_n_col_clusters = sorted(features_in_clusters.keys(), key=lambda x: len(features_in_clusters[x]), reverse=True)[:top_n_f]\n",
    "        top_n_col_clusters_info = {cluster: features_in_clusters[cluster] for cluster in top_n_col_clusters}\n",
    "\n",
    "        # Determine the best number of clusters for rows (instances) using silhouette score\n",
    "        best_num_clusters = 3\n",
    "        best_silhouette_score = -1\n",
    "        silhouette_scores = []\n",
    "\n",
    "        for n_clusters in range(3, 6):\n",
    "            row_clusters = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(shap_values)\n",
    "            silhouette_avg = silhouette_score(shap_values, row_clusters)\n",
    "            silhouette_scores.append((n_clusters, silhouette_avg))\n",
    "            if silhouette_avg > best_silhouette_score:\n",
    "                best_silhouette_score = silhouette_avg\n",
    "                best_num_clusters = n_clusters\n",
    "\n",
    "        # Plot silhouette scores\n",
    "        cluster_counts, scores = zip(*silhouette_scores)\n",
    "        plt.figure()\n",
    "        plt.plot(cluster_counts, scores, marker='o')\n",
    "        plt.title('Silhouette Score vs. Number of Clusters')\n",
    "        plt.xlabel('Number of Clusters')\n",
    "        plt.ylabel('Silhouette Score')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Perform hierarchical clustering on rows (instances) with the best number of clusters\n",
    "        row_clusters = AgglomerativeClustering(n_clusters=best_num_clusters).fit_predict(shap_values)\n",
    "\n",
    "        # Group rows into clusters\n",
    "        instances_in_clusters = {i: [] for i in range(best_num_clusters)}\n",
    "\n",
    "        for i, cluster_idx in enumerate(row_clusters):\n",
    "            instances_in_clusters[cluster_idx].append(i)\n",
    "\n",
    "        # Get top N clusters for instances\n",
    "        top_N_row_clusters = sorted(instances_in_clusters.keys(), key=lambda x: len(instances_in_clusters[x]), reverse=True)[:best_num_clusters]\n",
    "        row_clusters_info = {cluster: instances_in_clusters[cluster] for cluster in top_N_row_clusters}\n",
    "\n",
    "        return top_n_col_clusters_info, row_clusters_info, shap_df\n",
    "\n",
    "\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier,LogisticRegression, GaussianNB)):\n",
    "        top_n_col_clusters_info, row_clusters_info, shap_df = find_feature_clusters(X=X_test_OHE, \n",
    "                                                                                    shap_values=shap_values,\n",
    "                                                                                    selected_model=selected_model, \n",
    "                                                                                    data_dictionary=data_dictionary, \n",
    "                                                                                    top_n_f=top_n_f)\n",
    "    elif isinstance(selected_model, (cb.CatBoostClassifier, lgb.LGBMClassifier)):\n",
    "        top_n_col_clusters_info, row_clusters_info, shap_df = find_feature_clusters(X=X_test, \n",
    "                                                                                    shap_values=shap_values,\n",
    "                                                                                    selected_model=selected_model, \n",
    "                                                                                    data_dictionary=data_dictionary, \n",
    "                                                                                    top_n_f=top_n_f)\n",
    "        \n",
    "    print(\"\\nTop N clusters for features:\")\n",
    "    for cluster, features in top_n_col_clusters_info.items():\n",
    "        print(f\"Cluster {cluster}: {features}\")\n",
    "\n",
    "    print(\"\\nTop N clusters for instances:\")\n",
    "    for cluster, instances in row_clusters_info.items():\n",
    "        print(f\"Cluster {cluster}: {instances}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    def plot_confusion_matrix_for_clusters(X, y, cluster_info, model, shap_values, top_n):\n",
    "        # Get unique class labels from your data\n",
    "        unique_labels = np.unique(y)\n",
    "\n",
    "        # Get unique cluster labels\n",
    "        unique_clusters = list(cluster_info.keys())\n",
    "\n",
    "        # Iterate over each cluster\n",
    "        for cluster in unique_clusters:\n",
    "            # Subset the test data for the current cluster\n",
    "            cluster_indices = cluster_info[cluster]\n",
    "            X_cluster = X.iloc[cluster_indices]\n",
    "            y_cluster = y.iloc[cluster_indices]\n",
    "            shap_values_cluster = shap_values[cluster_indices]\n",
    "\n",
    "            if isinstance(model, (RandomForestClassifier)):\n",
    "                predictions_proba_cluster = model.predict_proba(X_cluster)[:, 1]\n",
    "                model_opt_threshold = opt_threshold_rf\n",
    "            elif model == 'QLattice':\n",
    "                predictions_proba_cluster = model.predict(X_cluster)[:, 1]\n",
    "            elif isinstance(model, (HistGradientBoostingClassifier)):\n",
    "                predictions_proba_cluster = model.predict_proba(X_cluster)[:, 1]\n",
    "                model_opt_threshold = opt_threshold_HGBC\n",
    "            elif isinstance(model, (lgb.LGBMClassifier)):\n",
    "                predictions_proba_cluster = model.predict_proba(X_cluster)[:, 1]\n",
    "                model_opt_threshold = opt_threshold_LGBM\n",
    "            elif isinstance(model, (cb.CatBoostClassifier)):\n",
    "                predictions_proba_cluster = model.predict_proba(X_cluster)[:, 1]\n",
    "                model_opt_threshold = opt_threshold_CB\n",
    "            elif isinstance(model, (LogisticRegression)):\n",
    "                predictions_proba_cluster = model.predict_proba(X_cluster)[:, 1]\n",
    "                model_opt_threshold = opt_threshold_LR\n",
    "            elif isinstance(model, (GaussianNB)):\n",
    "                predictions_proba_cluster = model.predict_proba(X_cluster)[:, 1]\n",
    "                model_opt_threshold = opt_threshold_NB\n",
    "            \n",
    "            predictions_class_cluster = np.where(predictions_proba_cluster >= model_opt_threshold, True, False)\n",
    "\n",
    "            # Compute confusion matrix\n",
    "            cm = confusion_matrix(y_cluster, predictions_class_cluster, labels=[False, True])\n",
    "\n",
    "            # Plot confusion matrix with actual label names\n",
    "            plt.figure(figsize=(4, 3))  # Adjust figure size as needed\n",
    "            myheatmap = sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=class_labels_display, yticklabels=class_labels_display, annot_kws={\"size\": 8})  # Adjust annot_kws to change annotation font size\n",
    "            myheatmap.invert_yaxis()\n",
    "            plt.title(f\"Confusion Matrix for Cluster {cluster}\", fontsize=10)\n",
    "            plt.xlabel(\"Predicted Label\", fontsize=8)\n",
    "            plt.ylabel(\"True Label\", fontsize=8)\n",
    "            plt.xticks(fontsize=8) \n",
    "            plt.yticks(fontsize=8)  \n",
    "            plt.show()\n",
    "\n",
    "            # Call categorical_shap_plot for the current cluster\n",
    "            if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "                shap.summary_plot(shap_values_cluster, X_cluster, show=False, alpha = 0.8, max_display=top_n_f)\n",
    "            elif any(X_test[column].dtype == 'category' for column in X_test.columns):\n",
    "                categorical_shap_plot(shap_values=shap_values_cluster, \n",
    "                                            data=X_cluster,\n",
    "                                            top_n=min(top_n, X_cluster.shape[1]),\n",
    "                                            jitter=0.1)\n",
    "            else: # it could be a LGBM or CATBoost model based on only numerical features \n",
    "                shap.summary_plot(shap_values_cluster, X_cluster, show=False, alpha = 0.8, max_display=top_n_f)\n",
    "\n",
    "    # based on the assumption that cluster_info is the dictionary indicating the instances in each cluster\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier,LogisticRegression, GaussianNB)):\n",
    "        plot_confusion_matrix_for_clusters(X=X_test_OHE, y=y_test, cluster_info=row_clusters_info, model=selected_model, shap_values=shap_values, top_n=top_n_f)\n",
    "\n",
    "    elif isinstance(selected_model, (cb.CatBoostClassifier, lgb.LGBMClassifier)):\n",
    "        plot_confusion_matrix_for_clusters(X=X_test, y=y_test, cluster_info=row_clusters_info, model=selected_model, shap_values=shap_values, top_n=top_n_f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b6e5d0",
   "metadata": {},
   "source": [
    "#### SHAP force plot for individuals (e.g., one patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc4c30e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # load JS visualization code to notebook\n",
    "    shap.initjs()\n",
    "    # Function to get a sample from each class\n",
    "    def get_samples_per_class(X, y):\n",
    "        samples = {}\n",
    "        for class_value in np.unique(y):\n",
    "            # Select a sample from each class\n",
    "            class_samples = X[y == class_value]\n",
    "            if class_samples.shape[0] > 0:\n",
    "                samples[class_value] = class_samples.sample(n=1, random_state=SEED)  # Random sample\n",
    "        return samples\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        # Get samples from each class\n",
    "        mysamples = get_samples_per_class(X_test_OHE, y_test)\n",
    "    else:\n",
    "        mysamples = get_samples_per_class(X_test, y_test)\n",
    "        \n",
    "    try:\n",
    "        # Generate SHAP force plots for each sample\n",
    "        for class_value, sample in mysamples.items():\n",
    "            \n",
    "            # Convert sample to appropriate format for explainer\n",
    "            if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "                sample_predicted_prob = selected_model.predict_proba(sample)\n",
    "                print(f\"predicted probability: {np.round(sample_predicted_prob,2)}\")\n",
    "                print(f\"class: {class_value}\")\n",
    "                # Get the SHAP values for the sample\n",
    "                sample_shap_values = explainer(sample)\n",
    "                # Generate the force plot\n",
    "                base_value = sample_shap_values.base_values\n",
    "                # shap.plots.force(base_value, sample_shap_values.values[:,:,1])\n",
    "                display(shap.plots.force(base_value, sample_shap_values.values, plot_cmap=\"RdBu\", feature_names=X_train_OHE_nocv.columns))\n",
    "            \n",
    "            elif isinstance(selected_model, (cb.CatBoostClassifier, lgb.LGBMClassifier)):\n",
    "                sample_predicted_prob = selected_model.predict_proba(sample)\n",
    "                print(f\"predicted probability: {sample_predicted_prob[:,1]}\")\n",
    "                # Get the SHAP values for the sample\n",
    "                sample_shap_values = explainer(sample)\n",
    "                # Generate the force plot\n",
    "                display(shap.plots.force(sample_shap_values, plot_cmap=\"RdBu\", feature_names=feature_names))\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}. Skipping to the next block.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dbf45c",
   "metadata": {},
   "source": [
    "#### Decision curve analysis\n",
    "Net benefit of the model compared to random guessing, extreme cases, and an alternative method or model. Read more here: https://en.wikipedia.org/wiki/Decision_curve_analysis#:~:text=Decision%20curve%20analysis%20evaluates%20a,are%20positive%20are%20also%20plotted.\n",
    "As an alternative model we here use logistic regression model but you can modify this or import prediction probabilities for the test samples from elsewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "199e1a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    if do_decision_curve_analysis:\n",
    "\n",
    "        # Calculate necessary information for adjusting hyperparameters\n",
    "        n_rows = X_train_OHE_nocv.shape[0]\n",
    "        n_cols = X_train_OHE_nocv.shape[1]\n",
    "        class_proportion = y_train.mean()  # binary classification\n",
    "        rf_params, lgbm_params, hgbc_params, cb_params, lr_params = set_parameters(n_rows, n_cols, class_proportion)\n",
    "        # Create a Logistic Regression instance\n",
    "        lr = LogisticRegression(penalty='l1',random_state=SEED, solver=\"liblinear\", **lr_params)\n",
    "        # lr = LogisticRegression(penalty='elasticnet',random_state=SEED, solver=\"saga\")\n",
    "        if hp_tuning:\n",
    "        \n",
    "            # Adjust hyperparameters based on the training data in this fold\n",
    "            rf_param_dist, lgbm_param_dist, hgbc_param_dist, cb_param_dist, lr_param_dist = adjust_hyperparameters(n_rows, n_cols)\n",
    "            # Create a RandomizedSearchCV instance\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=lr,\n",
    "                param_distributions=lr_param_dist,\n",
    "                n_iter=n_iter_hptuning,\n",
    "                scoring= custom_scorer, \n",
    "                cv=cv_folds_hptuning,\n",
    "                refit=True, \n",
    "                random_state=SEED,\n",
    "                verbose=0,\n",
    "                n_jobs=n_cpu_for_tuning\n",
    "            )\n",
    "\n",
    "            # Perform the random search on the training data\n",
    "            random_search.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)\n",
    "\n",
    "            # Get the best parameters and best estimator\n",
    "            best_params = random_search.best_params_\n",
    "            lr = LogisticRegression(penalty='l1',random_state=SEED, solver=\"liblinear\", **best_params)\n",
    "            # lr = LogisticRegression(penalty='elasticnet',random_state=SEED, solver=\"saga\", **best_params)\n",
    "        else:\n",
    "            lr = LogisticRegression(penalty='l1',random_state=SEED, solver=\"liblinear\", **lr_params)\n",
    "\n",
    "        # Fit the best estimator on the entire training data\n",
    "        lr.fit(X_train_OHE_nocv, y_train, sample_weight=sample_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ce2bbab",
   "metadata": {},
   "source": [
    "#### Cost-sensitive model evaluation\n",
    "\n",
    "Here we introduce cost-sensitive net benefit where we introduce weights as coefficients for the number of TP and FP cases.\n",
    "The weights are supposed to be positive values >0. Use w_tp=1, w_fp=1 for true positives and false positives respectively when they are equally important (equal cost)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4e553d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    if do_decision_curve_analysis:\n",
    "        def cost_sensitive_net_benefit(tp, fp, threshold, N, w_tp=1, w_fp=1):\n",
    "            # Calculate cost-sensitive net benefit\n",
    "            if N == 0:\n",
    "                return 0  # Prevent division by zero\n",
    "            net_benefit = (tp * w_tp / N) - (fp * w_fp / N) * (threshold / (1 - threshold))\n",
    "            return net_benefit\n",
    "        def decision_curve_analysis(pred_probs_selected_model, pred_probs_alternative_model, rand_pred_probs, w_tp=1, w_fp=1):\n",
    "            N = len(y_test)  # Total number of observations\n",
    "            \n",
    "            # Precompute TP and FP for extreme cases\n",
    "            tp_all_positive = np.sum(y_test.values == True)\n",
    "            fp_all_positive = np.sum(y_test.values == False)\n",
    "            \n",
    "            # Extreme cases do not vary by threshold\n",
    "            net_benefit_all_positive = [cost_sensitive_net_benefit(tp_all_positive, fp_all_positive, threshold, N, w_tp, w_fp) for threshold in np.linspace(0, 1, 100)]\n",
    "            net_benefit_all_negative = [cost_sensitive_net_benefit(0, 0, threshold, N, w_tp, w_fp) for threshold in np.linspace(0, 1, 100)]\n",
    "\n",
    "            # Initialize lists for storing net benefits\n",
    "            net_benefit_selected_model = []\n",
    "            net_benefit_alternative_model = []\n",
    "            net_benefit_rand = []\n",
    "\n",
    "            threshold_range = np.linspace(0, 1, 100)\n",
    "            \n",
    "            for threshold in threshold_range:\n",
    "                # Calculate TP and FP for selected model\n",
    "                tp_selected_model = np.sum((pred_probs_selected_model > threshold) & (y_test.values == True))\n",
    "                fp_selected_model = np.sum((pred_probs_selected_model > threshold) & (y_test.values == False))\n",
    "                \n",
    "                # Calculate TP and FP for alternative model\n",
    "                tp_alternative_model = np.sum((pred_probs_alternative_model > threshold) & (y_test.values == True))\n",
    "                fp_alternative_model = np.sum((pred_probs_alternative_model > threshold) & (y_test.values == False))\n",
    "                \n",
    "                # Calculate TP and FP for random predictions\n",
    "                tp_rand = np.sum((rand_pred_probs > threshold) & (y_test.values == True))\n",
    "                fp_rand = np.sum((rand_pred_probs > threshold) & (y_test.values == False))\n",
    "                \n",
    "                # Calculate net benefits\n",
    "                net_benefit_selected_model.append(cost_sensitive_net_benefit(tp_selected_model, fp_selected_model, threshold, N, w_tp, w_fp))\n",
    "                net_benefit_alternative_model.append(cost_sensitive_net_benefit(tp_alternative_model, fp_alternative_model, threshold, N, w_tp, w_fp))\n",
    "                net_benefit_rand.append(cost_sensitive_net_benefit(tp_rand, fp_rand, threshold, N, w_tp, w_fp))\n",
    "\n",
    "            # Find the maximum net benefit for y-axis limit\n",
    "            max_net_benefit = max(\n",
    "                max(net_benefit_selected_model),\n",
    "                max(net_benefit_alternative_model),\n",
    "                max(net_benefit_rand),\n",
    "                max(net_benefit_all_positive),\n",
    "                max(net_benefit_all_negative)\n",
    "            )\n",
    "\n",
    "            # Plot decision curve\n",
    "            plt.plot(threshold_range, net_benefit_selected_model, label='Selected model')\n",
    "            plt.plot(threshold_range, net_benefit_alternative_model, label='Alternative model')\n",
    "            plt.plot(threshold_range, net_benefit_rand, label='Random predictions')\n",
    "            plt.plot(threshold_range, net_benefit_all_positive, label='All positive')\n",
    "            plt.plot(threshold_range, net_benefit_all_negative, label='All negative')\n",
    "\n",
    "            plt.axvline(x=0.5, color='k', linestyle='--', label='Default threshold (0.5)')\n",
    "            \n",
    "            plt.xlabel('Probability threshold')\n",
    "            plt.ylabel('Net benefit')\n",
    "            plt.title('Cost-sensitive decision curve analysis')\n",
    "            plt.ylim(bottom=-0.01, top=max_net_benefit + 0.01)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "            np.random.seed(SEED)\n",
    "            rand_pred_probs = np.random.rand(len(X_test_OHE))\n",
    "            predictions = selected_model.predict_proba(X_test_OHE)\n",
    "            pred_probs_selected_model = predictions[:, 1]\n",
    "        elif isinstance(selected_model, (cb.CatBoostClassifier, lgb.LGBMClassifier)):\n",
    "            np.random.seed(SEED)\n",
    "            rand_pred_probs = np.random.rand(len(X_test))\n",
    "            predictions = selected_model.predict_proba(X_test)\n",
    "            pred_probs_selected_model = predictions[:, 1]\n",
    "\n",
    "        # pred_probs_selected_model = selected_model.predict_proba(X_test)[:, 1] \n",
    "        pred_probs_alternative_model = lr.predict_proba(X_test_OHE)[:, 1]\n",
    "\n",
    "        decision_curve_analysis(pred_probs_selected_model=pred_probs_selected_model,\n",
    "                                pred_probs_alternative_model= pred_probs_alternative_model,\n",
    "                                rand_pred_probs=rand_pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12b1123",
   "metadata": {},
   "source": [
    "#### Model calibration (optional)\n",
    "\n",
    "Here we applied isotonic regression as the model calibration method. Isotonic regression is a non-parametric approach used to calibrate the predicted probabilities of a classifier. Note that the calibration should be preferrebly done based on an unseen dataset (not the dataset the model is already trained).\n",
    "\n",
    "The following steps are done to apply the calibration: \n",
    "1) First, we obtain the raw predicted probabilities from the trained classifier (selected model) for the test set.\n",
    "2) We then apply isotonic regression to the raw probabilities. Isotonic regression fits a monotonic function to the raw probabilities, which ensures that the calibrated probabilities maintain the order of the original probabilities.\n",
    "3) After fitting the isotonic regression model, we obtain the calibrated probabilities, which represent the adjusted probabilities that have been calibrated to better reflect the true probabilities of class membership.\n",
    "4) Finally, we evaluate the effect of calibration by comparing the performance metrics (Brier score, MCC, ROC AUC, and PR AUC) of the uncalibrated and calibrated probabilities. This allows us to assess whether calibration improves the reliability and accuracy of the predicted probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f4a3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    if model_calibration:\n",
    "        # Get the raw predicted probabilities   \n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "            predictions = selected_model.predict_proba(X_test_OHE)\n",
    "            raw_probabilities = predictions[:, 1]\n",
    "        elif isinstance(selected_model, (cb.CatBoostClassifier, lgb.LGBMClassifier)):\n",
    "            predictions = selected_model.predict_proba(X_test)\n",
    "            raw_probabilities = predictions[:, 1]\n",
    "\n",
    "        # Apply isotonic regression to calibrate the probabilities\n",
    "        calibrated_model = IsotonicRegression()\n",
    "        calibrated_model.fit(raw_probabilities, y_test)\n",
    "\n",
    "        # Calibrate the predicted probabilities\n",
    "        calibrated_probabilities = calibrated_model.predict(raw_probabilities)\n",
    "\n",
    "\n",
    "        # Calculate Brier Score\n",
    "        brier_score_uncalibrated = brier_score_loss(y_test, raw_probabilities)\n",
    "        brier_score_calibrated = brier_score_loss(y_test, calibrated_probabilities)\n",
    "\n",
    "        print(\"Brier Score (Uncalibrated):\", brier_score_uncalibrated)\n",
    "        print(\"Brier Score (Calibrated):\", brier_score_calibrated)\n",
    "\n",
    "        # Calculate Matthews Correlation Coefficient (MCC)\n",
    "        mcc_uncalibrated = matthews_corrcoef(y_test, (raw_probabilities > 0.5).astype(int))\n",
    "        mcc_calibrated = matthews_corrcoef(y_test, (calibrated_probabilities > 0.5).astype(int))\n",
    "\n",
    "        print(\"MCC (Uncalibrated):\", mcc_uncalibrated)\n",
    "        print(\"MCC (Calibrated):\", mcc_calibrated)\n",
    "\n",
    "        # Calculate ROC AUC\n",
    "        roc_auc_uncalibrated = roc_auc_score(y_test, raw_probabilities)\n",
    "        roc_auc_calibrated = roc_auc_score(y_test, calibrated_probabilities)\n",
    "\n",
    "        print(\"ROC AUC (Uncalibrated):\", roc_auc_uncalibrated)\n",
    "        print(\"ROC AUC (Calibrated):\", roc_auc_calibrated)\n",
    "\n",
    "        # Calculate PR AUC\n",
    "        pr_auc_uncalibrated = average_precision_score(y_test, raw_probabilities)\n",
    "        pr_auc_calibrated = average_precision_score(y_test, calibrated_probabilities)\n",
    "\n",
    "        print(\"PR AUC (Uncalibrated):\", pr_auc_uncalibrated)\n",
    "        print(\"PR AUC (Calibrated):\", pr_auc_calibrated)\n",
    "\n",
    "        # Export the model\n",
    "        joblib.dump(calibrated_model, 'selected_model_calibrated.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a92c0d",
   "metadata": {},
   "source": [
    "#### Export the selected model to deploy\n",
    "The best performing model is exported (saved) on disk to be deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "3761990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skip_block:\n",
    "    # Export the model\n",
    "    joblib.dump(selected_model, 'selected_model.pkl')\n",
    "\n",
    "    # Load the model\n",
    "    loaded_model = joblib.load('selected_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7e62cc",
   "metadata": {},
   "source": [
    "### Survival models\n",
    "\n",
    "This part of the pipeline is intended to be used in case the data contains a column for time-to-event information as a survival outcome variable. If so it is possible to use the following code chunks to develop a random survival forest model and a Cox proportional hazard model and compare their performance in prediction performance. For survival models we use scikit-survival package and you can read about here: https://scikit-survival.readthedocs.io/en/stable/#\n",
    "This part may require minimal modifications according to the names used for the target column and whether Cox model outperforms the survival random forest model. By default, the assumption is that the outcome variable requires formatting as follows and the random survival forest outperforms its linear alternative that is the Cox model. It is of course possible to include more models from scikit-survival package, however it is expected that the random survival model to have similar performance to its alternative ensemble models.\n",
    "\n",
    "It should be noted that the survival models can work with one-hot encoded data with no missingness. So X_train_OHE and X_test_OHE are suitable for the analyses. Another thing to note is that the time-to-event column is not in X_train_OHE and X_test_OHE and so we get that column from the copy of the dataset that was initially made in the beginning of the pipeline as a back up to extract that information. In the following chunk you can see the column is called \"max_time_difference_days\", and so if that is different in your dataset, you should modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "85ffdfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    # X_train_OHE\n",
    "    X_train_surv = pd.merge(X_train_OHE_nocv, mydata_copy_survival[time_to_event_column], left_index=True, right_index=True, how='left')\n",
    "    X_test_surv = pd.merge(X_test_OHE, mydata_copy_survival[time_to_event_column], left_index=True, right_index=True, how='left')\n",
    "\n",
    "    y_train_surv = X_train_surv[time_to_event_column]\n",
    "    y_test_surv = X_test_surv[time_to_event_column]\n",
    "    X_train_surv.drop(columns=[time_to_event_column], inplace=True)\n",
    "    X_test_surv.drop(columns=[time_to_event_column], inplace=True)\n",
    "    \n",
    "    \n",
    "    if external_val:\n",
    "        X_extval_surv = pd.merge(X_extval_data_OHE, extval_data_copy[time_to_event_column], left_index=True, right_index=True, how='left')\n",
    "        y_extval_surv = X_extval_surv[time_to_event_column]\n",
    "        X_extval_surv.drop(columns=[time_to_event_column], inplace=True)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a8f13697",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    contains_nan = np.isnan(y_train_surv.values).sum()\n",
    "    contains_nan = np.isnan(y_test_surv.values).sum()\n",
    "\n",
    "    # Check for NaN values\n",
    "    nan_indices = np.isnan(y_train_surv.values)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    y_train_surv[nan_indices] = 0\n",
    "\n",
    "    # Check for NaN values\n",
    "    nan_indices = np.isnan(y_test_surv.values)\n",
    "\n",
    "    # Replace NaN values with 0\n",
    "    y_test_surv[nan_indices] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a7da1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "\n",
    "    y_train_surv_transformed = Surv().from_arrays(y_train.values, y_train_surv.values)\n",
    "    y_test_surv_transformed = Surv().from_arrays(y_test.values, y_test_surv.values)\n",
    "    \n",
    "    if external_val:\n",
    "        y_extval_surv_transformed = Surv().from_arrays(y_extval_data.values, y_extval_surv.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83da02",
   "metadata": {},
   "source": [
    "This is how the outcome column has to be formatted for survival models. In each array the first entry determines if there is any event or not and the second entry determines the last follow up time within a specific observation period. For example, when there is an event (e.g. daignosed disease) the first entry becomes True and the second entry show when it was recorded with respect to the baseline time (e.g. time of transplantation). If there was no event, then the last recorded sample of a patient is considered for the time and the event entry is False that clarifies that there was no event up to that time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "239924ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    print(\"10 samples from the test set:\")\n",
    "    print(y_train_surv_transformed[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd42ca5",
   "metadata": {},
   "source": [
    "#### Training and evaluation of the survival models\n",
    "\n",
    "First we do corss validation using the traing set (development set) to assess the prediction performance of RSF and CPH models. The cross validation follows the same folding setting (i.e., number of folds) of the binary classification models (except for the survival models it is not stratified by the biary outcome variable). After we do the assessment of the models based on cross validation, we train the models on the whole trainig set and evaluate them on the test set. Two metrics are used to evaluate the models: (1) concordance index (CI), and (2) Integrated Brier Score (IBS). These scores are explained here: https://scikit-survival.readthedocs.io/en/v0.23.0/api/metrics.html.\n",
    "\n",
    "#### Concordance Index (CI) and Integrated Brier Score (IBS)\n",
    "\n",
    "##### Concordance Index (CI)\n",
    "The **Concordance Index (CI)** is a performance measure for survival models. It evaluates how well the model can correctly rank survival times. The CI measures the proportion of all usable pairs of individuals where the model correctly predicts the order of survival times. A CI of `1.0` indicates perfect predictions, while `0.5` represents random guessing.\n",
    "\n",
    "- **Interpretation**: \n",
    "  - **CI = 1**: Perfect prediction, the model correctly ranks all pairs of individuals.\n",
    "  - **CI = 0.5**: Random prediction, no better than chance.\n",
    "  - **CI < 0.5**: Worse than random, model is predicting the reverse order of survival times.\n",
    "\n",
    "For more details: [Concordance Index in scikit-survival](https://scikit-survival.readthedocs.io/en/v0.23.0/api/generated/sksurv.metrics.concordance_index_censored.html#sksurv.metrics.concordance_index_censored).\n",
    "\n",
    "##### Integrated Brier Score (IBS)\n",
    "The **Integrated Brier Score (IBS)** is a measure of the accuracy of predicted survival probabilities over time. It is the average Brier score, which measures the difference between the predicted survival probability and the actual outcome (whether the event occurred or not), across a range of time points. A lower IBS indicates better performance.\n",
    "\n",
    "- **Interpretation**:\n",
    "  - **IBS = 0**: Perfect prediction, the model’s predicted probabilities match the true outcomes.\n",
    "  - **Higher IBS values**: Less accurate predictions.\n",
    "\n",
    "For more details: [Integrated Brier Score in scikit-survival](https://scikit-survival.readthedocs.io/en/v0.23.0/api/generated/sksurv.metrics.integrated_brier_score.html#sksurv.metrics.integrated_brier_score).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "b567af14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    n_rows = X_train_surv.shape[0]\n",
    "    #########\n",
    "    def adjust_hyperparameters_surv_models(n_rows):\n",
    "        RSF_param_dist = {\n",
    "            'n_estimators': np.linspace(50, max(1000, int(np.sqrt(n_rows))), num=10, dtype=int),  # Number of trees in the forest\n",
    "            'min_samples_split': np.linspace(2, 20, num=10, dtype=int),  # Minimum number of samples required to split an internal node\n",
    "            'min_samples_leaf': np.linspace(1, 20, num=10, dtype=int)  # Minimum number of samples required to be at a leaf node\n",
    "        }\n",
    "\n",
    "        # Define the parameter grid for CPH model\n",
    "        CoxPH_param_dist = {\n",
    "            'alpha': [1e-4, 0.01, 0.1, 1.0],  # Regularization parameter for ridge regression penalty\n",
    "            'ties': ['breslow', 'efron'],  # Method to handle tied event times\n",
    "            'n_iter': np.linspace(50, max(1000, int(np.sqrt(n_rows))), num=10, dtype=int),  # Maximum number of iterations\n",
    "            'tol': [1e-6, 1e-7, 1e-8],  # Convergence criteria\n",
    "            }\n",
    "        return RSF_param_dist, CoxPH_param_dist\n",
    "    def set_params_surv_models(n_rows):\n",
    "        RSF_params = {\n",
    "            'n_estimators': max(1000, int(np.sqrt(n_rows))),  # Number of trees in the forest\n",
    "            'min_samples_split': 2,  # Minimum number of samples required to split an internal node\n",
    "            'min_samples_leaf': 1  # Minimum number of samples required to be at a leaf node\n",
    "        }\n",
    "        CoxPH_param = {\n",
    "            'alpha': 1e-4,  # Regularization parameter for ridge regression penalty\n",
    "            'ties': 'efron',  # Method to handle tied event times\n",
    "            'n_iter': max(1000, int(np.sqrt(n_rows))),  # Maximum number of iterations\n",
    "            'tol': 1e-6,  # Convergence criteria\n",
    "            }\n",
    "        return RSF_params, CoxPH_param\n",
    "    #########\n",
    "    def calculate_surv_metrics(y_true, model, X_test, survival_train, survival_test):\n",
    "        event_indicator = y_true['event']\n",
    "        event_time = y_true['time']\n",
    "        \n",
    "        # Get the concordance index (CI)\n",
    "        CI = concordance_index_censored(event_indicator, event_time, model.predict(X_test))[0]\n",
    "\n",
    "        # Extract event times from survival_train and survival_test\n",
    "        train_times = survival_train['time']  # Training event times\n",
    "        test_times = survival_test['time']    # Test event times\n",
    "\n",
    "        # Obtain predicted survival functions for each test instance\n",
    "        survival_preds = model.predict_survival_function(X_test)\n",
    "\n",
    "        # Get the follow-up time interval from both training and test datasets\n",
    "        min_followup_time = max(train_times.min(), test_times.min())  # The later start\n",
    "        max_followup_time = min(train_times.max(), test_times.max())  # The earlier end\n",
    "\n",
    "        # Define valid times based on the overlap of the follow-up times from both datasets\n",
    "        valid_times = np.arange(min_followup_time, max_followup_time)\n",
    "\n",
    "        # Ensure valid_times does not include the maximum observed time\n",
    "        valid_times = valid_times[valid_times < max_followup_time]\n",
    "\n",
    "        # Generate predictions for each survival function at the valid time points\n",
    "        preds = np.asarray([[fn(t) for t in valid_times] for fn in survival_preds])\n",
    "\n",
    "        # Check for empty or invalid predictions before calculating IBS\n",
    "        if preds.size == 0:\n",
    "            raise ValueError(\"Predictions array is empty. Check the time points and survival functions.\")\n",
    "\n",
    "        # Calculate Integrated Brier Score (IBS) ensuring times are within the valid follow-up period\n",
    "        IBS = integrated_brier_score(survival_train=survival_train, survival_test=survival_test, estimate=preds, times=valid_times)\n",
    "        \n",
    "        return {'CI': CI, 'IBS': IBS}\n",
    "\n",
    "\n",
    "    #########\n",
    "    # Function to cross-validate the survival model\n",
    "    def cross_validate_surv_model(model_class, X, y, n_splits=cv_folds, random_state=SEED, measures=None, hp_tuning=False):\n",
    "        if measures is None:\n",
    "            measures = ['CI', 'IBS']\n",
    "        \n",
    "        fold_results = pd.DataFrame()\n",
    "        \n",
    "        # Convert survival data to DataFrame for indexing purposes\n",
    "        y_df = pd.DataFrame(y, columns=['event', 'time'])\n",
    "        \n",
    "        # Use KFold instead of StratifiedKFold (since this is survival data, not classification)\n",
    "        kf = KFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "        for fold, (train_index, test_index) in enumerate(kf.split(X), 1):\n",
    "            X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train_fold_df, y_test_fold_df = y_df.iloc[train_index], y_df.iloc[test_index]\n",
    "            \n",
    "            # Convert y_train_fold and y_test_fold back to original structured format\n",
    "            y_train_fold = np.array([(row['event'], row['time']) for _, row in y_train_fold_df.iterrows()],\n",
    "                                    dtype=[('event', '?'), ('time', '<f8')])\n",
    "            y_test_fold = np.array([(row['event'], row['time']) for _, row in y_test_fold_df.iterrows()],\n",
    "                                dtype=[('event', '?'), ('time', '<f8')])\n",
    "            \n",
    "            n_rows = X_train_fold.shape[0]\n",
    "            # Adjust hyperparameters based on the training data in this fold\n",
    "            RSF_param_dist, CoxPH_param_dist = adjust_hyperparameters_surv_models(n_rows)\n",
    "            RSF_params, CoxPH_params = set_params_surv_models(n_rows)\n",
    "            # Prepare survival data for calculating metrics\n",
    "            survival_train = Surv.from_dataframe('event', 'time', y_train_fold_df)\n",
    "            survival_test = Surv.from_dataframe('event', 'time', y_test_fold_df)\n",
    "            \n",
    "            # Initialize and fit the model\n",
    "            if model_class == RandomSurvivalForest:\n",
    "                RSF_model = RandomSurvivalForest(random_state=random_state, **RSF_params)\n",
    "                if hp_tuning:\n",
    "                    random_search = RandomizedSearchCV(\n",
    "                        estimator=RSF_model, \n",
    "                        param_distributions=RSF_param_dist, \n",
    "                        n_iter=n_iter_hptuning, \n",
    "                        cv=cv_folds_hptuning, \n",
    "                        refit=True, \n",
    "                        random_state=random_state)\n",
    "                    random_search.fit(X_train_fold, y_train_fold)\n",
    "                    RSF_model = random_search.best_estimator_\n",
    "                RSF_model.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = RSF_model.predict(X_test_fold)\n",
    "                \n",
    "                # Calculate evaluation metrics\n",
    "                metrics = calculate_surv_metrics(y_true=y_test_fold_df, model=RSF_model, X_test=X_test_fold, survival_train=survival_train, survival_test=survival_test)\n",
    "                fold_results = fold_results.append(metrics, ignore_index=True)\n",
    "                \n",
    "            elif model_class == CoxPHSurvivalAnalysis:\n",
    "                CPH_model = CoxPHSurvivalAnalysis(**CoxPH_params)\n",
    "                if hp_tuning:\n",
    "                    random_search = RandomizedSearchCV(\n",
    "                        estimator=CPH_model, \n",
    "                        param_distributions=CoxPH_param_dist, \n",
    "                        n_iter=n_iter_hptuning, \n",
    "                        cv=cv_folds_hptuning, \n",
    "                        refit=True, \n",
    "                        random_state=random_state)\n",
    "                    random_search.fit(X_train_fold, y_train_fold)\n",
    "                    CPH_model = random_search.best_estimator_\n",
    "                CPH_model.fit(X_train_fold, y_train_fold)\n",
    "                y_pred = CPH_model.predict(X_test_fold)\n",
    "                \n",
    "                # Calculate evaluation metrics\n",
    "                metrics = calculate_surv_metrics(y_true=y_test_fold_df, model=CPH_model, X_test=X_test_fold, survival_train=survival_train, survival_test=survival_test)\n",
    "                fold_results = fold_results.append(metrics, ignore_index=True)\n",
    "            \n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported model class: {model_class}\")\n",
    "            \n",
    "\n",
    "        # Aggregating the results\n",
    "        aggregated_results = {metric: np.nanmean(fold_results[metric].values).round(2) for metric in measures}\n",
    "        aggregated_results_sd = {metric: np.nanstd(fold_results[metric].values).round(2) for metric in measures}\n",
    "        \n",
    "        # Combine mean and standard deviation\n",
    "        combined_results = {metric: f\"{mean} ± {sd}\" for metric, mean in aggregated_results.items() for _, sd in aggregated_results_sd.items() if metric == _}\n",
    "        \n",
    "        # Create a DataFrame for displaying the results\n",
    "        results_table = pd.DataFrame(list(combined_results.items()), columns=['Metric', 'Result'])\n",
    "        \n",
    "        # Display the results\n",
    "        print(\"Aggregated Results:\")\n",
    "        print(results_table.to_string(index=False))\n",
    "        \n",
    "        return results_table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "0362bcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    # getting the cross validation results (performance) for RSF - mean and standard deviation of the metrics\n",
    "    results_table_RSF = cross_validate_surv_model(\n",
    "        model_class=RandomSurvivalForest,\n",
    "        X=X_train_surv, \n",
    "        y=y_train_surv_transformed,\n",
    "        n_splits=cv_folds,\n",
    "        random_state=SEED\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "79735215",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    # getting the cross validation results (performance) for CPH - mean and standard deviation of the metrics\n",
    "    results_table_CPH = cross_validate_surv_model(\n",
    "        model_class=CoxPHSurvivalAnalysis,\n",
    "        X=X_train_surv, \n",
    "        y=y_train_surv_transformed,\n",
    "        n_splits=cv_folds,\n",
    "        random_state=SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e1f0883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    # aggragation of the CV results for RSF and CPH models to make them into one table\n",
    "\n",
    "    results_table_RSF.rename(columns={'Result': 'RSF'}, inplace=True)\n",
    "    results_table_CPH.rename(columns={'Result': 'CPH'}, inplace=True)\n",
    "\n",
    "    # Merging the two tables on the 'Metric' column\n",
    "    merged_table = pd.merge(results_table_RSF, results_table_CPH, on='Metric')\n",
    "\n",
    "    # Display the merged table\n",
    "    print(merged_table.to_string(index=False))\n",
    "    merged_table.to_excel('CV_surv.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "94936fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we do the survival analysis using the whole trainig set for trainig the survival models and evaluating them on the test set\n",
    "if survival_analysis:\n",
    "    # Define the parameter grid for RSF model\n",
    "    n_rows = X_train_surv.shape[0]\n",
    "    RSF_param_dist, CoxPH_param_dist = adjust_hyperparameters_surv_models(n_rows)\n",
    "    \n",
    "    ############# RSF\n",
    "    # RandomizedSearchCV for Random Survival Forest (RSF)\n",
    "    rsf = RandomSurvivalForest(n_jobs=n_cpu_model_training, random_state=SEED)\n",
    "    random_search = RandomizedSearchCV(rsf, param_distributions=RSF_param_dist, n_iter=n_iter_hptuning, cv=cv_folds, random_state=SEED)\n",
    "    \n",
    "    # Fit RandomizedSearchCV on training data\n",
    "    random_search.fit(X_train_surv, y_train_surv_transformed)\n",
    "    print(\"Best Parameters for RSF:\", random_search.best_params_)\n",
    "    \n",
    "    # Train the RSF model with the best hyperparameters\n",
    "    rsf = RandomSurvivalForest(n_jobs=n_cpu_model_training, random_state=SEED, **random_search.best_params_)\n",
    "    rsf.fit(X_train_surv, y_train_surv_transformed)\n",
    "    \n",
    "    # Save the RSF model to disk\n",
    "    dump(rsf, 'RSF_model.pkl')\n",
    "    \n",
    "    # Predict survival function for test data\n",
    "    survival_preds_rsf = rsf.predict_survival_function(X_test_surv)\n",
    "    \n",
    "    # Define a time range for the Brier score calculation based on the test data\n",
    "    test_times_rsf = y_test_surv.values\n",
    "    valid_times_rsf = np.linspace(test_times_rsf.min(), test_times_rsf.max() - 1e-5, 100)\n",
    "    \n",
    "    # Create an estimate array for the predicted survival probabilities at each valid time point\n",
    "    estimate_rsf = np.zeros((len(X_test_surv), len(valid_times_rsf)))\n",
    "    for i, fn in enumerate(survival_preds_rsf):\n",
    "        estimate_rsf[i, :] = fn(valid_times_rsf)\n",
    "\n",
    "    # Calculate IBS for RSF\n",
    "    IBS_rsf = integrated_brier_score(survival_train=y_train_surv_transformed, survival_test=y_test_surv_transformed, estimate=estimate_rsf, times=valid_times_rsf)\n",
    "    print(f\"Integrated Brier Score for RSF on the test set: {IBS_rsf:.2f}\")\n",
    "    \n",
    "    # Get the concordance index (CI)\n",
    "    CI_rsf = concordance_index_censored(event_indicator= y_test, event_time = y_test_surv.values, estimate = rsf.predict(X_test_surv))[0]\n",
    "    print(f\"C-index for RSF on the test set: {CI_rsf:.2f}\")\n",
    "    \n",
    "    ############## CPH\n",
    "    # RandomizedSearchCV for Cox Proportional Hazards (CPH)\n",
    "    coxph = CoxPHSurvivalAnalysis()\n",
    "    random_search_coxph = RandomizedSearchCV(coxph, param_distributions=CoxPH_param_dist, n_iter=n_iter_hptuning, cv=cv_folds, random_state=SEED)\n",
    "    \n",
    "    # Fit RandomizedSearchCV on training data\n",
    "    random_search_coxph.fit(X_train_surv, y_train_surv_transformed)\n",
    "    print(\"Best Parameters for CPH:\", random_search_coxph.best_params_)\n",
    "    \n",
    "    # Train the CPH model with the best hyperparameters\n",
    "    coxph = CoxPHSurvivalAnalysis(**random_search_coxph.best_params_)\n",
    "    coxph.fit(X_train_surv, y_train_surv_transformed)\n",
    "    \n",
    "    # Save the CPH model to disk\n",
    "    dump(coxph, 'CPH_model.pkl')\n",
    "\n",
    "    # Calculate Integrated Brier Score (IBS) for CPH\n",
    "    survival_preds_cph = coxph.predict_survival_function(X_test_surv)\n",
    "    \n",
    "    # Define a time range for the Brier score calculation based on the test data\n",
    "    test_times_cph = y_test_surv.values\n",
    "    valid_times_cph = np.linspace(test_times_cph.min(), test_times_cph.max() - 1e-5, 100)\n",
    "\n",
    "    # Create an estimate array for the predicted survival probabilities at each valid time point\n",
    "    estimate_cph = np.zeros((len(X_test_surv), len(valid_times_cph)))\n",
    "    for i, fn in enumerate(survival_preds_cph):\n",
    "        estimate_cph[i, :] = fn(valid_times_cph)\n",
    "\n",
    "    # Calculate IBS for CPH\n",
    "    IBS_cph = integrated_brier_score(survival_train=y_train_surv_transformed, survival_test=y_test_surv_transformed, estimate=estimate_cph, times=valid_times_cph)\n",
    "    print(f\"Integrated Brier Score for CPH on the test set: {IBS_cph:.2f}\")\n",
    "    \n",
    "    # Get the concordance index (CI)\n",
    "    CI_cph = concordance_index_censored(event_indicator= y_test, event_time = y_test_surv.values, estimate = coxph.predict(X_test_surv))[0]\n",
    "    print(f\"C-index for CPH on the test set: {CI_cph:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "0d6afacf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis and external_val:\n",
    "    # RSF model evaluation on the external validation set\n",
    "    # Predict survival function for test data\n",
    "    survival_preds_rsf = rsf.predict_survival_function(X_extval_surv)\n",
    "    \n",
    "    # Define a time range for the Brier score calculation based on the test data\n",
    "    extval_times_rsf = y_extval_surv.values\n",
    "    valid_times_rsf = np.linspace(extval_times_rsf.min(), extval_times_rsf.max() - 1e-5, 100)\n",
    "    \n",
    "    # Create an estimate array for the predicted survival probabilities at each valid time point\n",
    "    estimate_rsf = np.zeros((len(X_extval_surv), len(valid_times_rsf)))\n",
    "    for i, fn in enumerate(survival_preds_rsf):\n",
    "        estimate_rsf[i, :] = fn(valid_times_rsf)\n",
    "\n",
    "    # Calculate IBS for RSF\n",
    "    IBS_rsf = integrated_brier_score(survival_train=y_train_surv_transformed, survival_test=y_extval_surv_transformed, estimate=estimate_rsf, times=valid_times_rsf)\n",
    "    print(f\"Integrated Brier Score for RSF on the external validation set: {IBS_rsf:.2f}\")\n",
    "    \n",
    "    # Get the concordance index (CI)\n",
    "    CI_rsf = concordance_index_censored(event_indicator= y_extval_data, event_time = y_extval_surv.values, estimate = rsf.predict(X_extval_surv))[0]\n",
    "    print(f\"C-index for RSF on the external validation set: {CI_rsf:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675437cc",
   "metadata": {},
   "source": [
    "feature importance for the survival model\n",
    "For each feature it shows if its relationship to survival time is removed (by random shuffling), the concordance index on the test data drops on average by mentioned values displayed on the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "fd526a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    # RSF model\n",
    "    perm_result = permutation_importance(rsf, X_test_surv, y_test_surv_transformed, n_repeats=15, random_state=SEED)\n",
    "\n",
    "    RSF_perm_fi = pd.DataFrame(\n",
    "        {\n",
    "            k: perm_result[k]\n",
    "            for k in (\n",
    "                \"importances_mean\",\n",
    "                \"importances_std\",\n",
    "            )\n",
    "        },\n",
    "        index=X_test_surv.columns,\n",
    "    ).sort_values(by=\"importances_mean\", ascending=False)\n",
    "    \n",
    "    print(RSF_perm_fi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fdc700",
   "metadata": {},
   "source": [
    "One of the methods based on SHAP that is implemented for survival ML models like RSF is to use SurvSHAP(t). It provides time-dependent explanations for the survival machine learning models. The impact of a variable over the observation period may change and that is the information that SurvSHAP(t) can reveal. SurvSHAP(t) is explained in details in https://www.sciencedirect.com/science/article/pii/S0950705122013302?via%3Dihub ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f61cb91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can see the impact of variables over time on the predictions of the survival model for one sample (patient)\n",
    "if survival_analysis:\n",
    "    # adopted from https://pypi.org/project/survshap/\n",
    "\n",
    "    # create explainer(X_train_surv, y_train_surv_transformed)\n",
    "    rsf_exp = SurvivalModelExplainer(model = rsf, data = X_train_surv, y = y_train_surv_transformed)\n",
    "\n",
    "    # compute SHAP values for a single instance\n",
    "    observation_A = X_train_surv.iloc[[0]]\n",
    "    survshap_A = PredictSurvSHAP(\n",
    "        random_state=SEED,        # Set the random seed for reproducibility\n",
    "        function_type = \"chf\", # Either \"sf\" representing survival function or \"chf\" representing cumulative hazard function (use chf if you want to see the direction of the feature impact aligned with binary classification models: positive SHAP equivalent to increase risk)\n",
    "        calculation_method=\"sampling\"  # \"shap_kernel\" for shap.KernelExplainer, \"kernel\" for exact KernelSHAP, \"sampling\" for sampling method, or \"treeshap\" for shap.TreeExplainer\n",
    "    )\n",
    "    survshap_A.fit(explainer = rsf_exp, new_observation = observation_A)\n",
    "\n",
    "    survshap_A.result \n",
    "    survshap_A.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "2d531897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can get the survival SHAP values for a group of patients (samples) or all samples on the test set for example\n",
    "if survival_analysis:\n",
    "    # rsf_exp = SurvivalModelExplainer(rsf, X_train_surv, y_train_surv_transformed)\n",
    "\n",
    "    # you can set this to smaller numbers for a subset of samples patients if it takes too long\n",
    "    n_samples = X_test_surv.shape[0]\n",
    "\n",
    "    def parallel_compute_shap_surv(data, i):\n",
    "        survshap = PredictSurvSHAP(random_state = SEED, function_type = \"chf\", calculation_method=\"sampling\")\n",
    "        survshap.fit(rsf_exp, data.iloc[[i]])\n",
    "        return survshap\n",
    "    # run it in parallel to speed up the processing\n",
    "    survshaps = Parallel(n_jobs=n_cpu_for_tuning, backend='loky')(delayed(parallel_compute_shap_surv)(X_test_surv, i) for i in range(n_samples))\n",
    "    if external_val:\n",
    "        survshaps_extval = Parallel(n_jobs=n_cpu_for_tuning, backend='loky')(delayed(parallel_compute_shap_surv)(X_extval_surv, i) for i in range(n_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "b5674f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    def plot_survshap_detailed(shap_results, top_n=10, sample_percentage=100):\n",
    "        \"\"\"\n",
    "        Optimized function to plot SHAP values over time for the top N features on separate subplots,\n",
    "        with an option to randomly sample a percentage of the data for each feature.\n",
    "\n",
    "        Parameters:\n",
    "        shap_results: List of SHAP results for each sample.\n",
    "        top_n: The number of top features to plot based on mean of max absolute SHAP values.\n",
    "        sample_percentage: Percentage of samples randomly selected to be displayed on the plots (0 < sample_percentage <= 100).\n",
    "        \"\"\"\n",
    "        # Combine results from all samples into one DataFrame\n",
    "        shap_df = pd.concat([shap.result for shap in shap_results], axis=0)\n",
    "        \n",
    "        # Extract time columns (assuming they start with 't = ')\n",
    "        time_columns = [col for col in shap_df.columns if col.startswith('t =')]\n",
    "        \n",
    "        # Get unique feature names\n",
    "        feature_names = shap_df['variable_name'].unique()\n",
    "\n",
    "        # Precompute mean of max absolute SHAP values for each feature\n",
    "        feature_data_dict = {}\n",
    "        for feature_name in feature_names:\n",
    "            feature_data = shap_df[shap_df['variable_name'] == feature_name]\n",
    "            shap_values = feature_data[time_columns].values\n",
    "\n",
    "            # Calculate the max absolute SHAP value for each sample and then compute the mean across all samples\n",
    "            max_abs_shap_per_sample = np.max(np.abs(shap_values), axis=1)\n",
    "            max_max_abs_shap = np.max(max_abs_shap_per_sample)\n",
    "            \n",
    "            feature_data_dict[feature_name] = {\n",
    "                'data': feature_data,\n",
    "                'shap_values': shap_values,\n",
    "                'max_max_abs_shap': max_max_abs_shap\n",
    "            }\n",
    "\n",
    "        # Sort features by their max of max absolute SHAP value and take only the top N features\n",
    "        sorted_features = sorted(feature_data_dict.keys(), key=lambda x: feature_data_dict[x]['max_max_abs_shap'], reverse=True)[:top_n]\n",
    "        \n",
    "        # Create subplots\n",
    "        num_features = len(sorted_features)\n",
    "        fig, axes = plt.subplots(num_features, 1, figsize=(10, 2 * num_features), sharex=True, constrained_layout=True)\n",
    "        \n",
    "        if num_features == 1:\n",
    "            axes = [axes]  # Ensure axes is iterable if only one subplot\n",
    "\n",
    "        # Colormap for feature values\n",
    "        cmap = cm.get_cmap('coolwarm')\n",
    "\n",
    "        # Process and plot each feature\n",
    "        for idx, (ax, feature_name) in enumerate(zip(axes, sorted_features)):\n",
    "            # Get data and SHAP values for the specific feature\n",
    "            feature_data = feature_data_dict[feature_name]['data']\n",
    "            shap_values = feature_data_dict[feature_name]['shap_values']\n",
    "            feature_values = feature_data['variable_value'].values\n",
    "            \n",
    "            if len(shap_values) == 0:\n",
    "                continue\n",
    "\n",
    "            # Randomly select a subset of samples if sample_percentage < 100\n",
    "            if sample_percentage < 100:\n",
    "                num_samples = len(feature_data)\n",
    "                sample_size = int(num_samples * (sample_percentage / 100))\n",
    "                sampled_indices = np.random.choice(num_samples, sample_size, replace=False)\n",
    "                shap_values = shap_values[sampled_indices]\n",
    "                feature_values = feature_values[sampled_indices]\n",
    "\n",
    "            # feature value normalization\n",
    "            normalized_values = QuantileTransformer(output_distribution='uniform').fit_transform(feature_values.reshape(-1, 1)).flatten()\n",
    "            \n",
    "            # Plot SHAP values for each sample\n",
    "            for i in range(len(shap_values)):\n",
    "                color = cmap(normalized_values[i])\n",
    "                ax.plot(time_columns, shap_values[i], color=color, alpha=0.6, lw=1)\n",
    "            \n",
    "            # Title and Y-axis label\n",
    "            ax.set_title(f\"{feature_name} (grand max |SHAP|: {feature_data_dict[feature_name]['max_max_abs_shap']:.2f})\", fontsize=10)\n",
    "            ax.set_ylabel(\"SHAP value\")\n",
    "            \n",
    "            # Horizontal line at y=0\n",
    "            ax.axhline(0, color='grey', linestyle=':', linewidth=1)\n",
    "        \n",
    "        # Simplify x-axis labels\n",
    "        xticks_interval = max(len(time_columns) // 10, 1)  # Ensure at least some ticks show up\n",
    "        plt.xticks(time_columns[::xticks_interval], rotation=90)\n",
    "\n",
    "        # Colorbar for all subplots\n",
    "        sm = plt.cm.ScalarMappable(cmap=cmap)\n",
    "        sm.set_array([])\n",
    "        fig.colorbar(sm, ax=axes, orientation='vertical', fraction=0.02, pad=0.02, label='Feature Value')\n",
    "\n",
    "        if external_val:\n",
    "            plt.savefig('shap_surv_curves_extval.tif', dpi=300, bbox_inches='tight')\n",
    "        else:\n",
    "            plt.savefig('shap_surv_curves_testset.tif', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    plot_survshap_detailed(survshaps, top_n=top_n_f, sample_percentage=100) # plot top top_n_f features for 100% of samples from the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "4e9a1c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis and external_val:\n",
    "    plot_survshap_detailed(survshaps_extval, top_n=top_n_f, sample_percentage=100) # plot top top_n_f features for 100% of samples from the test set\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "5e0a0107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we aggregate the shap values for each feature (variable) and display a SHAP summary plot based on the SHAP values calculated using survSHAP(t) for survival models\n",
    "# What we see here is the feature values and their relationships with the SHAP values (that could not be shown on the previous plot with SHAP values over time)\n",
    "if survival_analysis:\n",
    "\n",
    "    def aggregate_shap_values_with_base(survshaps, X_test_surv, aggregation='mean_abs'):\n",
    "        n_features = X_test_surv.shape[1]  # Number of features in the test set \n",
    "        n_samples = X_test_surv.shape[0]   # Number of samples\n",
    "        shap_values = np.zeros((n_samples, n_features))  # Initialize shap_values array\n",
    "        base_values = np.zeros(n_samples)   # Initialize base_values array\n",
    "\n",
    "        for i, survshap in enumerate(survshaps):\n",
    "            shap_df = survshap.result\n",
    "\n",
    "            # Extract columns corresponding to time points (these contain SHAP values)\n",
    "            time_point_columns = [col for col in shap_df.columns if col.startswith('t =')]\n",
    "            shap_values_sample = shap_df[time_point_columns].values  # shape: (n_timepoints, n_features_per_timepoint)\n",
    "            \n",
    "            # Group SHAP values by feature\n",
    "            feature_groups = shap_df.groupby('variable_name')\n",
    "\n",
    "            # Initialize temporary array to store aggregated SHAP values per feature for this sample\n",
    "            shap_values_aggregated = np.zeros(n_features)\n",
    "            \n",
    "            for feature_name, group in feature_groups:\n",
    "                # Aggregate SHAP values across time points for each feature\n",
    "                if aggregation == 'mean_abs':\n",
    "                    shap_agg = group[time_point_columns].abs().mean(axis=1)  # Aggregating by absolute mean\n",
    "                elif aggregation == 'sum':\n",
    "                    shap_agg = group[time_point_columns].sum(axis=1)  # Aggregating by sum\n",
    "                elif aggregation == 'mean':\n",
    "                    shap_agg = group[time_point_columns].mean(axis=1)  # Aggregating by mean\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown aggregation method: {aggregation}\")\n",
    "\n",
    "                # Map the aggregated SHAP values to the correct feature in shap_values_aggregated\n",
    "                feature_idx = X_test_surv.columns.get_loc(feature_name)  # Get the index of the feature\n",
    "                shap_values_aggregated[feature_idx] = shap_agg.mean()  # Store the aggregated value for this feature\n",
    "            \n",
    "            # Store the aggregated SHAP values for this sample\n",
    "            shap_values[i] = shap_values_aggregated\n",
    "\n",
    "            # Extract base value if available\n",
    "            if hasattr(survshap, 'base_value'):\n",
    "                base_values[i] = survshap.base_value\n",
    "            else:\n",
    "                base_values[i] = 0  # Default or adjust as needed\n",
    "        \n",
    "        return shap_values, base_values\n",
    "\n",
    "    # getting the shap summary plot for the survival model (RSF) on the test set\n",
    "    shap_values, base_values = aggregate_shap_values_with_base(survshaps, X_test_surv, aggregation='mean')\n",
    "\n",
    "    # Create the Explanation object with per-sample base_values\n",
    "    explainer_values = shap.Explanation(\n",
    "        values=shap_values,\n",
    "        base_values=base_values,\n",
    "        data=X_test_surv,\n",
    "        feature_names=X_test_surv.columns\n",
    "    )\n",
    "\n",
    "    shap.plots.beeswarm(explainer_values, max_display=top_n_f)\n",
    "    plt.savefig(\"shap_beeswarm_testset.tif\", dpi=300, bbox_inches='tight')\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "068a6b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the shap summary plot for the survival model (RSF) on the external validation set\n",
    "if survival_analysis and external_val:\n",
    "    shap_values_extval, base_values_extval = aggregate_shap_values_with_base(survshaps_extval, X_extval_surv, aggregation='mean')\n",
    "\n",
    "    # Create the Explanation object with per-sample base_values\n",
    "    explainer_values_extval = shap.Explanation(\n",
    "        values=shap_values_extval,\n",
    "        base_values=base_values_extval,\n",
    "        data=X_extval_surv,\n",
    "        feature_names=X_extval_surv.columns\n",
    "    )\n",
    "\n",
    "    shap.plots.beeswarm(explainer_values_extval, max_display=top_n_f)\n",
    "    plt.savefig(\"shap_beeswarm_extval.tif\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce017620",
   "metadata": {},
   "source": [
    "Predicted survival and cumulative hazard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "a09d683f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    # Predict cumulative hazard function for all training samples\n",
    "    # The informaiton here will be used to translate the predicted hazard functions for each individuals in the test set (or external validation set) to binary classifcation\n",
    "    surv_train = rsf.predict_cumulative_hazard_function(X_train_surv, return_array=True)\n",
    "\n",
    "    class_0_indices = np.where(y_train.values == False)[0] \n",
    "    class_1_indices = np.where(y_train.values == True)[0] \n",
    "\n",
    "    # Separate predictions into classes\n",
    "    surv_class_0 = surv_train[class_0_indices]\n",
    "    surv_class_1 = surv_train[class_1_indices]\n",
    "\n",
    "    # Calculate median and interquartile range for both classes\n",
    "    median_hazard_class_0_train = np.median(surv_class_0, axis=0)\n",
    "    median_hazard_class_1_train = np.median(surv_class_1, axis=0)\n",
    "    \n",
    "    rsf_riskscores_train = rsf.predict(X_train_surv)\n",
    "    \n",
    "    # Calculate average risk scores for each class in the training set\n",
    "    predicted_risk_socres_class_0 = rsf_riskscores_train[y_train]\n",
    "    predicted_risk_socres_class_1 = rsf_riskscores_train[~y_train]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b947688a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    \n",
    "    # Predict cumulative hazard function for all test samples\n",
    "    surv = rsf.predict_cumulative_hazard_function(X_test_surv, return_array=True)\n",
    "\n",
    "    class_0_indices = np.where(y_test.values == False)[0] \n",
    "    class_1_indices = np.where(y_test.values == True)[0]  \n",
    "\n",
    "    # Separate predictions into classes\n",
    "    surv_class_0 = surv[class_0_indices]\n",
    "    surv_class_1 = surv[class_1_indices]\n",
    "\n",
    "    # Calculate median and interquartile range for both classes\n",
    "    median_hazard_class_0_test = np.median(surv_class_0, axis=0)\n",
    "    q1_hazard_class_0_test = np.percentile(surv_class_0, 25, axis=0)\n",
    "    q3_hazard_class_0_test = np.percentile(surv_class_0, 75, axis=0)\n",
    "    iqr_hazard_class_0_test = q3_hazard_class_0_test - q1_hazard_class_0_test\n",
    "\n",
    "    median_hazard_class_1_test = np.median(surv_class_1, axis=0)\n",
    "    q1_hazard_class_1_test = np.percentile(surv_class_1, 25, axis=0)\n",
    "    q3_hazard_class_1_test = np.percentile(surv_class_1, 75, axis=0)\n",
    "    iqr_hazard_class_1_test = q3_hazard_class_1_test - q1_hazard_class_1_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "bc466899",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "\n",
    "    # Define a function to calculate the Euclidean distance\n",
    "    def euclidean_distance(x, y):\n",
    "        return np.sqrt(np.sum((x - y) ** 2))\n",
    "\n",
    "    # Predict cumulative hazard function for all test samples\n",
    "    surv = rsf.predict_cumulative_hazard_function(X_test_surv, return_array=True)\n",
    "\n",
    "    # Calculate distances from median curves for each individual\n",
    "    distances_class_0 = [euclidean_distance(curve, median_hazard_class_0_train) for curve in surv]\n",
    "    distances_class_1 = [euclidean_distance(curve, median_hazard_class_1_train) for curve in surv]\n",
    "\n",
    "    # Determine predicted class based on proximity to median curves\n",
    "    predicted_classes = []\n",
    "    for dist_0, dist_1 in zip(distances_class_0, distances_class_1):\n",
    "        if dist_0 < dist_1:\n",
    "            predicted_classes.append(0)\n",
    "        else:\n",
    "            predicted_classes.append(1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test.values, predicted_classes, labels=[False, True])\n",
    "\n",
    "    # Calculate metrics from confusion matrix\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp)\n",
    "    npv = tn / (tn + fn)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "    mcc = matthews_corrcoef(y_test.values, predicted_classes)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Sensitivity:\", round(sensitivity,2))\n",
    "    print(\"Specificity:\", round(specificity,2))\n",
    "    print(\"Positive Predictive Value (PPV):\", round(ppv,2))\n",
    "    print(\"Negative Predictive Value (NPV):\", round(npv,2))\n",
    "    print(\"Balanced Accuracy:\", round(balanced_accuracy,2))\n",
    "    print(\"Matthews Correlation Coefficient (MCC):\", round(mcc,2))\n",
    "\n",
    "    # Plot confusion matrix with actual label names\n",
    "    # class_labels_display = [class_0, class_1]\n",
    "    plt.figure(figsize=(4, 3))  \n",
    "    myheatmap = sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=class_labels_display, yticklabels=class_labels_display, annot_kws={\"size\": 8}) \n",
    "    myheatmap.invert_yaxis()\n",
    "    plt.title(\"Confusion Matrix for the test set\", fontsize=10)  \n",
    "    plt.xlabel(\"Predicted Label\", fontsize=8)  \n",
    "    plt.ylabel(\"True Label\", fontsize=8)  \n",
    "    plt.xticks(fontsize=8)  \n",
    "    plt.yticks(fontsize=8)  \n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "04d6d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis and external_val:\n",
    "\n",
    "    # Predict cumulative hazard function for all test samples\n",
    "    surv_extval = rsf.predict_cumulative_hazard_function(X_extval_surv, return_array=True)\n",
    "\n",
    "    # Calculate distances from median curves for each individual\n",
    "    distances_class_0 = [euclidean_distance(curve, median_hazard_class_0_train) for curve in surv_extval]\n",
    "    distances_class_1 = [euclidean_distance(curve, median_hazard_class_1_train) for curve in surv_extval]\n",
    "\n",
    "    # Determine predicted class based on proximity to median curves\n",
    "    predicted_classes = []\n",
    "    for dist_0, dist_1 in zip(distances_class_0, distances_class_1):\n",
    "        if dist_0 < dist_1:\n",
    "            predicted_classes.append(0)\n",
    "        else:\n",
    "            predicted_classes.append(1)\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_extval_data.values, predicted_classes, labels=[False, True])\n",
    "\n",
    "    # Calculate metrics from confusion matrix\n",
    "    tn, fp, fn, tp = conf_matrix.ravel()\n",
    "\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "    ppv = tp / (tp + fp)\n",
    "    npv = tn / (tn + fn)\n",
    "    balanced_accuracy = (sensitivity + specificity) / 2\n",
    "\n",
    "    mcc = matthews_corrcoef(y_extval_data.values, predicted_classes)\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "    print(\"Sensitivity:\", round(sensitivity,2))\n",
    "    print(\"Specificity:\", round(specificity,2))\n",
    "    print(\"Positive Predictive Value (PPV):\", round(ppv,2))\n",
    "    print(\"Negative Predictive Value (NPV):\", round(npv,2))\n",
    "    print(\"Balanced Accuracy:\", round(balanced_accuracy,2))\n",
    "    print(\"Matthews Correlation Coefficient (MCC):\", round(mcc,2))\n",
    "\n",
    "    # Plot confusion matrix with actual label names\n",
    "    # class_labels_display = [class_0, class_1]\n",
    "    plt.figure(figsize=(4, 3))  \n",
    "    myheatmap = sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False, xticklabels=class_labels_display, yticklabels=class_labels_display, annot_kws={\"size\": 8}) \n",
    "    myheatmap.invert_yaxis()\n",
    "    plt.title(\"Confusion matrix for the external validation set\", fontsize=10)  \n",
    "    plt.xlabel(\"Predicted label\", fontsize=8)  \n",
    "    plt.ylabel(\"True label\", fontsize=8)  \n",
    "    plt.xticks(fontsize=8)  \n",
    "    plt.yticks(fontsize=8)  \n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "eb6c47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    # Predict cumulative hazard function for all test samples\n",
    "    surv_test = rsf.predict_cumulative_hazard_function(X_test_surv, return_array=True)\n",
    "\n",
    "    class_0_indices = np.where(y_test.values == False)[0] \n",
    "    class_1_indices = np.where(y_test.values == True)[0]  \n",
    "    \n",
    "  \n",
    "    # Separate predictions into classes\n",
    "    surv_class_0 = surv_test[class_0_indices]\n",
    "    surv_class_1 = surv_test[class_1_indices]\n",
    "\n",
    "    # Calculate median and interquartile range for both classes\n",
    "    median_surv_class_0_test = np.median(surv_class_0, axis=0)\n",
    "    q1_surv_class_0_test = np.percentile(surv_class_0, 25, axis=0)\n",
    "    q3_surv_class_0_test = np.percentile(surv_class_0, 75, axis=0)\n",
    "    iqr_surv_class_0_test = q3_surv_class_0_test - q1_surv_class_0_test\n",
    "\n",
    "    median_surv_class_1_test = np.median(surv_class_1, axis=0)\n",
    "    q1_surv_class_1_test = np.percentile(surv_class_1, 25, axis=0)\n",
    "    q3_surv_class_1_test = np.percentile(surv_class_1, 75, axis=0)\n",
    "    iqr_surv_class_1_test = q3_surv_class_1_test - q1_surv_class_1_test\n",
    "    \n",
    "   \n",
    "    rsf_riskscores_test = rsf.predict(X_test_surv)\n",
    "    \n",
    "    # Calculate average risk scores for each class in the test set\n",
    "    predicted_risk_socres_class_0 = rsf_riskscores_test[y_test]\n",
    "    predicted_risk_socres_class_1 = rsf_riskscores_test[~y_test]\n",
    "    \n",
    "    # Perform Mann-Whitney U test to compare the medians of the two classes\n",
    "    statistic, p_value = mannwhitneyu(predicted_risk_socres_class_0, predicted_risk_socres_class_1)\n",
    "    # Add annotation for statistical test\n",
    "    if p_value < 0.001:\n",
    "        p_value_text = \"<0.001\"\n",
    "    else:\n",
    "        p_value_text = f\"= {p_value:.4f}\"\n",
    "        \n",
    "    # Create subplots: one for the survival plot and one for the table\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, gridspec_kw={'height_ratios': [3, 1]}, figsize=(10, 8))\n",
    "\n",
    "    # Plot median and interquartile range for class 0 on the top plot\n",
    "    ax1.step(rsf.unique_times_, median_surv_class_0_test, where=\"post\", label=f\"median of predicted cumulative hazard in samples from {class_labels_display[0]} class\", color='b')\n",
    "    ax1.fill_between(rsf.unique_times_, q1_surv_class_0_test, q3_surv_class_0_test, step=\"post\", alpha=0.3, color='b', label=\"IQR\")\n",
    "\n",
    "    # Plot median and interquartile range for class 1\n",
    "    ax1.step(rsf.unique_times_, median_surv_class_1_test, where=\"post\", label=f\"median of predicted cumulative hazard in samples from {class_labels_display[1]} class\", color='r')\n",
    "    ax1.fill_between(rsf.unique_times_, q1_surv_class_1_test, q3_surv_class_1_test, step=\"post\", alpha=0.3, color='r', label=\"IQR\")\n",
    "    ax1.set_ylabel(\"Predicted cumulative hazard (test set)\") \n",
    "    ax1.set_xlabel(\"Time in days\") # Note: you should manually change it if for example the time is in months or years rather than in days\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Divide time into 5 intervals\n",
    "    num_intervals = 5\n",
    "    time_intervals = np.linspace(min(rsf.unique_times_), max(rsf.unique_times_), num_intervals + 1)\n",
    "\n",
    "    # Initialize arrays to store at risk, events, and censored counts for each interval\n",
    "    at_risk_intervals = np.zeros(num_intervals, dtype=int)\n",
    "    events_intervals = np.zeros(num_intervals, dtype=int)\n",
    "    censored_intervals = np.zeros(num_intervals, dtype=int)\n",
    "    \n",
    "    # Loop through each interval and calculate metrics\n",
    "    for i in range(num_intervals):\n",
    "        start_time = time_intervals[i]\n",
    "        end_time = time_intervals[i+1]\n",
    "        \n",
    "        # Patients at risk at the start of the interval\n",
    "        at_risk_intervals[i] = np.sum(y_test_surv >= start_time)\n",
    "        \n",
    "        # Events within the interval\n",
    "        events_intervals[i] = np.sum((y_test_surv > start_time) & (y_test_surv <= end_time) & (y_test == True))\n",
    "        \n",
    "        # Censored within the interval\n",
    "        censored_intervals[i] = np.sum((y_test_surv > start_time) & (y_test_surv < end_time) & (y_test == False))\n",
    "        \n",
    "    # Create the table data with individual and cumulative counts\n",
    "    table_data = np.array([\n",
    "        [f'{int(time_intervals[i])}-{int(time_intervals[i+1])}' for i in range(num_intervals)],\n",
    "        [f'{at_risk_intervals[i]}' for i in range(num_intervals)],\n",
    "        [f'{events_intervals[i]}' for i in range(num_intervals)],\n",
    "        [f'{censored_intervals[i]}' for i in range(num_intervals)]\n",
    "    ])\n",
    "\n",
    "    # Create the table in the second subplot\n",
    "    row_labels = [\"time interval\",'at risk', 'events', 'censored']\n",
    "\n",
    "    # Hide the axis for the table\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Add the table to the second subplot\n",
    "    table_display = table(ax2, cellText=table_data, rowLabels=row_labels,cellLoc='center', loc='center')\n",
    "    \n",
    "    # Add annotation for statistical test\n",
    "    ax1.annotate(f'Mann-Whitney U test \\n comparing the predicted risk scores \\np {p_value_text}', xy=(0.15, 0.3), xycoords='axes fraction', ha='center', va='center', fontsize = 10)\n",
    "    \n",
    "    # Adjust the layout to ensure the plots don't overlap\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.grid(which='both', color=\"grey\")\n",
    "    plt.grid(which='minor', alpha=0.1)\n",
    "    plt.grid(which='major', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"predicted_hazard_test.tif\", bbox_inches='tight')\n",
    "    # Show the final plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "8ea27178",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    # conduct a statistical test to see if there is any significant difference between the groups in their survival data\n",
    "    # This is what it does according to https://scikit-survival.readthedocs.io/en/v0.23.0/api/generated/sksurv.compare.compare_survival.html#sksurv.compare.compare_survival\n",
    "    # K-sample log-rank hypothesis test of identical survival functions.\n",
    "    # Compares the pooled hazard rate with each group-specific hazard rate. The alternative hypothesis is that the hazard rate of at least one group differs from the others at some time.\n",
    "\n",
    "    # Run the survival comparison\n",
    "    chisq, pvalue, stats, covariance = compare_survival(\n",
    "        y=y_test_surv_transformed,\n",
    "        group_indicator=y_test,\n",
    "        return_stats=True\n",
    "    )\n",
    "\n",
    "    # Prepare the data to create a reportable DataFrame\n",
    "    comparison_data = {\n",
    "        \"Chi-Square\": chisq,\n",
    "        \"p-value\": pvalue,\n",
    "        \"Statistics\": stats,\n",
    "        \"Covariance\": covariance\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better readability\n",
    "    comparison_df = pd.DataFrame([comparison_data])\n",
    "    comparison_df.to_excel(\"comparison_df_surv_testset.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "fdecb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis and external_val:\n",
    "    # Predict cumulative hazard function for all external validation samples\n",
    "    surv_extval = rsf.predict_cumulative_hazard_function(X_extval_surv, return_array=True)\n",
    "\n",
    "    class_0_indices = np.where(y_extval_data.values == False)[0] \n",
    "    class_1_indices = np.where(y_extval_data.values == True)[0]  \n",
    "    \n",
    "  \n",
    "    # Separate predictions into classes\n",
    "    surv_class_0 = surv_extval[class_0_indices]\n",
    "    surv_class_1 = surv_extval[class_1_indices]\n",
    "\n",
    "    # Calculate median and interquartile range for both classes\n",
    "    median_surv_class_0_extval = np.median(surv_class_0, axis=0)\n",
    "    q1_surv_class_0_extval = np.percentile(surv_class_0, 25, axis=0)\n",
    "    q3_surv_class_0_extval = np.percentile(surv_class_0, 75, axis=0)\n",
    "    iqr_surv_class_0_extval = q3_surv_class_0_test - q1_surv_class_0_test\n",
    "\n",
    "    median_surv_class_1_extval = np.median(surv_class_1, axis=0)\n",
    "    q1_surv_class_1_extval = np.percentile(surv_class_1, 25, axis=0)\n",
    "    q3_surv_class_1_extval = np.percentile(surv_class_1, 75, axis=0)\n",
    "    iqr_surv_class_1_extval = q3_surv_class_1_extval - q1_surv_class_1_extval\n",
    "    \n",
    "   \n",
    "    rsf_riskscores_extval = rsf.predict(X_test_surv)\n",
    "    \n",
    "    # Calculate average risk scores for each class in the test set\n",
    "    predicted_risk_socres_class_0 = rsf_riskscores_extval[y_extval_data]\n",
    "    predicted_risk_socres_class_1 = rsf_riskscores_extval[~y_extval_data]\n",
    "    \n",
    "    # Perform Mann-Whitney U test to compare the medians of the two classes\n",
    "    statistic, p_value = mannwhitneyu(predicted_risk_socres_class_0, predicted_risk_socres_class_1)\n",
    "    # Add annotation for statistical test\n",
    "    if p_value < 0.001:\n",
    "        p_value_text = \"<0.001\"\n",
    "    else:\n",
    "        p_value_text = f\"= {p_value:.4f}\"\n",
    "        \n",
    "    # Create subplots: one for the survival plot and one for the table\n",
    "    fig, (ax1, ax2) = plt.subplots(nrows=2, gridspec_kw={'height_ratios': [3, 1]}, figsize=(10, 8))\n",
    "\n",
    "    # Plot median and interquartile range for class 0 on the top plot\n",
    "    ax1.step(rsf.unique_times_, median_surv_class_0_extval, where=\"post\", label=f\"median of predicted cumulative hazard in samples from {class_labels_display[0]} class\", color='b')\n",
    "    ax1.fill_between(rsf.unique_times_, q1_surv_class_0_test, q3_surv_class_0_extval, step=\"post\", alpha=0.3, color='b', label=\"IQR\")\n",
    "\n",
    "    # Plot median and interquartile range for class 1\n",
    "    ax1.step(rsf.unique_times_, median_surv_class_1_extval, where=\"post\", label=f\"median of predicted cumulative hazard in samples from {class_labels_display[1]} class\", color='r')\n",
    "    ax1.fill_between(rsf.unique_times_, q1_surv_class_1_extval, q3_surv_class_1_extval, step=\"post\", alpha=0.3, color='r', label=\"IQR\")\n",
    "    ax1.set_ylabel(\"Predicted cumulative hazard (external validation set)\") \n",
    "    ax1.set_xlabel(\"Time in days\") # Note: you should manually change it if for example the time is in months or years rather than in days\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Divide time into 5 intervals\n",
    "    num_intervals = 5\n",
    "    time_intervals = np.linspace(min(rsf.unique_times_), max(rsf.unique_times_), num_intervals + 1)\n",
    "\n",
    "    # Initialize arrays to store at risk, events, and censored counts for each interval\n",
    "    at_risk_intervals = np.zeros(num_intervals, dtype=int)\n",
    "    events_intervals = np.zeros(num_intervals, dtype=int)\n",
    "    censored_intervals = np.zeros(num_intervals, dtype=int)\n",
    "    \n",
    "    # Loop through each interval and calculate metrics\n",
    "    for i in range(num_intervals):\n",
    "        start_time = time_intervals[i]\n",
    "        end_time = time_intervals[i+1]\n",
    "        \n",
    "        # Patients at risk at the start of the interval\n",
    "        at_risk_intervals[i] = np.sum(y_extval_surv >= start_time)\n",
    "        \n",
    "        # Events within the interval\n",
    "        events_intervals[i] = np.sum((y_extval_surv > start_time) & (y_extval_surv <= end_time) & (y_extval_data == True))\n",
    "        \n",
    "        # Censored within the interval\n",
    "        censored_intervals[i] = np.sum((y_extval_surv > start_time) & (y_extval_surv < end_time) & (y_extval_data == False))\n",
    "        \n",
    "    # Create the table data with individual and cumulative counts\n",
    "    table_data = np.array([\n",
    "        [f'{int(time_intervals[i])}-{int(time_intervals[i+1])}' for i in range(num_intervals)],\n",
    "        [f'{at_risk_intervals[i]}' for i in range(num_intervals)],\n",
    "        [f'{events_intervals[i]}' for i in range(num_intervals)],\n",
    "        [f'{censored_intervals[i]}' for i in range(num_intervals)]\n",
    "    ])\n",
    "\n",
    "    # Create the table in the second subplot\n",
    "    row_labels = [\"time interval\",'at risk', 'events', 'censored']\n",
    "\n",
    "    # Hide the axis for the table\n",
    "    ax2.axis('tight')\n",
    "    ax2.axis('off')\n",
    "\n",
    "    # Add the table to the second subplot\n",
    "    table_display = table(ax2, cellText=table_data, rowLabels=row_labels,cellLoc='center', loc='center')\n",
    "    \n",
    "    # Add annotation for statistical test\n",
    "    ax1.annotate(f'Mann-Whitney U test \\n comparing the predicted risk scores \\np {p_value_text}', xy=(0.15, 0.3), xycoords='axes fraction', ha='center', va='center', fontsize = 10)\n",
    "    \n",
    "    # Adjust the layout to ensure the plots don't overlap\n",
    "    plt.gca().set_facecolor('white')\n",
    "    plt.grid(which='both', color=\"grey\")\n",
    "    plt.grid(which='minor', alpha=0.1)\n",
    "    plt.grid(which='major', alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"predicted_hazard_test.tif\", bbox_inches='tight')\n",
    "    # Show the final plot\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "f61810d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis and external_val:\n",
    "    # Run the survival comparison\n",
    "    chisq, pvalue, stats, covariance = compare_survival(\n",
    "        y=y_extval_surv_transformed,\n",
    "        group_indicator=y_extval_data,\n",
    "        return_stats=True\n",
    "    )\n",
    "\n",
    "    # Prepare the data to create a reportable DataFrame\n",
    "    comparison_data = {\n",
    "        \"Chi-Square\": chisq,\n",
    "        \"p-value\": pvalue,\n",
    "        \"Statistics\": stats,\n",
    "        \"Covariance\": covariance\n",
    "    }\n",
    "\n",
    "    # Convert the dictionary to a DataFrame for better readability\n",
    "    comparison_df = pd.DataFrame([comparison_data])\n",
    "    comparison_df.to_excel(\"comparison_df_surv_extval.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "85380473",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis:\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Ensure test times are correctly extracted from the structured survival arrays\n",
    "        test_times_rsf = y_test_surv_transformed['time']  # Extract the time from the structured array\n",
    "        valid_times_rsf = np.unique(test_times_rsf)\n",
    "        valid_times_rsf = np.clip(valid_times_rsf, test_times_rsf.min(), test_times_rsf.max() - 1e-5)\n",
    "\n",
    "        # Predict the risk scores using the Random Survival Forest model\n",
    "        rsf_risk_scores = rsf.predict(X_test_surv)\n",
    "\n",
    "        # Calculate cumulative dynamic AUC\n",
    "        rsf_auc, rsf_mean_auc = cumulative_dynamic_auc(\n",
    "            survival_train=y_train_surv_transformed,\n",
    "            survival_test=y_test_surv_transformed,\n",
    "            estimate=rsf_risk_scores,\n",
    "            times=valid_times_rsf\n",
    "        )\n",
    "\n",
    "        # Plot the AUC over time\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(valid_times_rsf, rsf_auc, label=f'RSF AUC (mean = {rsf_mean_auc:.2f})', color='b')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('AUC (test set)')\n",
    "        plt.title('AUC over time for random survival forest')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}. Skipping to the next block.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "6c5bab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if survival_analysis and external_val:\n",
    "    \n",
    "    try:\n",
    "\n",
    "        # Ensure test times are correctly extracted from the structured survival arrays\n",
    "        test_times_rsf = y_extval_surv_transformed['time']  # Extract the time from the structured array\n",
    "        valid_times_rsf = np.unique(test_times_rsf)\n",
    "        valid_times_rsf = np.clip(valid_times_rsf, test_times_rsf.min(), test_times_rsf.max() - 1e-5)\n",
    "\n",
    "        # Predict the risk scores using the Random Survival Forest model\n",
    "        rsf_risk_scores = rsf.predict(X_extval_surv)\n",
    "\n",
    "        # Calculate cumulative dynamic AUC\n",
    "        rsf_auc, rsf_mean_auc = cumulative_dynamic_auc(\n",
    "            survival_train=y_train_surv_transformed,\n",
    "            survival_test=y_extval_surv_transformed,\n",
    "            estimate=rsf_risk_scores,\n",
    "            times=valid_times_rsf\n",
    "        )\n",
    "\n",
    "        # Plot the AUC over time\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(valid_times_rsf, rsf_auc, label=f'RSF AUC (mean = {rsf_mean_auc:.2f})', color='b')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('AUC (external validation set)')\n",
    "        plt.title('AUC over time for random survival forest')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}. Skipping to the next block.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9908a3",
   "metadata": {},
   "source": [
    "### Regression models\n",
    "Like survival analysis, if the data contains a column for continuous outcome variable then this analysis is relevant and can be conducted using the following code chunks. The continuous outcome is provided from a copy of the data that is saved in the beginning of the pipeline and it gets merged back to the train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0159908",
   "metadata": {},
   "source": [
    "#### Interpreting Regression Model Performance Metrics\n",
    "\n",
    "1) Mean Squared Error (MSE)\n",
    "- **Formula:** MSE = (1/n) * Σ(yᵢ - ŷᵢ)²\n",
    "- **Where:**\n",
    "  - \\( n \\) is the number of observations\n",
    "  - \\( yᵢ \\) is the actual value\n",
    "  - \\( ŷᵢ \\) is the predicted value\n",
    "- **Interpretation:**\n",
    "  - Measures the average squared difference between actual and predicted values.\n",
    "  - Lower MSE indicates better fit.\n",
    "  - Sensitive to outliers.\n",
    "\n",
    "2) Mean Absolute Error (MAE)\n",
    "- **Formula:** MAE = (1/n) * Σ|yᵢ - ŷᵢ|\n",
    "- **Where:**\n",
    "  - \\( n \\) is the number of observations\n",
    "  - \\( yᵢ \\) is the actual value\n",
    "  - \\( ŷᵢ \\) is the predicted value\n",
    "- **Interpretation:**\n",
    "  - Measures the average absolute difference between actual and predicted values.\n",
    "  - Lower MAE indicates better fit.\n",
    "  - Less sensitive to outliers than MSE.\n",
    "  - Same units as the original data.\n",
    "\n",
    "3) R-squared (R²)\n",
    "- **Formula:** R² = 1 - (Σ(yᵢ - ŷᵢ)² / Σ(yᵢ - ȳ)²)\n",
    "- **Where:**\n",
    "  - \\( yᵢ \\) is the actual value\n",
    "  - \\( ŷᵢ \\) is the predicted value\n",
    "  - \\( ȳ \\) is the mean of actual values\n",
    "- **Interpretation:**\n",
    "  - Measures the proportion of variance in the dependent variable explained by the model.\n",
    "  - Values range from -∞ to 1.\n",
    "  - Higher values indicate better fit.\n",
    "  - Negative values indicate the model performs worse than a horizontal line (mean of the target variable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "f33c5312",
   "metadata": {},
   "outputs": [],
   "source": [
    "if regression_analysis:\n",
    "    y_train_reg = pd.merge(y_train, mydata_copy_regression[regression_outcome], left_index=True, right_index=True, how='left')\n",
    "    y_train_reg.drop(columns=outcome_var,inplace = True)\n",
    "    y_test_reg = pd.merge(y_test, mydata_copy_regression[regression_outcome], left_index=True, right_index=True, how='left')\n",
    "    y_test_reg.drop(columns=outcome_var,inplace = True)\n",
    "    # Create and fit the linear regression model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_OHE_nocv, y_train_reg)\n",
    "\n",
    "    # Make predictions on the test set\n",
    "    y_pred = model.predict(X_test_OHE)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    mae = mean_absolute_error(y_test_reg, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Mean Squared Error (MSE): {:.2f}\".format(mse))\n",
    "    print(\"Mean Absolute Error (MAE): {:.2f}\".format(mae))\n",
    "    print(\"R-squared (R2): {:.2f}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "c06927f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if regression_analysis:\n",
    "    # Define the parameter grid for the random search\n",
    "    rf_param_grid = {\n",
    "        'n_estimators': [50, 100, 200, 500, 1000],  # Number of trees in the forest\n",
    "        'max_depth': [None, 10, 20, 30],  # Maximum depth of the trees\n",
    "        'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "        'min_samples_leaf': [1, 2, 4]  # Minimum number of samples required at each leaf node\n",
    "    }\n",
    "\n",
    "    # Create Random Forest model\n",
    "    rf = RandomForestRegressor()\n",
    "\n",
    "    # Create RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(estimator=rf, param_distributions=rf_param_grid, n_iter=n_iter_hptuning,\n",
    "                                       scoring='neg_mean_squared_error', cv=cv_folds_hptuning)\n",
    "\n",
    "    # Perform random search to find the best hyperparameters\n",
    "    random_search.fit(X_train_OHE_nocv, y_train_reg)\n",
    "\n",
    "    # Extract the best parameters\n",
    "    best_params = random_search.best_params_\n",
    "    print(\"Best parameters found: \", best_params)\n",
    "\n",
    "    # Train the final model on the entire training set with the best hyperparameters\n",
    "    best_model = RandomForestRegressor(**best_params)\n",
    "    best_model.fit(X_train_OHE_nocv, y_train_reg)\n",
    "\n",
    "    # Make predictions on the test set using the best model\n",
    "    y_pred = best_model.predict(X_test_OHE)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    mse = mean_squared_error(y_test_reg, y_pred)\n",
    "    mae = mean_absolute_error(y_test_reg, y_pred)\n",
    "    r2 = r2_score(y_test_reg, y_pred)\n",
    "\n",
    "    # Print the evaluation metrics\n",
    "    print(\"Mean Squared Error (MSE): {:.2f}\".format(mse))\n",
    "    print(\"Mean Absolute Error (MAE): {:.2f}\".format(mae))\n",
    "    print(\"R-squared (R2): {:.2f}\".format(r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "74621080",
   "metadata": {},
   "outputs": [],
   "source": [
    "if regression_analysis:\n",
    "    \n",
    "\n",
    "    # Initialize explainer with the best model\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "\n",
    "    # Calculate SHAP values for the test set\n",
    "    shap_values = explainer.shap_values(X_test_OHE)\n",
    "\n",
    "    # Summary plot\n",
    "    shap.summary_plot(shap_values, X_test_OHE, feature_names=X_train_OHE_nocv.columns)\n",
    "    \n",
    "    # save the model to disk\n",
    "    joblib.dump(best_model, 'regression_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac44b8b",
   "metadata": {},
   "source": [
    "#### specific codes for BSI project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "7983d854",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")\n",
    "mydata_backup = pd.read_csv(\"train_set_ADMper_7_multiclassBSI.csv\")\n",
    "testset_backup = pd.read_csv(\"test_set_ADMper_7_multiclassBSI.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a4560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from scipy.stats import pearsonr  \n",
    "\n",
    "# Convert the 'date' column to datetime\n",
    "testset_backup['date'] = pd.to_datetime(testset_backup['date'])\n",
    "\n",
    "# Initialize empty lists to store AUC and MCC values for each year\n",
    "auc_values = []\n",
    "mcc_values = []\n",
    "\n",
    "# Loop through each year bin\n",
    "for year_bin in range(2010, 2021):\n",
    "    # Filter data for the current year bin\n",
    "    test_data = testset_backup[testset_backup['date'].dt.year == year_bin]\n",
    "    # y_test_year_bin = y_test[y_test.index.isin(test_data.index)]\n",
    "\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, RandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        if isinstance(selected_model, (HistGradientBoostingClassifier)):\n",
    "            thr = opt_threshold_HGBC\n",
    "        elif isinstance(selected_model, (RandomForestClassifier)):\n",
    "            thr = opt_threshold_RF\n",
    "        elif isinstance(selected_model, (LogisticRegression)):\n",
    "            thr = opt_threshold_LR\n",
    "        elif isinstance(selected_model, (GaussianNB)):\n",
    "            thr = opt_threshold_NB    \n",
    "        X_test_year_bin = X_test_OHE[X_test_OHE.index.isin(test_data.index)]\n",
    "        predictions_year_bin = selected_model.predict_proba(X_test_year_bin)\n",
    "        predictions_year_bin = predictions_year_bin[:, 1]\n",
    "        y_test_year_bin = y_test[y_test.index.isin(X_test_year_bin.index)]\n",
    "        y_pred_year_bin = [True if x >= thr else False for x in predictions_year_bin]\n",
    "        misclassified_year_bin = y_pred_year_bin != y_test_year_bin\n",
    "    elif isinstance(selected_model, (cb.CatBoostClassifier)):\n",
    "        thr = opt_threshold_CB\n",
    "        X_test_year_bin = X_test[X_test.index.isin(test_data.index)]\n",
    "        predictions_year_bin = selected_model.predict_proba(X_test_year_bin)\n",
    "        predictions_year_bin = predictions_year_bin[:, 1]\n",
    "        y_test_year_bin = y_test[y_test.index.isin(X_test_year_bin.index)]\n",
    "        y_pred_year_bin = [True if x >= thr else False for x in predictions_year_bin]\n",
    "        misclassified_year_bin = y_pred_year_bin != y_test_year_bin\n",
    "        \n",
    "    else: # LGBM\n",
    "        thr = opt_threshold_LGBM\n",
    "        X_test_year_bin = X_test[X_test.index.isin(test_data.index)]\n",
    "        predictions_year_bin = selected_model.predict_proba(X_test_year_bin)\n",
    "        predictions_year_bin = predictions_year_bin[:, 1]\n",
    "        y_test_year_bin = y_test[y_test.index.isin(X_test_year_bin.index)]\n",
    "        y_pred_year_bin = [True if x >= thr else False for x in predictions_year_bin]\n",
    "        misclassified_year_bin = y_pred_year_bin != y_test_year_bin\n",
    "\n",
    "    # Calculate AUC and MCC\n",
    "    auc = roc_auc_score(y_test_year_bin, predictions_year_bin)\n",
    "    mcc = matthews_corrcoef(y_test_year_bin, y_pred_year_bin)\n",
    "\n",
    "    # Append values to the lists\n",
    "    auc_values.append(auc)\n",
    "    mcc_values.append(mcc)\n",
    "\n",
    "years = range(2010, 2021)\n",
    "# Perform Pearson correlation test between AUC and years\n",
    "correlation_coefficient, p_value = pearsonr(auc_values, years)\n",
    "\n",
    "# Print the correlation coefficient and p-value\n",
    "print(f\"Pearson Correlation Coefficient (AUC): {correlation_coefficient}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Plotting AUC and MCC over the years\n",
    "plt.plot(years, auc_values, label='AUC')\n",
    "plt.plot(years, mcc_values, label='MCC')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('AUC and MCC Over the Years')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926024e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = y_test.index\n",
    "\n",
    "# Identify correct and incorrect predictions when y_test is True\n",
    "correctly_predicted_age_and_s = testset_backup.loc[test_index[(y_test == y_pred)], ['Age', 'Sex']]\n",
    "incorrectly_predicted_age_and_s = testset_backup.loc[test_index[(y_test != y_pred)], ['Age', 'Sex']]\n",
    "\n",
    "# Create DataFrames for correct and incorrect predictions\n",
    "correct_results_df = pd.DataFrame({\"Age\": correctly_predicted_age_and_s['Age'], 'Sex': correctly_predicted_age_and_s['Sex']})\n",
    "incorrect_results_df = pd.DataFrame({\"Age\": incorrectly_predicted_age_and_s['Age'], 'Sex': incorrectly_predicted_age_and_s['Sex']})\n",
    "\n",
    "# Save DataFrames to separate Excel files\n",
    "correct_results_df.to_excel(\"correctly_predicted_age_and_s.xlsx\", index=False)\n",
    "incorrect_results_df.to_excel(\"incorrectly_predicted_age_and_s.xlsx\", index=False)\n",
    "\n",
    "print(\"Results saved to correct_predictions.xlsx and incorrect_predictions.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9046a5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "# Combine correct and incorrect predictions in a single DataFrame for plotting\n",
    "combined_df = pd.concat([correct_results_df.assign(Type='Correct'), incorrect_results_df.assign(Type='Incorrect')])\n",
    "\n",
    "# Plotting boxplot for correct and incorrect predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Sex', y='Age', data=combined_df, hue='Type', palette='coolwarm')\n",
    "plt.title('Distribution of age for correct and incorrect predictions')\n",
    "\n",
    "# Statistical test (Mann-Whitney U test)\n",
    "statistic, p_value = mannwhitneyu(correct_results_df['Age'], incorrect_results_df['Age'])\n",
    "print(f\"Mann-Whitney U test p-value: {p_value}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099e6d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify correct and incorrect predictions when y_test is True\n",
    "correctly_predicted_ids = testset_backup.loc[test_index[(y_test == y_pred) & y_test], 'allbac']\n",
    "incorrectly_predicted_ids = testset_backup.loc[test_index[(y_test != y_pred) & y_test], 'allbac']\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrames for correct and incorrect predictions\n",
    "correct_results_df = pd.DataFrame({\"Correctly Predicted allbacs\": correctly_predicted_ids})\n",
    "incorrect_results_df = pd.DataFrame({\"Incorrectly Predicted allbacs\": incorrectly_predicted_ids})\n",
    "\n",
    "# Save DataFrames to separate Excel files\n",
    "correct_results_df.to_excel(\"correct_predictions.xlsx\", index=False)\n",
    "incorrect_results_df.to_excel(\"incorrect_predictions.xlsx\", index=False)\n",
    "\n",
    "print(\"Results saved to correct_predictions.xlsx and incorrect_predictions.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b470cb90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split names in the 'names' column and create a list of all names\n",
    "all_names = [name.split(';') for name in correct_results_df['Correctly Predicted allbacs']]\n",
    "\n",
    "# Flatten the list of lists\n",
    "flat_list = [name for sublist in all_names for name in sublist]\n",
    "\n",
    "# Create a Pandas Series from the flattened list\n",
    "names_series = pd.Series(flat_list)\n",
    "\n",
    "# Get the counts of each name\n",
    "name_counts = names_series.value_counts()\n",
    "\n",
    "# Plot the most frequent names\n",
    "top_names = name_counts.head(10)  # Change the number as per your preference\n",
    "top_names.plot(kind='bar', xlabel='Names', ylabel='Frequency', title='Top 10 most frequent pathogens correctly predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4469ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split names in the 'names' column and create a list of all names\n",
    "all_names = [name.split(';') for name in correct_results_df['Correctly Predicted allbacs']]\n",
    "\n",
    "# Flatten the list of lists\n",
    "flat_list = [name for sublist in all_names for name in sublist]\n",
    "\n",
    "# Create a Pandas Series from the flattened list\n",
    "names_series = pd.Series(flat_list)\n",
    "\n",
    "# Get the counts of each name\n",
    "name_counts = names_series.value_counts()\n",
    "\n",
    "# Plot the most frequent names\n",
    "top_names = name_counts.head(10)  # Change the number as per your preference\n",
    "top_names.plot(kind='bar', xlabel='Names', ylabel='Frequency', title='Top 10 most frequent pathogens correctly predicted')\n",
    "plt.show()\n",
    "\n",
    "# Split names in the 'names' column and create a list of all names\n",
    "all_names = [name.split(';') for name in incorrect_results_df['Incorrectly Predicted allbacs']]\n",
    "\n",
    "# Flatten the list of lists\n",
    "flat_list = [name for sublist in all_names for name in sublist]\n",
    "\n",
    "# Create a Pandas Series from the flattened list\n",
    "names_series = pd.Series(flat_list)\n",
    "\n",
    "# Get the counts of each name\n",
    "name_counts = names_series.value_counts()\n",
    "\n",
    "# Plot the most frequent names\n",
    "top_names = name_counts.head(10)  # Change the number as per your preference\n",
    "top_names.plot(kind='bar', xlabel='Names', ylabel='Frequency', title='Top 10 most frequent pathogens incorrectly predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ce4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract names from correct predictions\n",
    "all_names_correct = [name.split(';') for name in correct_results_df['Correctly Predicted allbacs']]\n",
    "flat_list_correct = [name for sublist in all_names_correct for name in sublist]\n",
    "names_series_correct = pd.Series(flat_list_correct)\n",
    "name_counts_correct = names_series_correct.value_counts()\n",
    "\n",
    "# Extract names from incorrect predictions\n",
    "all_names_incorrect = [name.split(';') for name in incorrect_results_df['Incorrectly Predicted allbacs']]\n",
    "flat_list_incorrect = [name for sublist in all_names_incorrect for name in sublist]\n",
    "names_series_incorrect = pd.Series(flat_list_incorrect)\n",
    "name_counts_incorrect = names_series_incorrect.value_counts()\n",
    "\n",
    "# Find common names\n",
    "common_names = list(set(name_counts_correct.index) & set(name_counts_incorrect.index))\n",
    "\n",
    "# Create a DataFrame with counts for each common name\n",
    "common_names_df = pd.DataFrame(index=common_names, columns=['Correct Counts', 'Incorrect Counts'])\n",
    "\n",
    "# Fill in the counts for correct predictions\n",
    "common_names_df['Correct Counts'] = name_counts_correct[common_names]\n",
    "\n",
    "# Fill in the counts for incorrect predictions\n",
    "common_names_df['Incorrect Counts'] = name_counts_incorrect[common_names]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(common_names_df)\n",
    "common_names_df.to_excel(\"common_names_df.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d10aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with counts for each common name\n",
    "common_names_df = pd.DataFrame(index=common_names, columns=['Correctly classified samples', 'Incorrectly classified samples'])\n",
    "\n",
    "# Fill in the counts for correct predictions\n",
    "common_names_df['Correctly classified samples'] = name_counts_correct[common_names]\n",
    "\n",
    "# Fill in the counts for incorrect predictions\n",
    "common_names_df['Incorrectly classified samples'] = name_counts_incorrect[common_names]\n",
    "\n",
    "# Sort by clusters (you might want to adjust the clustering method and metric)\n",
    "clustered_df = common_names_df.copy()\n",
    "clustered_df['Total Counts'] = clustered_df.sum(axis=1)\n",
    "clustered_df = clustered_df.sort_values(by='Total Counts', ascending=False)\n",
    "clustered_df = clustered_df.drop('Total Counts', axis=1)\n",
    "\n",
    "# Set smaller font size\n",
    "sns.set(font_scale=0.8)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(clustered_df, cmap='YlGnBu', annot=True, fmt='g', cbar_kws={'label': 'Counts'})\n",
    "# Shift y-axis tick labels down by adjusting their positions\n",
    "yticks = ax.get_yticks()  # Get current y-tick positions\n",
    "yticklabels = ax.get_yticklabels()  # Get current y-tick labels\n",
    "ax.set_yticks([tick + 0.1 for tick in yticks])  # Shift y-ticks down by 0.1 units\n",
    "ax.set_yticklabels(yticklabels, rotation=0)  # Keep the labels horizontal\n",
    "\n",
    "# Add gridlines for better readability\n",
    "ax.grid(axis='y', linestyle='--', linewidth=0.5)\n",
    "\n",
    "plt.tight_layout()  # Adjust layout to ensure labels fit well\n",
    "\n",
    "plt.title('classification results by pathogens')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bc67ca",
   "metadata": {},
   "source": [
    "## Report the environment\n",
    "Report Conda packages required to run the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff8129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conda_environment():\n",
    "    conda_list = subprocess.check_output(['conda', 'list']).decode('utf-8')\n",
    "    return conda_list\n",
    "\n",
    "def get_python_info():\n",
    "    python_version = platform.python_version()\n",
    "    return python_version\n",
    "\n",
    "def get_system_info():\n",
    "    system_info = {\n",
    "        'OS': platform.system(),\n",
    "        'Number of CPUs': n_cpu_model_training,\n",
    "        'Memory': f'{psutil.virtual_memory().total / (1024 ** 3):.2f} GB'\n",
    "    }\n",
    "    return system_info\n",
    "\n",
    "def get_gpu_info():\n",
    "    if GPU_avail:\n",
    "        try:\n",
    "            gpu_info = subprocess.check_output(['nvidia-smi']).decode('utf-8')\n",
    "            return gpu_info\n",
    "        except subprocess.CalledProcessError:\n",
    "            return \"GPU information not available (nvidia-smi not installed or no NVIDIA GPU detected)\"\n",
    "    else:\n",
    "        return \"GPU not used\"\n",
    "        \n",
    "# Record end time\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate duration\n",
    "duration = end_time - start_time\n",
    "\n",
    "# Define the filename for the report\n",
    "report_filename = 'pipeline_report.txt'\n",
    "\n",
    "# Open the file for writing\n",
    "with open(report_filename, 'w') as f:\n",
    "    # Write Conda environment to the file\n",
    "    f.write(\"Conda environment:\\n\")\n",
    "    f.write(get_conda_environment())\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    # Write Python version to the file\n",
    "    f.write(\"Python version:\\n\")\n",
    "    f.write(get_python_info())\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    # Write system information to the file\n",
    "    f.write(\"System information:\\n\")\n",
    "    system_info = get_system_info()\n",
    "    for key, value in system_info.items():\n",
    "        f.write(f\"{key}: {value}\\n\")\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # Write GPU information to the file\n",
    "    f.write(\"GPU information:\\n\")\n",
    "    f.write(get_gpu_info())\n",
    "    f.write(\"\\n\")\n",
    "    \n",
    "    # Write duration to the file\n",
    "    f.write(\"Pipeline execution duration (seconds):\\n\")\n",
    "    f.write(str(duration))\n",
    "\n",
    "print(f\"Report saved as {report_filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa9ace5",
   "metadata": {},
   "source": [
    "## Save the pipeline logs in HTML format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1926b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_notebook():\n",
    "    display(Javascript('IPython.notebook.save_checkpoint();'))\n",
    "\n",
    "save_notebook()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d18fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to where the notebook file is located\n",
    "os.chdir(\"..\")\n",
    "\n",
    "# Convert the notebook to HTML using nbconvert\n",
    "!jupyter nbconvert $JupyterNotebook_filename --to html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb946df8",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Information about all the packages (libraries) utilized in this pipeline is available in the exported report \"pipeline_report.txt\". \n",
    "\n",
    "More information about the methods used in this pipeline:\n",
    "\n",
    "- **QLattice model**\n",
    "  - Broløs, K. R. et al. An Approach to Symbolic Regression Using Feyn. (2021)\n",
    "\n",
    "- **Sci-kit learn**\n",
    "  - Pedregosa, F. et al. Scikit-learn: Machine learning in Python. J. Mach. Learn. Res. 12, 2825–2830 (2011)\n",
    "\n",
    "- **CatBoost**\n",
    "  - Dorogush, A. V., Ershov, V. & Gulin, A. CatBoost: gradient boosting with categorical features support. (2018)\n",
    "\n",
    "- **LightGBM**\n",
    "  - Ke, G. et al. LightGBM: A highly efficient gradient boosting decision tree. in Advances in Neural Information Processing Systems (2017)\n",
    "\n",
    "- **SHAP**\n",
    "  - Lundberg, S. M. & Lee, S.-I. A unified approach to interpreting model predictions. in Advances in neural information processing systems 4765–4774 (2017)\n",
    "  - Lundberg, S. M. et al. From local explanations to global understanding with explainable AI for trees. Nat. Mach. Intell. 2, 56–67 (2020)\n",
    "\n",
    "- **SHAP clustering**\n",
    "  - Zargari Marandi, R. et al. Development of a machine learning model for early prediction of plasma leakage in suspected dengue patients. PLoS Negl. Trop. Dis. 17, e0010758 (2023)\n",
    "  - Ramtin Zargari Marandi, ExplaineR: an R package to explain machine learning models, Bioinformatics Advances, Volume 4, Issue 1, 2024, vbae049, [link](https://doi.org/10.1093/bioadv/vbae049)\n",
    "  \n",
    "- **Survival SHAP**\n",
    "  - Krzyziński, Mateusz, et al. \"SurvSHAP (t): time-dependent explanations of machine learning survival models.\" Knowledge-Based Systems 262 (2023): 110234.\n",
    "  - https://github.com/MI2DataLab/survshap\n",
    "\n",
    "More information about for example data imputation and clustering methods, and other models can be found in https://scikit-learn.org/stable/.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
