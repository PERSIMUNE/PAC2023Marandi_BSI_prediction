{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "# Constants\n",
    "SEED = 123\n",
    "FOLDS = 5\n",
    "VERBOSE = 0\n",
    "lookback_period = 30 # optional\n",
    "gp_value = 0 # optional\n",
    "biokemi = pd.read_csv('biokemi.csv', sep=';', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi['ANALYSEKODE_MAIDS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi.groupby('ANALYSEKODE_MAIDS')['REPLYUNIT'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by 'analysis' and check if all corresponding 'unit' values are the same\n",
    "result = biokemi.groupby('ANALYSEKODE_MAIDS')['REPLYUNIT'].nunique()\n",
    "\n",
    "# Identify rows where 'unit' values for the same 'analysis' are different\n",
    "rows_with_different_units = result[result > 1].index\n",
    "\n",
    "# Display rows with different 'unit' values along with the units\n",
    "if not rows_with_different_units.empty:\n",
    "    print(\"Rows with different 'unit' values for the same 'analysis':\")\n",
    "    for analysis_value in rows_with_different_units:\n",
    "        units_for_analysis = biokemi.loc[biokemi['ANALYSEKODE_MAIDS'] == analysis_value, 'REPLYUNIT'].unique()\n",
    "        print(f\"Analysis: {analysis_value}, Units: {', '.join(map(str, units_for_analysis))}\")\n",
    "else:\n",
    "    print(\"All 'unit' values for the same 'analysis' are the same.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi['ANALYSEKODE_MAIDS'] = biokemi['ANALYSEKODE_MAIDS'].str.replace('NA', 'Sodium')\n",
    "biokemi['ANALYSEKODE_MAIDS'] = biokemi['ANALYSEKODE_MAIDS'].str.replace('nan', 'Sodium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi['ANALYSEKODE_MAIDS'] = biokemi['ANALYSEKODE_MAIDS'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi.dropna(subset=['REPLY'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the PRVTAG_TID column into a new column named parsed_date\n",
    "biokemi[\"parsed_date\"] = pd.to_datetime(biokemi[\"PRVTAG_TID\"], format=\"%d-%m-%Y %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "# If parsing failed, try to parse using dmy format for the date part only\n",
    "mask = biokemi[\"parsed_date\"].isna()\n",
    "biokemi.loc[mask, \"parsed_date\"] = pd.to_datetime(\n",
    "    biokemi.loc[mask, \"PRVTAG_TID\"],\n",
    "    format=\"%d-%m-%Y\",\n",
    "    errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Extract date from parsed_date and store it in a new column named date_only\n",
    "biokemi[\"date_only\"] = biokemi[\"parsed_date\"].dt.date\n",
    "\n",
    "biokemi[\"date_only\"] = biokemi[\"date_only\"] + pd.to_timedelta(gp_value, unit='d')\n",
    "\n",
    "# Remove the parsed_date column if it's no longer needed\n",
    "biokemi.drop(\"parsed_date\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Filter records from the beginning of 2010 to the end of 2020\n",
    "biokemi = biokemi[(biokemi['date_only'] >= pd.to_datetime('2010-01-01').date()) & (biokemi['date_only'] <= pd.to_datetime('2020-12-31').date())]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(biokemi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace ',' with '.' in the 'repl' column\n",
    "biokemi['REPLY'] = biokemi['REPLY'].str.replace(',', '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# biokemi['REPLY'] = biokemi['REPLY'].astype(float)\n",
    "\n",
    "# Convert 'value' column to float, handling invalid entries by coercing to NaN\n",
    "biokemi['REPLY'] = pd.to_numeric(biokemi['REPLY'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion function\n",
    "def convert_units(row):\n",
    "    if row['ANALYSEKODE_MAIDS'] == 'TRANS':\n",
    "        if row['REPLYUNIT'] == 'µmol/L':\n",
    "            # No conversion needed for transferrin concentration in µmol/L\n",
    "            return row['REPLY']\n",
    "        elif row['REPLYUNIT'] == 'g/L':\n",
    "            # Conversion from g/L to µmol/L using the molecular weight of transferrin\n",
    "            molecular_weight = 79570.0  # molecular weight of transferrin in daltons\n",
    "            conversion_factor = 1e6 / molecular_weight  # 1 g/L = 1e6 µmol/L (molecular weight in µg)\n",
    "            return row['REPLY'] * conversion_factor\n",
    "    else:\n",
    "        return row['REPLY']\n",
    "\n",
    "# Update the 'value' column with converted values for 'transferrin'\n",
    "biokemi['REPLY'] = biokemi.apply(convert_units, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biokemi[\"INVESTIGATION_NAME\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_analys_values = biokemi[\"ANALYSEKODE_MAIDS\"].unique()\n",
    "# Print all unique values one by one\n",
    "for value in unique_analys_values:\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot the DataFrame to wide format\n",
    "wide_bi = biokemi.pivot_table(index=['ID',\"date_only\"], columns='ANALYSEKODE_MAIDS', values='REPLY', aggfunc='mean')\n",
    "# Reset the index if needed\n",
    "wide_bi.reset_index(inplace=True)\n",
    "\n",
    "# Display the wide-format DataFrame\n",
    "print(wide_bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi[\"BASO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all(col in wide_bi.columns for col in [\"BASO\", \"BASOPO\"]):\n",
    "    wide_bi[\"BASO\"] = np.nanmean(wide_bi[[\"BASO\", \"BASOPO\"]], axis=1)\n",
    "    wide_bi.drop([\"BASOPO\"], axis=1, inplace=True)\n",
    "else:\n",
    "    print(\"Either 'BASO' or 'BASOPO' not found in the DataFrame columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_process = [\n",
    "    (\"CAI\", \"CAIPOC\"),\n",
    "    (\"CL\", \"CLPOC\"),\n",
    "    (\"GLU\", \"GLUPOC\"),\n",
    "    (\"HB\", \"HBPOC\"),\n",
    "    (\"K\", \"KPOC\"),\n",
    "    (\"LEU\", \"LEUPOC\"),\n",
    "    (\"LYMFO\", \"LYMFOP\"),\n",
    "    (\"MONO\", \"MONOPO\")\n",
    "]\n",
    "\n",
    "for col, colpoc in columns_to_process:\n",
    "    if all(c in wide_bi.columns for c in [col, colpoc]):\n",
    "        wide_bi[col] = np.nanmean(wide_bi[[col, colpoc]], axis=1)\n",
    "        wide_bi.drop([colpoc], axis=1, inplace=True)\n",
    "    else:\n",
    "        print(f\"Either '{col}' or '{colpoc}' not found in the DataFrame columns.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize biochemistry lab data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select columns to visualize (excluding \"ID\" and \"date\")\n",
    "columns_to_visualize = wide_bi.columns.difference([\"ID\", \"date_only\"])\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(nrows=len(columns_to_visualize) // 3 + 1, ncols=3, figsize=(15, 5 * (len(columns_to_visualize) // 3 + 1)))\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through columns and plot histograms\n",
    "for i, column in enumerate(columns_to_visualize):\n",
    "    sns.histplot(wide_bi[column].dropna(), ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Distribution of {column}')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Select columns to filter (excluding \"ID\" and \"date\")\n",
    "columns_to_filter = wide_bi.columns.difference([\"ID\", \"date_only\"])\n",
    "\n",
    "# Define a custom function to filter negative and zero values and extreme outliers\n",
    "def filter_negatives_and_extreme_outliers(column):\n",
    "    # Convert negative values to NaN\n",
    "    column[column <= 0] = np.nan\n",
    "    \n",
    "    # Calculate the first and third quartiles\n",
    "    Q1 = column.quantile(0.25)\n",
    "    Q3 = column.quantile(0.75)\n",
    "    \n",
    "    # Calculate the interquartile range (IQR)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define the lower and upper bounds for extreme outliers\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    \n",
    "    # Convert extreme outliers to NaN\n",
    "    column[(column < lower_bound) | (column > upper_bound)] = np.nan\n",
    "    \n",
    "    return column\n",
    "\n",
    "# Apply the custom function to each selected column\n",
    "wide_bi[columns_to_filter] = wide_bi[columns_to_filter].apply(filter_negatives_and_extreme_outliers)\n",
    "\n",
    "# Display the modified DataFrame\n",
    "print(wide_bi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the numerical features you want to scale\n",
    "numerical_columns = wide_bi.select_dtypes(include=['float64', 'int64']).columns\n",
    "has_negative_values = np.any(wide_bi[numerical_columns] < 0)\n",
    "\n",
    "if has_negative_values:\n",
    "    print(\"There are negative values in wide_bi.\")\n",
    "else:\n",
    "    print(\"There are no negative values in wide_bi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Select columns to visualize (excluding \"ID\" and \"date\")\n",
    "columns_to_visualize = wide_bi.columns.difference([\"ID\", \"date_only\"])\n",
    "\n",
    "# Set up subplots\n",
    "fig, axes = plt.subplots(nrows=len(columns_to_visualize) // 3 + 1, ncols=3, figsize=(15, 5 * (len(columns_to_visualize) // 3 + 1)))\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Loop through columns and plot histograms\n",
    "for i, column in enumerate(columns_to_visualize):\n",
    "    sns.histplot(wide_bi[column].dropna(), ax=axes[i], kde=True)\n",
    "    axes[i].set_title(f'Distribution of {column}')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline variables for biochemical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi.rename(columns={'date_only': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi.rename(columns={'SodiumPOC': 'Sodium'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wide_bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load LPR data (admission data already cleaned and filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LPR = pd.read_csv('LPR_filtered.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(LPR[\"ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LPR[\"PATIENTTYPE\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load bacteremia data (already cleaned and filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteremia = pd.read_csv('bacteremia_filtered.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteremia.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to exclude\n",
    "exclude_columns = [\"ID\",\"TestDate\",\"Sex\",\"Age\",\"BSIClass\"]\n",
    "bacteremia['allbac'] = bacteremia.apply(lambda row: ';'.join([col for col in bacteremia.columns if col not in exclude_columns and row[col]]), axis=1)\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(bacteremia)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "in_columns1 = [\"Staphylococcus aureus\", \"Staphylococcus aureus (MRSA)\"]\n",
    "bacteremia['Staphylococcus_aureus'] = bacteremia[in_columns1].any(axis=1)\n",
    "\n",
    "in_columns2 = [\"E. coli\", \"E. coli (ESBL positiv)\"]\n",
    "bacteremia['E.coli'] = bacteremia[in_columns2].any(axis=1)\n",
    "\n",
    "in_columns3 = ~bacteremia.columns.isin(in_columns1 + in_columns2 + exclude_columns)\n",
    "bacteremia['others'] = bacteremia.iloc[:, in_columns3].any(axis=1)\n",
    "\n",
    "bacteremia[\"BSImulticlass\"] = \"noBSI\"\n",
    "for i in range(len(bacteremia)):\n",
    "    if bacteremia.loc[i, 'Staphylococcus_aureus']:\n",
    "        bacteremia.loc[i, \"BSImulticlass\"] = 'Staphylococcus_aureus'\n",
    "    elif bacteremia.loc[i, 'E.coli']:\n",
    "        bacteremia.loc[i, \"BSImulticlass\"] = 'E.coli'\n",
    "    elif bacteremia.loc[i, 'others']:\n",
    "        bacteremia.loc[i, \"BSImulticlass\"] = 'others'\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_counts = bacteremia[\"BSImulticlass\"].value_counts()\n",
    "category_percentages = bacteremia[\"BSImulticlass\"].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"Category Counts:\")\n",
    "print(category_counts)\n",
    "\n",
    "print(\"\\nCategory Percentages:\")\n",
    "print(category_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteremia.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "df = bacteremia.copy()\n",
    "\n",
    "# Convert the 'Date' column to datetime type\n",
    "df['TestDate'] = pd.to_datetime(df['TestDate'])\n",
    "\n",
    "# Specify the columns to exclude\n",
    "excluded_columns = ['ID', 'TestDate', 'Sex', 'BSIClass', 'Age','allbac']\n",
    "\n",
    "# Create time bins (1-year intervals)\n",
    "time_bins = pd.date_range(start='2010-01-01', end='2020-12-31', freq='Y')\n",
    "\n",
    "# Cut the 'Date' column into time bins\n",
    "df['Time Bin'] = pd.cut(df['TestDate'], bins=time_bins, labels=time_bins[:-1])\n",
    "\n",
    "# Initialize an empty dictionary to store results for each year\n",
    "results_dict = {}\n",
    "\n",
    "# Loop through each year\n",
    "for i, year in enumerate(time_bins[:-1]):\n",
    "    start_date = time_bins[i]\n",
    "    end_date = time_bins[i + 1]\n",
    "    \n",
    "    # Filter data for the current year\n",
    "    subset = df[(df['TestDate'] >= start_date) & (df['TestDate'] < end_date)]\n",
    "\n",
    "    # Exclude specified columns\n",
    "    subset = subset.drop(columns=excluded_columns, errors='ignore')\n",
    "\n",
    "    # Count the cumulative occurrences of each bacteria for the current year\n",
    "    bacteria_counts = subset.sum()\n",
    "\n",
    "    # Find the top 10 most common bacteria for the current year\n",
    "    top10_bacteria = bacteria_counts.nlargest(10).index.tolist()\n",
    "\n",
    "    # Store the results for the current year in the dictionary\n",
    "    results_dict[year] = {'Top 10 Bacteria': top10_bacteria, 'Bacteria Counts': bacteria_counts}\n",
    "\n",
    "# Transpose the results for plotting\n",
    "results_df = pd.DataFrame(results_dict).T\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(15, 8))\n",
    "\n",
    "# Use seaborn for better color and style options\n",
    "palette = sns.color_palette('husl', n_colors=15) \n",
    "markers = ['o', 's', '^', 'D', 'v', '>', '<', 'p', '*', 'h', '+', 'x', '|', '_', '.', ',']  \n",
    "\n",
    "legend_names_added = set()\n",
    "\n",
    "# Plot counts of top 10 bacteria for each year\n",
    "for i, bacterium in enumerate(results_df['Top 10 Bacteria'].explode().unique()):\n",
    "    marker = markers[i % len(markers)]  # Cycle through markers\n",
    "    color = palette[i % len(palette)]  # Cycle through colors\n",
    "\n",
    "    for year in results_df.index:\n",
    "        # Check if the bacterium is in the top 10 for the current year\n",
    "        if bacterium in results_df.loc[year, 'Top 10 Bacteria']:\n",
    "            count = results_df.loc[year, 'Bacteria Counts'][bacterium]\n",
    "            label = bacterium if bacterium not in legend_names_added else None\n",
    "            plt.scatter([year], count, marker=marker, color=color, label=label)\n",
    "            legend_names_added.add(bacterium)\n",
    "\n",
    "# Plot counts of 'BSI' for each year on a logarithmic scale\n",
    "bsi_counts = df[df['BSIClass'] == 'BSI'].groupby('Time Bin')['BSIClass'].count()\n",
    "plt.scatter(bsi_counts.index, bsi_counts, marker='x', color='black', label='BSI')\n",
    "\n",
    "plt.yscale('log')  # Set y-axis to logarithmic scale\n",
    "\n",
    "plt.title('Top 10 most common bloodstream infections for each year (2010-2020)')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Count')\n",
    "plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n",
    "# Save the plot with high resolution\n",
    "plt.savefig('top10infections.svg', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bacteremia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LPR.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"ID\", \"INDDATO_DATO_TID\", \"UDDATO_DATO_TID\"]\n",
    "ADM = LPR[selected_columns] # Admission data\n",
    "ADM = ADM.dropna(subset=['ID'])\n",
    "\n",
    "ADM['ID'] = ADM['ID'].astype(int)\n",
    "ADM['INDDATO_DATO_TID'] = pd.to_datetime(ADM['INDDATO_DATO_TID'])\n",
    "ADM['UDDATO_DATO_TID'] = pd.to_datetime(ADM['UDDATO_DATO_TID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM.rename(columns={'INDDATO_DATO_TID': 'IN_DATE'}, inplace=True)\n",
    "ADM.rename(columns={'UDDATO_DATO_TID': 'OUT_DATE'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_adm_duration = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where the difference between IN_DATE and OUT_DATE is less than or equal to 90 days\n",
    "ADM_filtered = ADM[(ADM['OUT_DATE'] - ADM['IN_DATE']).dt.days <= max_adm_duration]\n",
    "\n",
    "# Create a new column 'Date' and extend the rows per ID for the filtered DataFrame\n",
    "ADM_filtered['Date'] = ADM_filtered.apply(lambda row: pd.date_range(row['IN_DATE'], row['OUT_DATE'], freq='D'), axis=1)\n",
    "\n",
    "# Explode the 'Date' column to have one row per date\n",
    "ADM_exploded = ADM_filtered.explode('Date')\n",
    "\n",
    "# Reset the index if needed\n",
    "ADM_exploded = ADM_exploded.reset_index(drop=True)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(ADM_exploded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM_exploded.rename(columns={'Date': 'date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM_filtered.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"ID\", \"TestDate\", \"Sex\", \"BSIClass\", \"Age\",'allbac','BSImulticlass']\n",
    "bacteremia_filtered = bacteremia[selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteremia_filtered.rename(columns={'TestDate': 'date'}, inplace=True)\n",
    "bacteremia_filtered['date'] = pd.to_datetime(bacteremia_filtered['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bacteremia_filtered[\"date\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi['date'] = pd.to_datetime(wide_bi['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate keys in bacteremia_filtered\n",
    "duplicate_keys = bacteremia_filtered[bacteremia_filtered.duplicated(subset=['ID', 'date'], keep=False)]\n",
    "print(duplicate_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate keys in wide_bi\n",
    "duplicate_keys = wide_bi[wide_bi.duplicated(subset=['ID', 'date'], keep=False)]\n",
    "print(duplicate_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_biokemi_bacteremia = pd.merge(wide_bi, bacteremia_filtered, on=['ID', 'date'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate keys in ADM_exploded\n",
    "duplicate_keys = ADM_exploded[ADM_exploded.duplicated(subset=['ID', 'date'], keep=False)]\n",
    "print(duplicate_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_biokemi_bacteremia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADM_exploded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate keys in merged_biokemi_bacteremia\n",
    "duplicate_keys_merged = merged_biokemi_bacteremia[merged_biokemi_bacteremia.duplicated(subset=['ID', 'date'], keep=False)]\n",
    "print(duplicate_keys_merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate keys in ADM_exploded\n",
    "ADM_exploded_no_duplicates = ADM_exploded.drop_duplicates(subset=['ID', 'date'])\n",
    "\n",
    "# Now, perform the left join again\n",
    "merged_df = pd.merge(merged_biokemi_bacteremia, ADM_exploded_no_duplicates, on=['ID', 'date'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN values in 'Column1'\n",
    "merged_df = merged_df.dropna(subset=['BSIClass'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_bi.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify columns you want to check for missing values\n",
    "columns_to_check = wide_bi.columns\n",
    "\n",
    "# Specify columns to exclude from the check\n",
    "columns_to_exclude = ['ID', 'date']\n",
    "\n",
    "# Remove excluded columns from the check\n",
    "columns_to_check = [col for col in columns_to_check if col not in columns_to_exclude]\n",
    "\n",
    "# Find rows where all specified columns have missing values\n",
    "missing_rows = merged_df[merged_df[columns_to_check].isna().all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the identified rows from the DataFrame\n",
    "merged_df = merged_df.drop(missing_rows.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the ratios\n",
    "merged_df['NEUTRO_to_LYMFO'] = np.where(merged_df['LYMFO'] != 0, merged_df['NEUTRO'] / merged_df['LYMFO'], np.nan)\n",
    "merged_df['Platelet-to-lymphocyte'] = np.where(merged_df['LYMFO'] != 0, merged_df['THROM'] / merged_df['LYMFO'], np.nan)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_exclude = ['ID', 'IN_DATE',\"OUT_DATE\",\"date\",\"Sex\",\"BSIClass\",\"Age\",\"allbac\"]\n",
    "\n",
    "# Check for NaN values in all columns except those to exclude\n",
    "merged_df_mask = merged_df.drop(columns=columns_to_exclude).isna().all(axis=1)\n",
    "\n",
    "# Count the number of rows that meet the condition\n",
    "count_rows_with_all_nan_except_some = merged_df_mask.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of rows with all NaN except for some columns: {count_rows_with_all_nan_except_some}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to exclude rows where all columns except for some are NaN\n",
    "merged_df = merged_df[~merged_df_mask]\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.drop([\"IN_DATE\",\"OUT_DATE\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for NaN values in the 'ID' column\n",
    "nan_ids = merged_df['BSIClass'].isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Extract features, labels, and groups\n",
    "X = merged_df.drop('BSIClass', axis=1)  # Features\n",
    "y = merged_df['BSIClass']  # Labels\n",
    "groups = merged_df['ID']  # Groups\n",
    "\n",
    "# Initialize GroupKFold with 5 splits\n",
    "group_kfold = GroupKFold(n_splits=5)\n",
    "\n",
    "# Split the data\n",
    "for train_index, test_index in group_kfold.split(X, y, groups):\n",
    "    train_set = merged_df.iloc[train_index]\n",
    "    test_set = merged_df.iloc[test_index]\n",
    "\n",
    "    # Display the result for each fold\n",
    "    print(\"Training set:\")\n",
    "    print(train_set)\n",
    "    print(\"\\nTest set:\")\n",
    "    print(test_set)\n",
    "    print(\"\\n\" + \"=\"*40)  # Separating folds with a line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each class in the training set\n",
    "train_class_counts = train_set['BSIClass'].value_counts()\n",
    "\n",
    "# Count the occurrences of each class in the test set\n",
    "test_class_counts = test_set['BSIClass'].value_counts()\n",
    "\n",
    "# Display the proportions\n",
    "print(\"Proportion of 'pos' to 'neg' in the training set:\")\n",
    "print(\"pos:\", train_class_counts.get('BSI', 0) / len(train_set))\n",
    "print(\"neg:\", train_class_counts.get('noBSI', 0) / len(train_set))\n",
    "\n",
    "print(\"\\nProportion of 'pos' to 'neg' in the test set:\")\n",
    "print(\"pos:\", test_class_counts.get('BSI', 0) / len(test_set))\n",
    "print(\"neg:\", test_class_counts.get('noBSI', 0) / len(test_set))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for common IDs between the training and test sets\n",
    "common_ids = set(train_set['ID']).intersection(test_set['ID'])\n",
    "\n",
    "# Display the result\n",
    "if len(common_ids) == 0:\n",
    "    print(\"No common IDs between the training and test sets.\")\n",
    "else:\n",
    "    print(\"Common IDs between the training and test sets:\", common_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(common_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_params = f'ADMper_{max_adm_duration}_multiclassBSI'\n",
    "\n",
    "# Save the dataframes with the parameterized names\n",
    "merged_df.to_csv(f'merged_df_{name_params}.csv', index=False)\n",
    "train_set.to_csv(f'train_set_{name_params}.csv', index=False)\n",
    "test_set.to_csv(f'test_set_{name_params}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
