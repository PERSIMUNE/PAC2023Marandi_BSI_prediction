{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7d8031c",
   "metadata": {},
   "source": [
    "# Project: prediction of BSI using machine learning based on biochemical variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa3646a",
   "metadata": {},
   "source": [
    "## Load data and libraries and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb588ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    "from sklearn import preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import ensemble\n",
    "import seaborn as sns\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Constants\n",
    "SEED = 123\n",
    "FOLDS = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# processing\n",
    "n_cpu_for_tuning = 20\n",
    "n_cpu_model_training = 40\n",
    "n_iter = 3 # repeated cross validation for hyperparameter tunning\n",
    "scale_data = False\n",
    "data_complete = False # filter out incomplete samples (more than 50% of biochemical variables missing) - optional\n",
    "from sklearn.metrics import make_scorer, matthews_corrcoef, balanced_accuracy_score, f1_score\n",
    "tun_score = \"roc_auc\" # f1_score \n",
    "verbosity_option = -1 # no training details required to show\n",
    "filter_highly_mis_feats = True # remove highly missing features (optional)\n",
    "feat_sel = True # optional\n",
    "\n",
    "# optimization\n",
    "no_biochemical_variables = False\n",
    "only_biochemical_variables = False\n",
    "use_default_threshold = True\n",
    "outcome_var = \"BSIClass\"\n",
    "# lookback_period = 30 # optional\n",
    "# gp_value = 0 # optional\n",
    "\n",
    "ADMper= 7\n",
    "devset_name = f'R/train_set_ADMper_{ADMper}'\n",
    "testset_name = f'R/test_set_ADMper_{ADMper}'\n",
    "\n",
    "devset = pd.read_csv(f'{devset_name}.csv')\n",
    "testset = pd.read_csv(f'{testset_name}.csv')\n",
    "\n",
    "main_folder_name = 'results'\n",
    "subfolder_name = f'{devset_name}'\n",
    "\n",
    "# Check if the main folder exists, and create it if not\n",
    "if not os.path.exists(main_folder_name):\n",
    "    os.makedirs(main_folder_name)\n",
    "\n",
    "# Change the current working directory to the main folder\n",
    "os.chdir(main_folder_name)\n",
    "\n",
    "# Check if the subfolder exists, and create it if not\n",
    "if not os.path.exists(subfolder_name):\n",
    "    os.makedirs(subfolder_name)\n",
    "\n",
    "# Change the current working directory to the subfolder\n",
    "os.chdir(subfolder_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092dbe3-b850-4bda-9e4b-9f9c541762c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "category_counts = devset[\"Sex\"].value_counts()\n",
    "total_count = len(devset[\"Sex\"])\n",
    "\n",
    "# Number and percent of each category\n",
    "result = pd.DataFrame({\n",
    "    'Category': category_counts.index,\n",
    "    'Count': category_counts.values,\n",
    "    'Percent': (category_counts / total_count) * 100\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a2036-f8f6-4d7c-88ba-4889501f1981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "category_counts = testset[\"Sex\"].value_counts()\n",
    "total_count = len(testset[\"Sex\"])\n",
    "\n",
    "# Number and percent of each category\n",
    "result = pd.DataFrame({\n",
    "    'Category': category_counts.index,\n",
    "    'Count': category_counts.values,\n",
    "    'Percent': (category_counts / total_count) * 100\n",
    "})\n",
    "\n",
    "# Display the result\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e5dfa9-f065-4dee-82d6-01ba4b1874a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_complete:\n",
    "    df = devset.copy()\n",
    "\n",
    "    # List of columns to exclude from the threshold check\n",
    "    exclude_columns = [\"ID\",\"date\",\"Sex\",\"BSIClass\",\"Age\",\"allbac\"]\n",
    "    \n",
    "    # Calculate the threshold for the remaining columns\n",
    "    included_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "    threshold = int(0.5 * len(included_columns))\n",
    "    \n",
    "    # Remove rows with more than 50% missing values, considering only the included columns\n",
    "    df_cleaned = df.dropna(subset=included_columns, thresh=threshold)\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(df_cleaned.shape)\n",
    "    print(len(df_cleaned[\"ID\"].unique()))\n",
    "    del devset\n",
    "    devset = df_cleaned.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50e7afa-c033-449c-b76f-08149e8995ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "if data_complete:\n",
    "    df = testset.copy()\n",
    "    \n",
    "    # List of columns to exclude from the threshold check\n",
    "    exclude_columns = [\"ID\",\"date\",\"Sex\",\"BSIClass\",\"Age\",\"allbac\"]\n",
    "    \n",
    "    # Calculate the threshold for the remaining columns\n",
    "    included_columns = [col for col in df.columns if col not in exclude_columns]\n",
    "    threshold = int(0.5 * len(included_columns))\n",
    "    \n",
    "    # Remove rows with more than 50% missing values, considering only the included columns\n",
    "    df_cleaned = df.dropna(subset=included_columns, thresh=threshold)\n",
    "    \n",
    "    print(df.shape)\n",
    "    print(df_cleaned.shape)\n",
    "    print(len(df_cleaned[\"ID\"].unique()))\n",
    "    del testset\n",
    "    testset = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada319df-459d-4859-824b-9d40a81b8610",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset = devset.reset_index(drop=True)\n",
    "testset = testset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e974b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(devset[\"ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83c3e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testset[\"ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5a43c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef16f23-1087-4c3f-98fa-a65b313b1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save back up data\n",
    "devset_backup = devset.copy()\n",
    "testset_backup = testset.copy()\n",
    "\n",
    "# Drop columns from the original datasets\n",
    "devset = devset.drop(columns=['ID', 'date', 'allbac'])\n",
    "testset = testset.drop(columns=['ID', 'date', 'allbac'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171d615e",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_withmissing = devset.copy()\n",
    "testset_withmissing = testset.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d3f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if no_biochemical_variables:\n",
    "    non_biochemical_cols_to_use = ['Sex', 'Age', outcome_var]\n",
    "    devset_withmissing = devset_withmissing[non_biochemical_cols_to_use]\n",
    "    testset_withmissing = testset_withmissing[non_biochemical_cols_to_use]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76195721-6146-4aec-8b2e-b6484e54888e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if only_biochemical_variables:\n",
    "    non_biochemical_cols_to_use = ['Sex', 'Age']\n",
    "    devset_withmissing = devset_withmissing.drop(columns=non_biochemical_cols_to_use)\n",
    "    testset_withmissing = testset_withmissing.drop(columns=non_biochemical_cols_to_use)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21c26c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_withmissing.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4409b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_withmissing.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fde457",
   "metadata": {},
   "outputs": [],
   "source": [
    "if only_biochemical_variables==False:\n",
    "    common_sex_category = 'Male'\n",
    "\n",
    "    devset_withmissing['Sex'] = pd.Categorical(devset_withmissing['Sex'], categories=[common_sex_category])\n",
    "    devset_withmissing = pd.get_dummies(devset_withmissing, columns=['Sex'], prefix='Sex')\n",
    "\n",
    "    testset_withmissing['Sex'] = pd.Categorical(testset_withmissing['Sex'], categories=[common_sex_category])\n",
    "    testset_withmissing = pd.get_dummies(testset_withmissing, columns=['Sex'], prefix='Sex')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc05d2f-c7e9-4d0e-ae40-27c50005f1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata = pd.concat([devset_withmissing, testset_withmissing])\n",
    "mydata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b745f76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e8e395",
   "metadata": {},
   "source": [
    "##### limit the number of lines etc. to display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dfd21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 50)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.width', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba679ceb",
   "metadata": {},
   "source": [
    "### data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164248a",
   "metadata": {},
   "source": [
    "#### display the type of the variables (columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a9a96",
   "metadata": {},
   "source": [
    "#### filter out the data based on observation period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20438d7-078d-467e-9553-88ec9b959dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bc43e1",
   "metadata": {},
   "source": [
    "#### check what columns have missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f7e9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with missing values\n",
    "columns_with_missing_values = mydata.columns[mydata.isnull().any()]\n",
    "columns_with_missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d8b145-a3b4-414a-baf7-b500c566e613",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1150d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdafd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a05ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51720c8d-a600-4318-8297-65f5204e6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653d277e",
   "metadata": {},
   "source": [
    "#### check for missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9eb487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# Step 1: Calculate the total number of missing values for each column\n",
    "missing_values = mydata.isnull().sum()\n",
    "\n",
    "# Step 2: Divide by the total number of rows\n",
    "total_rows = len(mydata)\n",
    "missing_percentage = (missing_values / total_rows) * 100\n",
    "\n",
    "# Step 3: Round the percentages to two decimal points\n",
    "missing_percentage = missing_percentage.round(2)\n",
    "\n",
    "# Step 4: Sort the percentages in ascending order\n",
    "missing_percentage = missing_percentage.sort_values(ascending=False)\n",
    "\n",
    "# Step 5: Calculate the mean and standard deviation of the missingness\n",
    "mean_missingness = np.mean(missing_percentage)\n",
    "std_missingness = np.std(missing_percentage)\n",
    "\n",
    "# Step 6: Display the missing percentages, mean, and standard deviation\n",
    "print(\"Missing Value Percentages:\")\n",
    "print(missing_percentage)\n",
    "print(\"Mean ± Standard Deviation of Missingness: {:.2f} ± {:.2f}\".format(mean_missingness, std_missingness))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293cbcc3-6849-46fc-92a4-20312184fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_withmissing.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831f3bfa-0bff-40e5-912a-0907f6635bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pointbiserialr\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "df_imputed = devset_withmissing.copy()\n",
    "\n",
    "# Convert 'outcome_var' to numerical variable\n",
    "df_imputed[outcome_var] = pd.factorize(df_imputed[outcome_var])[0]\n",
    "\n",
    "# Calculate point-biserial correlation for each feature multiple times\n",
    "num_iterations = 1000  # You can adjust the number of iterations as needed\n",
    "biserial_corr_values = []\n",
    "\n",
    "def calculate_biserial_corr(iteration, df, outcome_var):\n",
    "    # Impute NaNs with random values within each iteration\n",
    "    df_copy = df.copy()\n",
    "    for col in df_copy.columns:\n",
    "        col_missing = df_copy[col].isnull()\n",
    "        if col_missing.sum() > 0:\n",
    "            random_values = np.random.choice(df_copy.loc[~col_missing, col], size=col_missing.sum())\n",
    "            df_copy.loc[col_missing, col] = random_values\n",
    "    \n",
    "    biserial_corr_iter = [pointbiserialr(df_copy[col], df_copy[outcome_var])[0] for col in df_copy.drop(outcome_var, axis=1)]\n",
    "    return biserial_corr_iter\n",
    "\n",
    "# Calculate point-biserial correlation for each feature multiple times\n",
    "biserial_corr_values = Parallel(n_jobs=50)(delayed(calculate_biserial_corr)(i, df_imputed, outcome_var) for i in range(num_iterations))\n",
    "\n",
    "# Calculate the lower and upper quantiles of point-biserial correlation for each feature\n",
    "lower_quantile_corr = np.percentile(biserial_corr_values, 25, axis=0)\n",
    "median_corr = np.percentile(biserial_corr_values, 50, axis=0)\n",
    "upper_quantile_corr = np.percentile(biserial_corr_values, 75, axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names, lower and upper quantiles of point-biserial correlation\n",
    "corr_summary_df = pd.DataFrame({'Feature': df_imputed.drop(outcome_var, axis=1).columns,\n",
    "                                'median_Corr': median_corr,\n",
    "                                'Lower_Quantile_Corr': lower_quantile_corr,\n",
    "                                'Upper_Quantile_Corr': upper_quantile_corr})\n",
    "corr_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98942ea3-1dd9-40a5-860e-abb003463db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f063fe25-f053-48e4-bbd1-9d49acc9547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import pointbiserialr, wilcoxon\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "df_imputed = devset_withmissing.copy()\n",
    "\n",
    "# Convert 'outcome_var' to numerical variable\n",
    "df_imputed[outcome_var] = pd.factorize(df_imputed[outcome_var])[0]\n",
    "\n",
    "# Calculate point-biserial correlation for each feature multiple times\n",
    "num_iterations = 1000  # You can adjust the number of iterations as needed\n",
    "biserial_corr_values = []\n",
    "\n",
    "def calculate_biserial_corr(iteration, df, outcome_var):\n",
    "    df_copy = df.copy()\n",
    "    for col in df_copy.columns:\n",
    "        col_missing = df_copy[col].isnull()\n",
    "        if col_missing.sum() > 0:\n",
    "            random_values = np.random.choice(df_copy.loc[~col_missing, col], size=col_missing.sum())\n",
    "            df_copy.loc[col_missing, col] = random_values\n",
    "    \n",
    "    biserial_corr_iter = [pointbiserialr(df_copy[col], df_copy[outcome_var])[0] for col in df_copy.drop(outcome_var, axis=1)]\n",
    "    return biserial_corr_iter\n",
    "\n",
    "# Calculate point-biserial correlation for each feature multiple times\n",
    "biserial_corr_values = Parallel(n_jobs=50)(delayed(calculate_biserial_corr)(i, df_imputed, outcome_var) for i in range(num_iterations))\n",
    "\n",
    "# Create a DataFrame with feature names and correlation coefficients from all iterations\n",
    "corr_df = pd.DataFrame(biserial_corr_values, columns=df_imputed.drop(outcome_var, axis=1).columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2ff1b-462b-4370-a5c7-2dccfa4a576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pointbiserialr, norm, iqr\n",
    "# Calculate the lower and upper quantiles of point-biserial correlation for each feature\n",
    "lower_quantile_corr = np.percentile(corr_df, 25, axis=0)\n",
    "median_corr = np.percentile(corr_df, 50, axis=0)\n",
    "upper_quantile_corr = np.percentile(corr_df, 75, axis=0)\n",
    "\n",
    "# Filter features based on quantiles\n",
    "significant_features = [feature for feature, lower, upper in zip(corr_df.columns, lower_quantile_corr, upper_quantile_corr) if lower > 0 or upper < 0]\n",
    "\n",
    "# Filter the original DataFrame to include only significant features\n",
    "df_imputed_filtered = df_imputed[significant_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9e9e6-4d1f-4866-affd-982a28685cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mark significant features with a different color\n",
    "corr_summary_df['Color'] = np.where(corr_summary_df['Feature'].isin(significant_features), 'significant', 'not significant')\n",
    "\n",
    "# Sort DataFrame by median correlation for better visualization\n",
    "corr_summary_df = corr_summary_df.sort_values(by='median_Corr', ascending=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Plot significant features in red and non-significant features in blue\n",
    "sns.pointplot(x='median_Corr', y='Feature', data=corr_summary_df, hue='Color', dodge=True, join=False)\n",
    "\n",
    "# Plot error bars for all features\n",
    "plt.errorbar(x=corr_summary_df['median_Corr'], y=corr_summary_df['Feature'],\n",
    "             xerr=[corr_summary_df['median_Corr'] - corr_summary_df['Lower_Quantile_Corr'], corr_summary_df['Upper_Quantile_Corr'] - corr_summary_df['median_Corr']],\n",
    "             fmt='o', color='black', capsize=5, label='IQR')\n",
    "\n",
    "# Customize plot\n",
    "plt.title('median and quantile correlation coefficients for features')\n",
    "plt.xlabel('PB correlation values')\n",
    "plt.ylabel('Feature')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e91184-b1c5-4a27-8e59-90e7c2081178",
   "metadata": {},
   "outputs": [],
   "source": [
    "significant_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c69c47f-fa3d-4974-a768-b8811c43feee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# outcome_var = 'outcome_var'\n",
    "df_imputed = devset_withmissing.copy()\n",
    "\n",
    "# Add a random feature with missing values to df_imputed\n",
    "df_imputed['random_feature'] = np.random.choice([np.nan, np.random.rand()], size=len(df_imputed))\n",
    "\n",
    "# Calculate mutual information for each feature multiple times, including the random feature\n",
    "num_iterations = 1000  # You can adjust the number of iterations as needed\n",
    "mutual_info_values = []\n",
    "\n",
    "def calculate_mutual_info(iteration, df, outcome_var):\n",
    "    # Impute NaNs with random values within each iteration\n",
    "    df_copy = df.copy()\n",
    "    for col in df_copy.columns:\n",
    "        col_missing = df_copy[col].isnull()\n",
    "        if col_missing.sum() > 0:\n",
    "            random_values = np.random.choice(df_copy.loc[~col_missing, col], size=col_missing.sum())\n",
    "            df_copy.loc[col_missing, col] = random_values\n",
    "    \n",
    "    mutual_info_iter = mutual_info_classif(df_copy.drop(outcome_var, axis=1), df_copy[outcome_var])\n",
    "    return mutual_info_iter\n",
    "\n",
    "# Calculate mutual information for each feature multiple times, including the random feature\n",
    "mutual_info_values = Parallel(n_jobs=50)(delayed(calculate_mutual_info)(i, df_imputed, outcome_var) for i in range(num_iterations))\n",
    "\n",
    "# Calculate the lower and upper quantiles of mutual information for each feature\n",
    "lower_quantile_mi = np.percentile(mutual_info_values, 25, axis=0)\n",
    "median_mi = np.percentile(mutual_info_values, 50, axis=0)\n",
    "upper_quantile_mi = np.percentile(mutual_info_values, 75, axis=0)\n",
    "\n",
    "# Create a DataFrame with feature names, lower and upper quantiles of mutual information\n",
    "mi_summary_df = pd.DataFrame({'Feature': df_imputed.drop(outcome_var, axis=1).columns,'median_MI':median_mi, 'Lower_Quantile_MI': lower_quantile_mi, 'Upper_Quantile_MI': upper_quantile_mi})\n",
    "\n",
    "# Get median_random_feature_mi from the mutual_info_values\n",
    "median_random_feature_mi = np.median(mutual_info_values, axis=0)\n",
    "\n",
    "# Filter out \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da0bcc-da7b-4f55-9145-bf1b059aea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort mi_summary_df by Median_MI\n",
    "mi_summary_df_sorted = mi_summary_df.sort_values(by='median_MI', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a731b559-5757-4943-a9b7-a29ff6cf65fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_summary_df_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188bb0e1-74e4-46e2-9b97-cae18c1581d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = mi_summary_df_sorted.copy()\n",
    "\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Swap 'Feature' and 'median_MI' in the plot functions\n",
    "sns.pointplot(x='median_MI', y='Feature', data=df, color='blue', label='Median MI')\n",
    "\n",
    "# Swap 'Feature' and 'median_MI' in the errorbar function\n",
    "plt.errorbar(x=df['median_MI'], y=df['Feature'], xerr=[df['median_MI'] - df['Lower_Quantile_MI'], df['Upper_Quantile_MI'] - df['median_MI']],\n",
    "             fmt='o', color='blue', capsize=5, label='Quantile MI Range')\n",
    "\n",
    "# Set smaller font size\n",
    "sns.set(font_scale=0.6)\n",
    "# Customize plot\n",
    "plt.title('Median and Quantile MI for Features')\n",
    "plt.xlabel('MI Value')\n",
    "plt.ylabel('Feature')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba6cc9b-d79c-419f-9dab-c55b373c3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(devset_withmissing)\n",
    "df2 = pd.DataFrame(testset_withmissing)\n",
    "# Function to create the summary table for a single dataset\n",
    "def create_summary_table(dataframe, dataset_name):\n",
    "    summary_data = {'Variable': [], 'Value': []}\n",
    "\n",
    "    for col in sorted(dataframe.columns):  # Sort variable names alphabetically\n",
    "        # Variable Name\n",
    "        summary_data['Variable'].append(col)\n",
    "        summary_data['Value'].append('Variable Name')\n",
    "\n",
    "        # For numerical variables - Median (lower quantile, higher quantile)\n",
    "        if pd.api.types.is_numeric_dtype(dataframe[col]):\n",
    "            median = dataframe[col].median()\n",
    "            q25 = dataframe[col].quantile(0.25)\n",
    "            q75 = dataframe[col].quantile(0.75)\n",
    "            summary_data['Variable'].append('')\n",
    "            summary_data['Value'].append(f'{median:.2f} ({q25:.2f}, {q75:.2f})')\n",
    "\n",
    "        # For categorical variables - Categories, Counts, and Percentages\n",
    "        elif pd.api.types.is_categorical_dtype(dataframe[col]):\n",
    "            categories = dataframe[col].value_counts()\n",
    "            total_count = len(dataframe[col])\n",
    "            summary_data['Variable'].extend(['', ''])\n",
    "            summary_data['Value'].extend(['Categories', 'Counts'])\n",
    "            \n",
    "            for category, count in categories.items():\n",
    "                percentage = (count / total_count) * 100\n",
    "                summary_data['Variable'].append(category)\n",
    "                summary_data['Value'].append(f'{count} - {percentage:.2f}%')\n",
    "\n",
    "        # Missing values for all variable types\n",
    "        missing_count = dataframe[col].isnull().sum()\n",
    "        missing_percentage = (missing_count / len(dataframe)) * 100\n",
    "        summary_data['Variable'].append('')\n",
    "        summary_data['Value'].append(f' {missing_percentage:.2f}%')\n",
    "\n",
    "    # Add a column for the dataset name\n",
    "    summary_data['Variable'].append('Dataset')\n",
    "    summary_data['Value'].append(dataset_name)\n",
    "\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    return summary_df\n",
    "\n",
    "# Create summary tables for each dataset\n",
    "summary_table1 = create_summary_table(df1, 'dataset1')\n",
    "summary_table2 = create_summary_table(df2, 'dataset2')\n",
    "\n",
    "# Concatenate summary tables horizontally\n",
    "final_summary_table = pd.concat([summary_table1.set_index('Variable'), summary_table2.set_index('Variable')], axis=1).reset_index()\n",
    "\n",
    "# Display the final summary table\n",
    "print(final_summary_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488890f-58ed-4567-9ad5-337ede64f460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final summary table to an Excel file\n",
    "final_summary_table.to_excel('summary_table.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d4e960-9f40-49d5-b18e-5284cffd3776",
   "metadata": {},
   "outputs": [],
   "source": [
    "if feat_sel: # feature selection\n",
    "    devset_withmissing = devset_withmissing[significant_features + [outcome_var]]\n",
    "    testset_withmissing = testset_withmissing[significant_features + [outcome_var]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700e9eb0-9f3d-4205-8575-6021d9292739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the numerical features you want to scale\n",
    "numerical_columns = devset_withmissing.select_dtypes(include=['float64', 'int64']).columns\n",
    "has_negative_values = np.any(devset_withmissing[numerical_columns] < 0)\n",
    "\n",
    "if has_negative_values:\n",
    "    print(\"There are negative values in devset_withmissing.\")\n",
    "else:\n",
    "    print(\"There are no negative values in devset_withmissing.\")\n",
    "\n",
    "# Display the minimum value of each column\n",
    "min_values = devset_withmissing[numerical_columns].min()\n",
    "print(\"\\nMinimum values of each column:\")\n",
    "print(min_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24dfcc5-3334-4961-a467-c34accf8ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# Step 1: Calculate the total number of missing values for each column\n",
    "missing_values = devset_withmissing.isnull().sum()\n",
    "\n",
    "# Step 2: Divide by the total number of rows\n",
    "total_rows = len(devset_withmissing)\n",
    "missing_percentage = (missing_values / total_rows) * 100\n",
    "\n",
    "# Step 3: Round the percentages to two decimal points\n",
    "missing_percentage = missing_percentage.round(2)\n",
    "\n",
    "# Step 4: Sort the percentages in ascending order\n",
    "missing_percentage = missing_percentage.sort_values(ascending=False)\n",
    "\n",
    "# Step 5: Calculate the mean and standard deviation of the missingness\n",
    "mean_missingness = np.mean(missing_percentage)\n",
    "std_missingness = np.std(missing_percentage)\n",
    "\n",
    "# Step 6: Display the missing percentages, mean, and standard deviation\n",
    "print(\"Missing Value Percentages:\")\n",
    "print(missing_percentage)\n",
    "print(\"Mean ± Standard Deviation of Missingness: {:.2f} ± {:.2f}\".format(mean_missingness, std_missingness))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b8c25e-37dc-45fa-a126-608b8d4af327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Identify columns with more than 90% missingness\n",
    "highly_missing_features = missing_percentage[missing_percentage > 90].index.tolist()\n",
    "\n",
    "# Step 8: Print the highly missing features\n",
    "print(\"Columns with more than 90% missingness:\")\n",
    "print(highly_missing_features)\n",
    "\n",
    "if filter_highly_mis_feats:\n",
    "    # Step 9: Drop columns with more than 90% missingness from devset_withmissing\n",
    "    devset_withmissing = devset_withmissing.drop(columns=highly_missing_features)\n",
    "    \n",
    "    # Step 10: Drop columns with more than 90% missingness from testset_withmissing\n",
    "    testset_withmissing = testset_withmissing.drop(columns=highly_missing_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f47868-a40d-44a0-aa66-9ddccf053d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler # optional\n",
    "if scale_data:\n",
    "    # Specify the numerical features you want to scale\n",
    "    numerical_columns = devset_withmissing.select_dtypes(include=['float64', 'int64']).columns\n",
    "    \n",
    "    robust_scaler = RobustScaler().fit(devset_withmissing[numerical_columns])\n",
    "    \n",
    "    # Use the RobustScaler to scale the numerical features\n",
    "    devset_withmissing[numerical_columns] = robust_scaler.fit_transform(devset_withmissing[numerical_columns])\n",
    "    testset_withmissing[numerical_columns] = robust_scaler.fit_transform(testset_withmissing[numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff630d4b-18dd-423c-a7b0-cef748b20a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e01c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Separate features and target variable for the training set\n",
    "X_train_withmissing = devset_withmissing.drop(outcome_var, axis=1)\n",
    "y_train = devset_withmissing[outcome_var]\n",
    "\n",
    "# Separate features and target variable for the test set\n",
    "X_test_withmissing = testset_withmissing.drop(outcome_var, axis=1)\n",
    "y_test = testset_withmissing[outcome_var]\n",
    "\n",
    "min_values = X_train_withmissing.min()\n",
    "max_values = X_train_withmissing.max()\n",
    "\n",
    "if scale_data:\n",
    "    mice_imputer = IterativeImputer(\n",
    "        max_iter=10, \n",
    "        random_state=SEED, tol=1e-4\n",
    "    )\n",
    "else:\n",
    "    mice_imputer = IterativeImputer(max_iter=10,  random_state=SEED, min_value=min_values, max_value=max_values, tol=1e-4)\n",
    "# Fit and transform X_train_withmissing with MICE imputation\n",
    "X_train_imputed = mice_imputer.fit_transform(X_train_withmissing)\n",
    "\n",
    "# Convert the imputed array back to a DataFrame with column names\n",
    "X_train_imputed = pd.DataFrame(X_train_imputed, columns=X_train_withmissing.columns)\n",
    "\n",
    "# Combine X_train_imputed and y_train into a single DataFrame\n",
    "devset_imputed = pd.concat([X_train_imputed, y_train], axis=1)\n",
    "\n",
    "# Transform X_test_withmissing using the same MICE imputer\n",
    "X_test_imputed = mice_imputer.transform(X_test_withmissing)\n",
    "\n",
    "# Convert the imputed array back to a DataFrame with column names\n",
    "X_test_imputed = pd.DataFrame(X_test_imputed, columns=X_test_withmissing.columns)\n",
    "\n",
    "# Combine X_test_imputed and y_test into a single DataFrame\n",
    "testset_imputed = pd.concat([X_test_imputed, y_test], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b60fcb",
   "metadata": {},
   "source": [
    "#### check the number of rows and columns of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9e8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdc5d53",
   "metadata": {},
   "source": [
    "#### Use data dictionary to have a description of the variables in results\n",
    "\n",
    "using the data dictionary the name of the variables are displayed in figures and tables in publishable form as in a research article for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = {'Sodium': 'Sodium', 'ALAT': 'Alanine Aminotransferase', 'ALB': 'Albumin', 'APTT': 'Activated Partial Thromboplastin Time',\n",
    "       'BASO': 'Basophils', 'CA': 'Calcium', 'CAI': 'Calcium Ionized', 'CHOL': 'Cholesterol', 'CL': 'Chloride',\n",
    "       'CREA': 'Creatinine', 'CRP': 'C-Reactive Protein', 'EOS': 'Eosinophils', 'ERY': 'Erythrocytes',\n",
    "       'FERRIT': 'Ferritin', 'GGT': 'Gamma-glutamyltransferase', 'GLU': 'Glucose', 'GLUF': 'Glycated Hemoglobin',\n",
    "       'HAPTO': 'Haptoglobin', 'HB': 'Hemoglobin', 'HBA1C': 'Hemoglobin A1c', 'HDL': 'High-density lipoprotein',\n",
    "       'INR': 'International Normalized Ratio', 'JERN': 'Iron', 'K': 'Potassium', 'LDH': 'Lactate dehydrogenase',\n",
    "       'LDL': 'Low-density lipoprotein', 'LEU': 'Leukocytes', 'LYMFO': 'Lymphocytes', 'MG': 'Magnesium',\n",
    "       'MONO': 'Monocytes', 'NEUTRO': 'Neutrophils', 'PROCAL': 'Procalcitonin', 'THROM': 'Platelets',\n",
    "       'TRANS': 'Transferrin', 'TRIG': 'Triglycerides', 'TSH': 'Thyroid-stimulating hormone',\n",
    "       'Sex': 'Sex', 'Age': 'Age', 'BSIClass': 'BSI Class', 'PrevAdmissionRate': 'Previous Medical Encounter Rate',\n",
    "       'PrevInfectionRate': 'Previous Infection Rate', 'biochemical_abnormality_score': 'Biochemical Abnormality Score',\n",
    "       'biochemical_abnormality_score_NAexcluded': 'Adjusted Biochemical Abnormality Score',\n",
    "       'modified_biochemical_abnormality_score': 'Minimal Biochemical Abnormality Score', 'Sodium_base': 'Sodium (Baseline)',\n",
    "       'ALAT_base': 'Alanine Aminotransferase (Baseline)', 'ALB_base': 'Albumin (Baseline)', 'APTT_base': 'Activated Partial Thromboplastin Time (Baseline)',\n",
    "       'BASO_base': 'Basophils (Baseline)', 'CA_base': 'Calcium (Baseline)', 'CAI_base': 'Calcium Ionized (Baseline)',\n",
    "       'CHOL_base': 'Cholesterol (Baseline)', 'CL_base': 'Chloride (Baseline)', 'CREA_base': 'Creatinine (Baseline)',\n",
    "       'CRP_base': 'C-Reactive Protein (Baseline)', 'EOS_base': 'Eosinophils (Baseline)', 'ERY_base': 'Erythrocytes (Baseline)',\n",
    "       'FERRIT_base': 'Ferritin (Baseline)', 'GGT_base': 'Gamma-glutamyltransferase (Baseline)', 'GLU_base': 'Glucose (Baseline)',\n",
    "       'GLUF_base': 'Glycated Hemoglobin (Baseline)', 'HAPTO_base': 'Haptoglobin (Baseline)', 'HB_base': 'Hemoglobin (Baseline)',\n",
    "       'HBA1C_base': 'Hemoglobin A1c (Baseline)', 'HDL_base': 'High-density lipoprotein (Baseline)','INR_base': 'International Normalized Ratio (Baseline)',\n",
    "       'JERN_base': 'Iron (Baseline)', 'K_base': 'Potassium (Baseline)', 'LDH_base': 'Lactate dehydrogenase (Baseline)',\n",
    "       'LDL_base': 'Low-density lipoprotein (Baseline)', 'LEU_base': 'Leukocytes (Baseline)', 'LYMFO_base': 'Lymphocytes (Baseline)',\n",
    "       'MG_base': 'Magnesium (Baseline)', 'MONO_base': 'Monocytes (Baseline)', 'NEUTRO_base': 'Neutrophils (Baseline)',\n",
    "       'PROCAL_base': 'Procalcitonin (Baseline)', 'THROM_base': 'Platelets (Baseline)', 'TRANS_base': 'Transferrin (Baseline)',\n",
    "       'TRIG_base': 'Triglycerides (Baseline)', 'TSH_base': 'Thyroid-stimulating hormone (Baseline)',\n",
    "       \"NEUTRO_to_LYMFO\": \"Neutrophils-to-lymphocytes ratio\",\n",
    "       \"Platelet-to-lymphocyte\": \"Platelet-to-lymphocyte ratio\",\n",
    "}\n",
    "       # \"PBCR\": \"Extended positive blood culture rate\",\n",
    "       # \"CER\": \"Clinical encounter rate\",\n",
    "       # \"BVA\": \"Biochemical variable abnormality\",\n",
    "       # \"NBVA\": \"Non-missing biochemical variable abnormality\",\n",
    "       # \"SBVA\": \"Subset biochemical variable abnormality\",\n",
    "       # \"modifiedPIR\": \"Positive blood culture rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce35a40f",
   "metadata": {},
   "source": [
    "#### check the outcome variable and its categories (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9864a73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85449e89",
   "metadata": {},
   "source": [
    "#### data visualization\n",
    "\n",
    "an overview of the distribution of the variables (continuous and categorical) is provided to compare the distributions per class (outcome category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fb2646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "mydata[outcome_var] = mydata[outcome_var].map({\"noBSI\": 'negative', \"BSI\": 'positive'}).astype('category')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a5c0ce-0e05-4b1f-8552-7fc652c92675",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mydata = pd.concat([devset_imputed, testset_imputed])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a2039b-cb1b-4a3d-9109-c15ec86da5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519a2272-7370-45ee-bc9c-c9d09fe00c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_imputed[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdce2583-f386-4f83-b7b7-3d09915f7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_withmissing[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c21358-817c-4234-9b59-93abb4ad1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset_withmissing[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabde461-8ee1-4bd8-804e-6e4adb406c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbfcbdb-ca54-4996-ae0b-de4d65c9f3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd57f18f-5a74-42f6-9780-aa5d201a66fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0b661c-9daa-4dcd-8a00-31850c8b9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edb6f54-dcec-47c4-8ff1-d4d585c108c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mydata[outcome_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41a079-8208-4938-b7c0-1a9c46ad6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "continuous_vars = mydata.select_dtypes(include=['float64', 'int64'])\n",
    "categorical_vars = mydata.select_dtypes(include=['category'])\n",
    "outcome_variable = mydata[outcome_var].copy()\n",
    "\n",
    "# Calculate the number of rows and columns for subplots\n",
    "num_continuous_vars = len(continuous_vars.columns)\n",
    "num_categorical_vars = len(categorical_vars.columns)\n",
    "num_cols_to_plot = 3\n",
    "num_rows = (num_continuous_vars + num_categorical_vars + num_cols_to_plot - 1) // num_cols_to_plot + 1  # Adjust the number of rows based on the number of variables\n",
    "\n",
    "# Create subplots for continuous variables\n",
    "fig, axes = plt.subplots(num_rows, num_cols_to_plot, figsize=(12, num_rows * 2))  # Adjust the figsize as desired\n",
    "\n",
    "# Iterate over continuous variables\n",
    "for i, column in enumerate(continuous_vars.columns):\n",
    "    # Determine the subplot indices\n",
    "    row_idx = i // num_cols_to_plot\n",
    "    col_idx = i % num_cols_to_plot\n",
    "\n",
    "    # Check if subplot index is within the bounds of axes\n",
    "    if row_idx < num_rows:\n",
    "        # Get the axis for the current subplot\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Iterate over each outcome category\n",
    "        for outcome_category, ax_offset in zip(outcome_variable.unique(), [-0.2, 0.2]):\n",
    "            # Filter the data for the current outcome category\n",
    "            filtered_data = continuous_vars[outcome_variable == outcome_category][column]\n",
    "\n",
    "            # Create a box plot for the current outcome category\n",
    "            positions = np.array([1 + ax_offset])\n",
    "            ax.boxplot(filtered_data.dropna(), positions=positions, widths=0.3, vert=False)  # Vert=False for horizontal box plots\n",
    "\n",
    "        ax.set_title(f'{column}', fontsize=8)\n",
    "        ax.set_yticks([1 - ax_offset, 1 + ax_offset])\n",
    "        ax.set_yticklabels(outcome_variable.unique(), fontsize=8)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "        ax.legend(fontsize=6)\n",
    "\n",
    "# Iterate over categorical variables\n",
    "for i, column in enumerate(categorical_vars.columns):\n",
    "    # Determine the subplot indices\n",
    "    row_idx = (i + num_continuous_vars) // num_cols_to_plot\n",
    "    col_idx = (i + num_continuous_vars) % num_cols_to_plot\n",
    "\n",
    "    # Check if subplot index is within the bounds of axes\n",
    "    if row_idx < num_rows:\n",
    "        # Get the axis for the current subplot\n",
    "        ax = axes[row_idx, col_idx]\n",
    "\n",
    "        # Normalize the counts for the current categorical variable stratified by outcome variable\n",
    "        category_counts = categorical_vars.groupby(outcome_variable)[column].value_counts(normalize=True).unstack()\n",
    "        category_counts.plot(kind='barh', ax=ax)\n",
    "\n",
    "        # Set the title with the feature name\n",
    "        ax.set_title(f'{column}', fontsize=8)\n",
    "\n",
    "        ax.set_ylabel(None)\n",
    "        ax.tick_params(axis='both', labelsize=8)\n",
    "        ax.legend(fontsize=6)\n",
    "\n",
    "# Remove any empty subplots at the end\n",
    "if num_continuous_vars + num_categorical_vars < num_rows * num_cols_to_plot:\n",
    "    for i in range(num_continuous_vars + num_categorical_vars, num_rows * num_cols_to_plot):\n",
    "        fig.delaxes(axes.flatten()[i])\n",
    "\n",
    "# Remove the subplot for outcome_var at the end\n",
    "if num_continuous_vars + num_categorical_vars == num_rows * num_cols_to_plot - 1:\n",
    "    last_ax_index = num_continuous_vars + num_categorical_vars - 1\n",
    "    if last_ax_index >= 0:\n",
    "        fig.delaxes(axes.flatten()[last_ax_index])\n",
    "\n",
    "# Adjust the layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b36dbb",
   "metadata": {},
   "source": [
    "##### check out the distribution of the samples per class in the training (development) set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aacd09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of samples per class in devset\n",
    "devset_class_counts = devset_withmissing[outcome_var].value_counts()\n",
    "\n",
    "# Calculate the percentage of samples per class in devset\n",
    "devset_class_percentages = (devset_class_counts / len(devset_withmissing)) * 100\n",
    "\n",
    "# Count the number of samples per class in testset\n",
    "testset_class_counts = testset_withmissing[outcome_var].value_counts()\n",
    "\n",
    "# Calculate the percentage of samples per class in testset\n",
    "testset_class_percentages = (testset_class_counts / len(testset_withmissing)) * 100\n",
    "\n",
    "# Display the summary of the number of samples per class and their percentages\n",
    "print(\"Development Set:\")\n",
    "print(devset_class_counts)\n",
    "print(devset_class_percentages)\n",
    "\n",
    "print(\"\\nTest Set:\")\n",
    "print(testset_class_counts)\n",
    "print(testset_class_percentages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34b270-18d1-4a27-9793-abf9e0d0eeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [] # no categorical feature is declared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69290d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "devset_imputed.loc[:, outcome_var] = devset_imputed[outcome_var].replace({\"BSI\": True, \"noBSI\": False}).astype(bool)\n",
    "testset_imputed.loc[:, outcome_var] = testset_imputed[outcome_var].replace({\"BSI\": True, \"noBSI\": False}).astype(bool)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc63d1",
   "metadata": {},
   "source": [
    "##### function to evaluate models and generate ROC curve, PR curve and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0a05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, roc_auc_score, brier_score_loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import catboost as cb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Define a function for bootstrap sampling\n",
    "def bootstrap_sample(data, n_samples):\n",
    "    indices = np.random.randint(0, len(data), (n_samples, len(data)))\n",
    "    return data[indices]\n",
    "\n",
    "def calculate_confidence_interval(metric_values, alpha=0.95):\n",
    "    # Filter out NaN values from metric_values\n",
    "    non_nan_values = metric_values[~np.isnan(metric_values)]\n",
    "    \n",
    "    # Check if there are non-NaN values to calculate the confidence interval\n",
    "    if len(non_nan_values) == 0:\n",
    "        return np.nan, np.nan\n",
    "    \n",
    "    # Calculate confidence intervals for non-NaN values\n",
    "    lower_bound = np.percentile(non_nan_values, (1 - alpha) / 2 * 100)\n",
    "    upper_bound = np.percentile(non_nan_values, (1 + alpha) / 2 * 100)\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "def evaluate_and_plot_model(model, testset, y_test, class_labels, filename, threshold=0.5, bootstrap_samples=1000, min_positive_instances=1):\n",
    "\n",
    "\n",
    "    bootstrap_values = []\n",
    "\n",
    "    for _ in range(bootstrap_samples):\n",
    "        # Perform bootstrap sampling\n",
    "        bootstrap_sample_indices = np.random.choice(len(testset), len(testset), replace=True)\n",
    "        bootstrap_sample_testset = testset.iloc[bootstrap_sample_indices]\n",
    "        bootstrap_sample_y_test = y_test.iloc[bootstrap_sample_indices]\n",
    "\n",
    "        if isinstance(model, (cb.CatBoostClassifier, lgb.LGBMClassifier, GaussianNB,RandomForestClassifier, LogisticRegression, BalancedRandomForestClassifier, HistGradientBoostingClassifier)):\n",
    "            predictions = model.predict_proba(bootstrap_sample_testset)[:, 1]\n",
    "            # print(predictions)\n",
    "        else:\n",
    "            predictions = model.predict(bootstrap_sample_testset)\n",
    "\n",
    "        predictions_class = [True if x >= threshold else False for x in predictions]\n",
    "\n",
    "        # Check if the number of positive instances is below the threshold\n",
    "        if np.sum(bootstrap_sample_y_test) < min_positive_instances:\n",
    "            # Set metrics to NaN or another suitable value\n",
    "            PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, roc_auc, pr_auc, brier_score = [np.nan] * 9\n",
    "        else:\n",
    "            cm = confusion_matrix(bootstrap_sample_y_test, predictions_class)\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "            PPV = tp / (tp + fp)\n",
    "            NPV = tn / (tn + fn)\n",
    "            sensitivity = tp / (tp + fn)\n",
    "            specificity = tn / (tn + fp)\n",
    "            balanced_accuracy = (sensitivity + specificity) / 2\n",
    "            MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "            roc_auc = roc_auc_score(y_true=bootstrap_sample_y_test, y_score=predictions)\n",
    "            brier_score = brier_score_loss(y_true=bootstrap_sample_y_test, y_prob=predictions, pos_label=True)\n",
    "            precision, recall, _ = precision_recall_curve(y_true=bootstrap_sample_y_test, probas_pred=predictions, pos_label=True)\n",
    "            pr_auc = metrics.auc(x=recall, y=precision)\n",
    "\n",
    "        # Store the metric values for each bootstrap iteration\n",
    "        bootstrap_values.append([PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, roc_auc, pr_auc, brier_score])\n",
    "\n",
    "\n",
    "    # Convert the list of metric values into a numpy array for easier manipulation\n",
    "    bootstrap_values = np.array(bootstrap_values)\n",
    "\n",
    "    # Calculate confidence intervals for each metric\n",
    "    lower_bounds, upper_bounds = zip(*[calculate_confidence_interval(bootstrap_values[:, i]) for i in range(bootstrap_values.shape[1])])\n",
    "\n",
    "    # Calculate the measures for the whole testset\n",
    "    if np.sum(y_test) < min_positive_instances:\n",
    "        # Set metrics to NaN or another suitable value\n",
    "        PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, roc_auc, pr_auc, brier_score = [np.nan] * 9\n",
    "    else:\n",
    "        if isinstance(model, (cb.CatBoostClassifier, lgb.LGBMClassifier, GaussianNB,RandomForestClassifier, LogisticRegression,BalancedRandomForestClassifier, HistGradientBoostingClassifier)):\n",
    "            predictions = model.predict_proba(testset)[:, 1]\n",
    "            # print(predictions)\n",
    "        else:\n",
    "            predictions = model.predict(testset)\n",
    "\n",
    "        predictions_class = [True if x >= threshold else False for x in predictions]\n",
    "\n",
    "        cm = confusion_matrix(y_test, predictions_class)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        PPV = tp / (tp + fp)\n",
    "        NPV = tn / (tn + fn)\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "        balanced_accuracy = (sensitivity + specificity) / 2\n",
    "        MCC = (tp * tn - fp * fn) / ((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn)) ** 0.5\n",
    "        roc_auc = roc_auc_score(y_true=y_test, y_score=predictions)\n",
    "        brier_score = brier_score_loss(y_true=y_test, y_prob=predictions, pos_label=True)\n",
    "        precision, recall, _ = precision_recall_curve(y_true=y_test, probas_pred=predictions, pos_label=True)\n",
    "        pr_auc = metrics.auc(x=recall, y=precision)\n",
    "\n",
    "    # Convert the list of metric values into a numpy array for easier manipulation\n",
    "    bootstrap_values = np.array(bootstrap_values)\n",
    "\n",
    "    # Calculate confidence intervals for each metric\n",
    "    lower_bounds, upper_bounds = zip(*[calculate_confidence_interval(bootstrap_values[:, i]) for i in range(bootstrap_values.shape[1])])\n",
    "\n",
    "    # Calculate the measures for the whole testset\n",
    "    results = {\n",
    "        'Metric': ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'ROCAUC', 'PRAUC', 'Brier Score'],\n",
    "        'Value': [PPV, NPV, sensitivity, specificity, balanced_accuracy, MCC, roc_auc, pr_auc, brier_score],\n",
    "        'Lower Bound': lower_bounds,\n",
    "        'Upper Bound': upper_bounds\n",
    "    }\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df['Value'] = results_df['Value'].round(2)\n",
    "    results_df['Lower Bound'] = results_df['Lower Bound'].round(2)\n",
    "    results_df['Upper Bound'] = results_df['Upper Bound'].round(2)\n",
    "    print(results_df)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, predictions, pos_label=True, drop_intermediate=False)\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_test, predictions, pos_label=True)\n",
    "\n",
    "    # Finding the index closest to the custom threshold instead of 0.5\n",
    "    threshold_index = (np.abs(thresholds - threshold)).argmin()\n",
    "    threshold_custom = thresholds[threshold_index]\n",
    "    tpr_custom = tpr[threshold_index]\n",
    "    fpr_custom = fpr[threshold_index]\n",
    "\n",
    "    pr_threshold_index = (np.abs(pr_thresholds - threshold)).argmin()\n",
    "    pr_threshold_custom = pr_thresholds[pr_threshold_index]\n",
    "    precision_custom = precision[pr_threshold_index]\n",
    "    recall_custom = recall[pr_threshold_index]\n",
    "\n",
    "    def display_confusion_matrix(y_true, y_pred, labels):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot(cmap='Blues', ax=ax3, xticks_rotation='vertical', values_format='d')\n",
    "        ax3.set_xticklabels(ax3.get_xticklabels(), fontsize=8)\n",
    "        ax3.set_yticklabels(ax3.get_yticklabels(), fontsize=8)\n",
    "        plt.xlabel('Predicted', fontsize=8)\n",
    "        plt.ylabel('True', fontsize=8)\n",
    "        ax3.legend(fontsize=8)\n",
    "\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(15, 4.5))\n",
    "    \n",
    "\n",
    "    ax1.plot(fpr, tpr, color='blue', label='ROC AUC ≈ {:.2f}'.format(roc_auc))\n",
    "    ax1.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "    ax1.scatter(fpr_custom, tpr_custom, color='red', label=f'Threshold = {threshold_custom:.2f}', s=50, marker='x')\n",
    "    # ax1.scatter(fpr_0_5, tpr_0_5, color='red', label='Threshold = {:.2f}'.format(threshold_0_5), s=50, marker='x')\n",
    "    ax1.set_xlim([0, 1])\n",
    "    ax1.set_ylim([0, 1])\n",
    "    ax1.set_xlabel('False Positive Rate', fontsize=8)\n",
    "    ax1.set_ylabel('True Positive Rate', fontsize=8)\n",
    "    ax1.set_title('ROC curve', fontsize=8)\n",
    "    ax1.legend(loc=\"lower right\", fontsize=8)\n",
    "\n",
    "    chance_level_precision = np.sum(y_test) / len(y_test)\n",
    "    \n",
    "    ax2.plot(recall, precision, color='green', label='PR AUC ≈ {:.2f}'.format(pr_auc))\n",
    "    # ax2.scatter(recall_0_5, precision_0_5, color='orange', label='Threshold = {:.2f}'.format(pr_threshold_0_5), s=50, marker='x')\n",
    "    ax2.scatter(recall_custom, precision_custom, color='orange', label=f'Threshold = {pr_threshold_custom:.2f}', s=50, marker='x')\n",
    "    ax2.axhline(y=chance_level_precision, color='grey', linestyle='--', label='Chance Level')\n",
    "    ax2.set_xlim([0, 1])\n",
    "    ax2.set_ylim([0, 1])\n",
    "    ax2.set_xlabel('Recall', fontsize=8)\n",
    "    ax2.set_ylabel('Precision', fontsize=8)\n",
    "    ax2.set_title('Precision-Recall curve', fontsize=8)\n",
    "    ax2.legend(loc=\"upper right\", fontsize=8)\n",
    "\n",
    "    # ax2.legend(loc=\"lower left\", fontsize=8)\n",
    "\n",
    "    display_confusion_matrix(y_test, predictions_class, labels=class_labels)\n",
    "    ax3.set_title('Confusion matrix', fontsize=8)\n",
    "\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.size'] = 8\n",
    "\n",
    "    plt.savefig(filename, dpi=300)\n",
    "\n",
    "    print(f'Threshold closest to {threshold} (ROC): {threshold_custom:.2f}')\n",
    "    print(f'Threshold closest to {threshold} (PR): {pr_threshold_custom:.2f}')\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11fd14a",
   "metadata": {},
   "source": [
    "##### function to generate ROC and confusion matrix figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56fc46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Train each model on the training set.\n",
    "X_train = devset_imputed.drop(outcome_var, axis=1)\n",
    "y_train = devset_imputed[outcome_var]\n",
    "X_test = testset_imputed.drop(outcome_var, axis=1)\n",
    "y_test = testset_imputed[outcome_var]\n",
    "\n",
    "devset_withmissing.loc[:, outcome_var] = devset_withmissing[outcome_var].replace({\"positive\": True, \"negative\": False}).astype(bool)\n",
    "testset_withmissing.loc[:, outcome_var] = testset_withmissing[outcome_var].replace({\"positive\": True, \"negative\": False}).astype(bool)\n",
    "\n",
    "# Train each model on the training set.\n",
    "X_train_withmissing = devset_withmissing.drop(outcome_var, axis=1)\n",
    "X_test_withmissing = testset_withmissing.drop(outcome_var, axis=1)\n",
    "\n",
    "# Convert categories to str type\n",
    "X_train_withmissing_cat = X_train_withmissing.copy()\n",
    "X_test_withmissing_cat = X_test_withmissing.copy()\n",
    "X_train_withmissing_cat[cat_features] = X_train_withmissing_cat[cat_features].apply(lambda x: x.astype(str))\n",
    "X_test_withmissing_cat[cat_features] = X_test_withmissing_cat[cat_features].apply(lambda x: x.astype(str))\n",
    "\n",
    "# Combine X_train and X_test into a single DataFrame\n",
    "combined = pd.concat([X_train, X_test], axis=0)\n",
    "# Loop through the columns of the combined DataFrame and check their data types\n",
    "for col in combined.columns:\n",
    "    if combined[col].dtype == 'object' or combined[col].dtype.name == 'category':\n",
    "        # Perform one-hot encoding on the column\n",
    "        combined = pd.get_dummies(combined, columns=[col], prefix=[col], drop_first=True)\n",
    "# when performing one-hot encoding, one of the one-hot encoded columns is redundant and should be omitted. To achieve this, you can use the drop_first=True parameter in the get_dummies() function.\n",
    "\n",
    "# Split the combined DataFrame back into X_train and X_test\n",
    "X_train_OHE = combined[:len(X_train)]\n",
    "X_test_OHE = combined[len(X_train):]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d971e6b",
   "metadata": {},
   "source": [
    "##### variable type encoding for QLattice model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f1fbf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# create an empty dictionary to store the stypes\n",
    "stypes = {}\n",
    "\n",
    "# iterate over each column in the dataset\n",
    "for col in devset_imputed.columns:\n",
    "    # check if the column dtype is 'category'\n",
    "    if pd.api.types.is_categorical_dtype(devset_imputed[col]):\n",
    "        # if it is, add the column name to the stypes dictionary with a value of 'c'\n",
    "        stypes[col] = 'c'\n",
    "\n",
    "stypes[outcome_var] = 'b'\n",
    "# print the stypes dictionary\n",
    "print(stypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85bc6b9",
   "metadata": {},
   "source": [
    "#### set model weights based on class balance from the development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49730bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=devset_imputed[outcome_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b1b205",
   "metadata": {},
   "source": [
    "## Initiate machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b580375c",
   "metadata": {},
   "source": [
    "### QLattice model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb9175a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feyn\n",
    "ql = feyn.QLattice(random_seed=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc40a546",
   "metadata": {},
   "source": [
    "##### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b44e86-3b16-4650-ab29-e58518fd3cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, brier_score_loss, precision_recall_curve, auc, matthews_corrcoef\n",
    "import feyn\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "\n",
    "def solve(m1, m2, std1, std2):\n",
    "    a = 1/(2*std1**2) - 1/(2*std2**2)\n",
    "    b = m2/(std2**2) - m1/(std1**2)\n",
    "    c = m1**2 /(2*std1**2) - m2**2 / (2*std2**2) - np.log(std2/std1)\n",
    "    return np.roots([a, b, c])\n",
    "\n",
    "def find_closest_to_default(roots, default=0.5):\n",
    "    roots = np.asarray(roots)\n",
    "    closest_index = np.argmin(np.abs(roots - default))\n",
    "    return roots[closest_index]\n",
    "\n",
    "def calculate_metrics(y_true, y_pred, y_pred_proba):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    PPV = tp / (tp + fp) \n",
    "    NPV = tn / (tn + fn) \n",
    "    Sensitivity = tp / (tp + fn) \n",
    "    Specificity = tn / (tn + fp) \n",
    "    Balanced_Accuracy = (Sensitivity + Specificity) / 2\n",
    "    MCC = matthews_corrcoef(y_true, y_pred)\n",
    "    ROC_AUC = roc_auc_score(y_true, y_pred_proba)\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_pred_proba)\n",
    "    PR_AUC = auc(recall, precision)\n",
    "    Brier_Score = brier_score_loss(y_true, y_pred_proba)\n",
    "\n",
    "    return {\n",
    "        'PPV': PPV,\n",
    "        'NPV': NPV,\n",
    "        'Sensitivity': Sensitivity,\n",
    "        'Specificity': Specificity,\n",
    "        'Balanced Accuracy': Balanced_Accuracy,\n",
    "        'MCC': MCC,\n",
    "        'ROCAUC': ROC_AUC,\n",
    "        'PRAUC': PR_AUC,\n",
    "        'Brier Score': Brier_Score\n",
    "    }\n",
    "\n",
    "def cross_validate_model(model_class, X, y, sample_weights=None, n_splits=5, random_state=None, measures=None, use_default_threshold=False, **model_params):\n",
    "    if measures is None:\n",
    "        measures = ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'ROCAUC', 'PRAUC', 'Brier Score']\n",
    "\n",
    "    fold_results = pd.DataFrame()\n",
    "    aggregated_predictions = np.array([])\n",
    "    aggregated_labels = np.array([])\n",
    "    skf = StratifiedKFold(n_splits=n_splits, random_state=random_state, shuffle=True)\n",
    "\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "        sample_weights_fold = sample_weights[train_index] if sample_weights is not None else None\n",
    "\n",
    "        # Check if the model is a BalancedRandomForestClassifier\n",
    "        if model_class == BalancedRandomForestClassifier:\n",
    "            # Define the corrected parameter grid for random search\n",
    "            param_dist = {\n",
    "                'n_estimators': [50, 100, 150, 200],\n",
    "                'max_depth': [None, 10, 20, 30],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'max_features': ['sqrt', 'log2', None],  # Corrected 'auto' to 'sqrt'\n",
    "            }\n",
    "            \n",
    "            # Explicitly set sampling_strategy to 'all'\n",
    "            model = BalancedRandomForestClassifier(random_state=SEED, sampling_strategy='all',n_jobs=n_cpu_model_training)\n",
    "            \n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=model, \n",
    "                param_distributions=param_dist, \n",
    "                n_iter=n_iter,\n",
    "                scoring=tun_score, \n",
    "                cv=5,\n",
    "                refit=True,\n",
    "                random_state=random_state,\n",
    "                verbose=0, n_jobs = n_cpu_for_tuning)\n",
    "            # Fit the RandomizedSearchCV object to the data\n",
    "            random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            \n",
    "            # Get the best parameters and best estimator from the random search\n",
    "            best_params = random_search.best_params_\n",
    "            model = BalancedRandomForestClassifier(random_state=SEED, sampling_strategy='all',n_jobs=n_cpu_model_training, **best_params)\n",
    "            # model = random_search.best_estimator_\n",
    "\n",
    "            # Fit the best estimator on the entire training data\n",
    "            model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = model.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "                # Check if the model is a QLattice\n",
    "        elif model_class == 'QLattice':\n",
    "            X_train_fold_ql = X_train_fold.copy()\n",
    "            X_train_fold_ql[outcome_var] = y_train_fold\n",
    "            from sklearn.metrics import balanced_accuracy_score\n",
    "            from joblib import Parallel, delayed\n",
    "        \n",
    "            best_balanced_accuracy = 0\n",
    "            best_parameters = {'n_epochs': 50, 'max_complexity': 10}\n",
    "        \n",
    "            def evaluate_params(n_epochs, max_complexity):\n",
    "                ql = feyn.QLattice(random_seed=random_state)\n",
    "                models = ql.auto_run(\n",
    "                    data=X_train_fold_ql,\n",
    "                    output_name=outcome_var,\n",
    "                    kind='classification',\n",
    "                    n_epochs=n_epochs,\n",
    "                    stypes=stypes,\n",
    "                    criterion=\"aic\",\n",
    "                    loss_function='binary_cross_entropy',\n",
    "                    max_complexity=max_complexity,\n",
    "                    sample_weights=sample_weights_fold\n",
    "                )\n",
    "                best_model = models[0]\n",
    "                \n",
    "                predictions_proba = best_model.predict(X_test_fold)\n",
    "                balanced_acc = balanced_accuracy_score(y_test_fold, (predictions_proba > 0.5).astype(int))\n",
    "                return balanced_acc, {'n_epochs': n_epochs, 'max_complexity': max_complexity}\n",
    "        \n",
    "            results = Parallel(n_jobs=n_cpu_for_tuning)(\n",
    "                delayed(evaluate_params)(n_epochs, max_complexity)\n",
    "                for n_epochs in [50, 100]\n",
    "                for max_complexity in [5, 10]\n",
    "            )\n",
    "        \n",
    "            for balanced_acc, params in results:\n",
    "                if balanced_acc > best_balanced_accuracy:\n",
    "                    best_balanced_accuracy = balanced_acc\n",
    "                    best_parameters = params\n",
    "\n",
    "            \n",
    "            print(\"Best Parameters:\", best_parameters)\n",
    "            print(\"Best Balanced Accuracy:\", best_balanced_accuracy)\n",
    "                # Use the best parameters from the grid search\n",
    "            best_n_epochs = best_parameters['n_epochs']\n",
    "            best_max_complexity = best_parameters['max_complexity']\n",
    "            \n",
    "            # Train the final model with the best parameters\n",
    "            ql = feyn.QLattice(random_seed=random_state)\n",
    "            models = ql.auto_run(\n",
    "                data=X_train_fold_ql,\n",
    "                output_name=outcome_var,\n",
    "                kind='classification',\n",
    "                n_epochs=best_n_epochs,\n",
    "                stypes=stypes,\n",
    "                criterion=\"aic\",\n",
    "                loss_function='binary_cross_entropy',\n",
    "                max_complexity=best_max_complexity,\n",
    "                sample_weights=sample_weights_fold\n",
    "            )\n",
    "\n",
    "            model = models[0]\n",
    "            predictions_proba = model.predict(X_test_fold)\n",
    "\n",
    "        elif model_class == HistGradientBoostingClassifier:\n",
    "            # Create a HistGradientBoostingClassifier instance\n",
    "            model = HistGradientBoostingClassifier(random_state=random_state, early_stopping=True, validation_fraction=0.2)\n",
    "            param_dist = {\n",
    "                'learning_rate': [0.01, 0.05, 0.1 ,0.2],\n",
    "                'max_depth': [None, 3, 15, 100],\n",
    "                'min_samples_leaf': [10, 20, 40],\n",
    "                'max_leaf_nodes': [10, 20, 50],\n",
    "                'l2_regularization': [0.01, 0.05, 0.1, 0.2],\n",
    "                \"max_iter\": [500, 1000]\n",
    "            }\n",
    "            # Create a RandomizedSearchCV instance\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=model, \n",
    "                param_distributions=param_dist, \n",
    "                n_iter=n_iter,\n",
    "                scoring=tun_score, #'roc_auc',\n",
    "                cv=5,\n",
    "                refit=True,\n",
    "                random_state=random_state,\n",
    "                verbose=0,\n",
    "            n_jobs = n_cpu_for_tuning)\n",
    "\n",
    "            # Perform the random search on the training data\n",
    "            random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "            # Get the best parameters and best estimator\n",
    "            best_params = random_search.best_params_\n",
    "            model = HistGradientBoostingClassifier(random_state=random_state, early_stopping=True, validation_fraction=0.2, **best_params)\n",
    "            # model = random_search.best_estimator_\n",
    "\n",
    "            # Fit the best estimator on the entire training data\n",
    "            model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = model.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "\n",
    "        elif model_class == lgb.LGBMClassifier:\n",
    "            param_dist = {\n",
    "                'num_leaves': randint(6, 50),\n",
    "                'min_child_samples': randint(100, 500),\n",
    "                'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "                'subsample': uniform(loc=0.2, scale=0.8),\n",
    "                'colsample_bytree': uniform(loc=0.4, scale=0.6),\n",
    "                'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "                'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "                'max_depth': randint(3, 15)  \n",
    "            }\n",
    "\n",
    "            model = lgb.LGBMClassifier(random_state=random_state,\n",
    "                                       n_estimators=500,\n",
    "                                       learning_rate=0.1,\n",
    "                                       verbosity=verbosity_option,\n",
    "                                       n_jobs=n_cpu_model_training) \n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=model, \n",
    "                param_distributions=param_dist, \n",
    "                n_iter=n_iter,\n",
    "                scoring=tun_score, #'roc_auc',\n",
    "                cv=5,\n",
    "                refit=True,\n",
    "                random_state=random_state,\n",
    "                verbose=0,n_jobs = n_cpu_for_tuning)\n",
    "           \n",
    "            # Perform the random search on the training data\n",
    "            random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "            # Get the best parameters and best estimator\n",
    "            best_params = random_search.best_params_\n",
    "            model = lgb.LGBMClassifier(random_state=random_state,\n",
    "                                       n_estimators=500,\n",
    "                                       learning_rate=0.1,\n",
    "                                       verbosity=verbosity_option,\n",
    "                                       n_jobs=n_cpu_model_training,\n",
    "                                       **best_params)\n",
    "\n",
    "            # Fit the best estimator on the entire training data\n",
    "            model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = model.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "        elif model_class == cb.CatBoostClassifier:\n",
    "            \n",
    "            # Define the CatBoost classifier\n",
    "            # Check if there are categorical features\n",
    "            if len(cat_features) > 0:\n",
    "                # If there are categorical features, use them\n",
    "                model = cb.CatBoostClassifier(random_state=SEED, cat_features=cat_features, verbose=0) # verbosity for cb can't be negative\n",
    "            else:\n",
    "                # If there are no categorical features, do not use cat_features parameter\n",
    "                model = cb.CatBoostClassifier(random_state=SEED, verbose=0)\n",
    "            \n",
    "            # Define the parameter grid for random search\n",
    "            param_dist = {\n",
    "                'learning_rate': np.logspace(-3, 0, num=10),\n",
    "                'depth': np.arange(4, 11),\n",
    "                'l2_leaf_reg': np.logspace(-1, 3, num=10),\n",
    "                'iterations': np.arange(100, 1000, 10),\n",
    "                'subsample': np.linspace(0.5, 1, 10),\n",
    "                'random_strength': np.linspace(0, 10, 10)\n",
    "            }\n",
    "\n",
    "            # Create a RandomizedSearchCV instance\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=model, \n",
    "                param_distributions=param_dist, \n",
    "                n_iter=n_iter,\n",
    "                scoring=tun_score, # balanced accuracy had issues for catboost\n",
    "                cv=5,\n",
    "                refit=True,\n",
    "                random_state=random_state,\n",
    "                verbose=0,n_jobs = n_cpu_for_tuning\n",
    "            )\n",
    "\n",
    "            # Perform the random search on the training data\n",
    "            random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "            # Get the best parameters and best estimator\n",
    "            best_params = random_search.best_params_\n",
    "            if len(cat_features) > 0:\n",
    "                # If there are categorical features, use them\n",
    "                model = cb.CatBoostClassifier(random_state=SEED, cat_features=cat_features, verbose=0, **best_params)\n",
    "            else:\n",
    "                # If there are no categorical features, do not use cat_features parameter\n",
    "                model = cb.CatBoostClassifier(random_state=SEED, verbose=0, **best_params)\n",
    "\n",
    "            # Fit the best estimator on the entire training data\n",
    "            model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "\n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = model.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "        # Check if the specified model class is LogisticRegression\n",
    "        elif model_class == LogisticRegression:\n",
    "                    \n",
    "            # Define the Logistic Regression classifier\n",
    "            model = LogisticRegression(random_state=random_state)\n",
    "            param_dist = {\n",
    "            'penalty': ['l2', 'none'],\n",
    "            'C': [0.01, 0.1, 1],\n",
    "            'max_iter': [500, 1000, 2000],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'class_weight': ['balanced', None]}\n",
    "        \n",
    "            # Create a RandomizedSearchCV instance\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=model, \n",
    "                param_distributions=param_dist, \n",
    "                n_iter=n_iter,\n",
    "                scoring=tun_score, #'roc_auc',\n",
    "                cv=5,\n",
    "                refit=True,\n",
    "                random_state=random_state,\n",
    "                verbose=0, n_jobs=n_cpu_for_tuning\n",
    "            )\n",
    "        \n",
    "            # Perform the random search on the training data\n",
    "            random_search.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "        \n",
    "            # Get the best parameters and best estimator\n",
    "            best_params = random_search.best_params_\n",
    "            model = LogisticRegression(random_state=random_state, **best_params)\n",
    "        \n",
    "            # Fit the best estimator on the entire training data\n",
    "            model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "        \n",
    "            # Get predictions on the test data\n",
    "            predictions_proba = model.predict_proba(X_test_fold)[:, 1]\n",
    "        \n",
    "        else:\n",
    "            model = model_class(**model_params)\n",
    "            model.fit(X_train_fold, y_train_fold, sample_weight=sample_weights_fold)\n",
    "            predictions_proba = model.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "        # Aggregate predictions and labels\n",
    "        aggregated_predictions = np.concatenate((aggregated_predictions, predictions_proba))\n",
    "        aggregated_labels = np.concatenate((aggregated_labels, y_test_fold))\n",
    "\n",
    "    # Estimate parameters and find optimal threshold\n",
    "    m1, std1 = np.mean(aggregated_predictions[aggregated_labels == False]), np.std(aggregated_predictions[aggregated_labels == False])\n",
    "    m2, std2 = np.mean(aggregated_predictions[aggregated_labels == True]), np.std(aggregated_predictions[aggregated_labels == True])\n",
    "    intersections = solve(m1, m2, std1, std2)\n",
    "\n",
    "        # Use default threshold if specified\n",
    "    if use_default_threshold:\n",
    "        optimal_threshold = 0.5\n",
    "    else:\n",
    "        optimal_threshold = find_closest_to_default(intersections)\n",
    "\n",
    "    # Calculate and store metrics for each fold\n",
    "    for fold, (train_index, test_index) in enumerate(skf.split(X, y), 1):\n",
    "        X_test_fold = X.iloc[test_index]\n",
    "        y_test_fold = y.iloc[test_index]\n",
    "\n",
    "        # Get predictions for the current fold using the optimal threshold\n",
    "        if model_class == 'QLattice':\n",
    "            predictions_proba_fold = model.predict(X_test_fold)\n",
    "        else:\n",
    "            predictions_proba_fold = model.predict_proba(X_test_fold)[:, 1]\n",
    "\n",
    "        predictions_class_fold = np.where(predictions_proba_fold >= optimal_threshold, 1, 0)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = calculate_metrics(y_test_fold, predictions_class_fold, predictions_proba_fold)\n",
    "        metrics['Fold'] = fold\n",
    "        # fold_results = fold_results.append(metrics, ignore_index=True)\n",
    "        fold_results = pd.concat([fold_results, pd.DataFrame(metrics, index=[0])], ignore_index=True)\n",
    "\n",
    "\n",
    "    # Aggregate results across folds\n",
    "    aggregated_results = {metric: np.nanmean(fold_results[metric].values).round(2) for metric in measures}\n",
    "    aggregated_results_sd = {metric: np.nanstd(fold_results[metric].values).round(2) for metric in measures}\n",
    "\n",
    "    # Combining mean and standard deviation\n",
    "    combined_results = {metric: f\"{mean} ± {sd}\" for metric, mean in aggregated_results.items() for _, sd in aggregated_results_sd.items() if metric == _}\n",
    "    \n",
    "    # Creating a DataFrame for tabular display\n",
    "    results_table = pd.DataFrame(list(combined_results.items()), columns=['Metric', 'Result'])\n",
    "\n",
    "    # Displaying the results\n",
    "    print(\"Aggregated Results:\")\n",
    "    print(results_table.to_string(index=False))\n",
    "\n",
    "    return fold_results, results_table, optimal_threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af9e4f7-d14b-4d4b-81d2-9e91a7779e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import feyn\n",
    "fold_results_QLattice, aggregated_results_formatted_QLattice, optimal_threshold_QLattice = cross_validate_model(model_class='QLattice',\n",
    "                                                                                                                X=X_train, \n",
    "                                                                                                                y=y_train, \n",
    "                                                                                                                sample_weights=sample_weights, \n",
    "                                                                                                                random_state=SEED,\n",
    "                                                                                                               use_default_threshold = use_default_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c692cb",
   "metadata": {},
   "source": [
    "#### QLattice model development using the whole development set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1584b3-1f8e-4246-93fb-25c9df75a24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "best_balanced_accuracy = 0\n",
    "best_parameters = {'n_epochs': 50, 'max_complexity': 10}\n",
    "\n",
    "# Specify the grid search space\n",
    "epoch_values = [50, 100]\n",
    "complexity_values = [5, 10]\n",
    "\n",
    "for n_epochs in epoch_values:\n",
    "    for max_complexity in complexity_values:\n",
    "        # Update the parameters for the QLattice model\n",
    "        ql_params = {\n",
    "            'n_epochs': n_epochs,\n",
    "            'max_complexity': max_complexity,\n",
    "            'stypes': stypes,\n",
    "            'criterion': \"aic\",\n",
    "            'loss_function': 'binary_cross_entropy',\n",
    "            'sample_weights': sample_weights\n",
    "        }\n",
    "\n",
    "        # Create the QLattice model\n",
    "        ql = feyn.QLattice(random_seed=SEED)\n",
    "        models = ql.auto_run(data=devset_imputed, output_name=outcome_var, kind='classification', **ql_params)\n",
    "        best_model = models[0]\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        predictions_proba = best_model.predict(devset_imputed)\n",
    "\n",
    "        # Calculate balanced accuracy\n",
    "        balanced_acc = balanced_accuracy_score(y_train, (predictions_proba > 0.5).astype(int))\n",
    "\n",
    "        # Check if the current parameters yield a better balanced accuracy\n",
    "        if balanced_acc > best_balanced_accuracy:\n",
    "            best_balanced_accuracy = balanced_acc\n",
    "            best_parameters = {'n_epochs': n_epochs, 'max_complexity': max_complexity}\n",
    "\n",
    "print(\"Best Parameters:\", best_parameters)\n",
    "print(\"Best Balanced Accuracy:\", best_balanced_accuracy)\n",
    "\n",
    "# Use the best parameters to train the final model\n",
    "best_n_epochs = best_parameters['n_epochs']\n",
    "best_max_complexity = best_parameters['max_complexity']\n",
    "\n",
    "final_ql = feyn.QLattice(random_seed=SEED)\n",
    "models = final_ql.auto_run(\n",
    "    data=devset_imputed,\n",
    "    output_name=outcome_var,\n",
    "    kind='classification',\n",
    "    n_epochs=best_n_epochs,\n",
    "    stypes=stypes,\n",
    "    criterion=\"aic\",\n",
    "    loss_function='binary_cross_entropy',\n",
    "    max_complexity=best_max_complexity,\n",
    "    sample_weights=sample_weights\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c716614",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c06cd6",
   "metadata": {},
   "source": [
    "#### associations of the QLattice model variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf8057e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_signal(devset_imputed,corr_func='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd4c3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_signal(testset_imputed,corr_func='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b18083",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_signal(devset_imputed,corr_func='mutual_information')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d29e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.plot_signal(testset_imputed,corr_func='mutual_information')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b404a2",
   "metadata": {},
   "source": [
    "#### QLattice model performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6277054",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_QLattice = evaluate_and_plot_model(model = best_model,\n",
    "                                              threshold = optimal_threshold_QLattice,\n",
    "                                              testset = testset_imputed,\n",
    "                                              y_test = y_test,\n",
    "                                              class_labels = ['Negative', 'Positive'], filename='ROC_CM_QLattice.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666add3",
   "metadata": {},
   "source": [
    "#### QLattice model overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f593bd2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_model.plot(devset_imputed, testset_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0a4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selected by the model\n",
    "best_model.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35afd78c",
   "metadata": {},
   "source": [
    "#### distribution of model predicted probabilities for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f763ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "best_model.plot_probability_scores(testset_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d58f98",
   "metadata": {},
   "source": [
    "#### model representation as a closed-form expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78b4c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sympy_model = best_model.sympify(symbolic_lr=True, include_weights=True)\n",
    "\n",
    "sympy_model.as_expr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1ec06b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5faefefa",
   "metadata": {},
   "source": [
    "### stratified 5-fold cross validation \n",
    "\n",
    "Here we do cross validation to see how the model may perform on the test set.\n",
    "This is done for each of the laternative models that is Random Forest, LightGBM, CATBoost, and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4a5534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "fold_results_NB, aggregated_results_formatted_NB, optimal_threshold_NB = cross_validate_model(model_class=GaussianNB,\n",
    "                                                                                              X=X_train_OHE,\n",
    "                                                                                              y=y_train,\n",
    "                                                                                              sample_weights=sample_weights,\n",
    "                                                                                              random_state=SEED,\n",
    "                                                                                             use_default_threshold = use_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fbefc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "fold_results_LR, aggregated_results_formatted_LR, optimal_threshold_LR = cross_validate_model(model_class=LogisticRegression,\n",
    "                                                                                              X=X_train_OHE, \n",
    "                                                                                              y=y_train,\n",
    "                                                                                              sample_weights=sample_weights,\n",
    "                                                                                              random_state=SEED, \n",
    "                                                                                             use_default_threshold = use_default_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652abd26",
   "metadata": {},
   "source": [
    "##### Balanced Random Forest Classifier (RBF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8414eba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "fold_results_BRF, aggregated_results_formatted_BRF, optimal_threshold_BRF = cross_validate_model(model_class=BalancedRandomForestClassifier,\n",
    "                                                                                                 X = X_train_OHE,\n",
    "                                                                                                 y = y_train,\n",
    "                                                                                                 sample_weights = sample_weights,\n",
    "                                                                                                 random_state = SEED,\n",
    "                                                                                                use_default_threshold = use_default_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77965d2e",
   "metadata": {},
   "source": [
    "##### Histogram-based Gradient Boosting Classification Tree (HGBC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f3e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "fold_results_HGBC, aggregated_results_formatted_HGBC, optimal_threshold_HGBC = cross_validate_model(model_class=HistGradientBoostingClassifier,\n",
    "                                                                                                    X = X_train_OHE,\n",
    "                                                                                                    y = y_train,\n",
    "                                                                                                    sample_weights = sample_weights,\n",
    "                                                                                                    random_state = SEED,\n",
    "                                                                                                   use_default_threshold = use_default_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6759697d",
   "metadata": {},
   "source": [
    "##### light gradient-boosting machine (LightGBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756ab6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "fold_results_LGBM, aggregated_results_formatted_LGBM, optimal_threshold_LGBM = cross_validate_model(model_class=lgb.LGBMClassifier,\n",
    "                                                                                                    X = X_train_withmissing,\n",
    "                                                                                                    y = y_train,\n",
    "                                                                                                    sample_weights = sample_weights,\n",
    "                                                                                                    random_state = SEED,\n",
    "                                                                                                   use_default_threshold = use_default_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc852846",
   "metadata": {},
   "source": [
    "##### categorical boosting (CATBoost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6b2813",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = list(cat_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost as cb\n",
    "fold_results_CB, aggregated_results_formatted_CB, optimal_threshold_CB = cross_validate_model(model_class=cb.CatBoostClassifier,\n",
    "                                                                                              X = X_train_withmissing,\n",
    "                                                                                              y = y_train,\n",
    "                                                                                              sample_weights = sample_weights,\n",
    "                                                                                              random_state = SEED,\n",
    "                                                                                              cat_features=cat_features, \n",
    "                                                                                              # iterations=500, \n",
    "                                                                                              verbose=0,\n",
    "                                                                                             use_default_threshold = use_default_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84365deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [aggregated_results_formatted_QLattice, aggregated_results_formatted_LR, aggregated_results_formatted_BRF,\n",
    "          aggregated_results_formatted_HGBC, aggregated_results_formatted_LGBM, aggregated_results_formatted_CB,\n",
    "          aggregated_results_formatted_NB]\n",
    "\n",
    "# Set 'Metric' as the index for each model's DataFrame\n",
    "for model in models:\n",
    "    model.set_index('Metric', inplace=True)\n",
    "\n",
    "# Concatenate the DataFrames along the columns\n",
    "aggregated_results_formatted_all = pd.concat(models, axis=1)\n",
    "\n",
    "# Set the column names\n",
    "aggregated_results_formatted_all.columns = [\"QLattice\", \"LR\", \"BRF\", \"HGBC\", \"LGBM\", \"CB\", \"NB\"]\n",
    "\n",
    "# Display the results\n",
    "print(aggregated_results_formatted_all)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "aggregated_results_formatted_all.to_excel('aggregated_results_formatted_all.xlsx', index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8402fc70",
   "metadata": {},
   "source": [
    "### Model training using the whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa50c61-7528-43bc-8a99-aa3d65e7b46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0bf94f-14d5-45e7-bc50-8bac23f3fccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Train Dummy Classifier\n",
    "dummy_classifier = DummyClassifier(strategy='most_frequent')  \n",
    "dummy_classifier.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "results_df_dummy = evaluate_and_plot_model(model = dummy_classifier,\n",
    "                                        threshold = 0.5,\n",
    "                                        testset = X_test_OHE,\n",
    "                                        y_test = y_test,\n",
    "                                        class_labels = ['Negative', 'Positive'],\n",
    "                                        filename='ROC_CM_dummy_most_frequent.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e593723-c2ed-4eaf-a19e-2af625428373",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "# Train Dummy Classifier with 'stratified' strategy\n",
    "dummy_classifier = DummyClassifier(strategy='stratified')\n",
    "dummy_classifier.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "results_df_dummy = evaluate_and_plot_model(model = dummy_classifier,\n",
    "                                        threshold = 0.5,\n",
    "                                        testset = X_test_OHE,\n",
    "                                        y_test = y_test,\n",
    "                                        class_labels = ['Negative', 'Positive'],\n",
    "                                        filename='ROC_CM_dummy_stratified.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8134e0f",
   "metadata": {},
   "source": [
    "### evaluate alternative models on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8be679",
   "metadata": {},
   "source": [
    "##### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89b2796-9184-4f31-b180-bebe4b555324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes\n",
    "nb_classifier = GaussianNB()\n",
    "nb_classifier.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "results_df_NB = evaluate_and_plot_model(model = nb_classifier, threshold = optimal_threshold_NB, testset = X_test_OHE, y_test = y_test, class_labels = ['Negative', 'Positive'], filename='ROC_CM_NB.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276125f3-3154-41fc-9e9b-8ff88d5a4ab2",
   "metadata": {},
   "source": [
    "##### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ea0939-9b42-477a-bcb3-c220eb4a754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_dist = {\n",
    "            'penalty': ['l2', 'none'],\n",
    "            'C': [0.01, 0.1, 1],\n",
    "            'max_iter': [500, 1000, 2000],\n",
    "            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "            'class_weight': ['balanced', None]}\n",
    "\n",
    "# Create a Logistic Regression instance\n",
    "lr = LogisticRegression(random_state=SEED)\n",
    "\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lr,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=n_iter, \n",
    "    scoring=tun_score,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=SEED,\n",
    "    verbose=0,\n",
    "    n_jobs=n_cpu_for_tuning\n",
    ")\n",
    "\n",
    "# Perform the random search on the training data\n",
    "random_search.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = random_search.best_params_\n",
    "lr = LogisticRegression(random_state=SEED, **best_params)\n",
    "\n",
    "# Fit the best estimator on the entire training data\n",
    "lr.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486d03e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_LR = evaluate_and_plot_model(model = lr,\n",
    "                                        threshold = optimal_threshold_LR,\n",
    "                                        testset = X_test_OHE,\n",
    "                                        y_test = y_test,\n",
    "                                        class_labels = ['Negative', 'Positive'],\n",
    "                                        filename='ROC_CM_LR.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cfeeb8",
   "metadata": {},
   "source": [
    "##### HGBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ca825a-d6bf-4df2-8338-c8705fa56ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_dist = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1 ,0.2],\n",
    "    'max_depth': [None, 3, 15, 100],\n",
    "    'min_samples_leaf': [10, 20, 40],\n",
    "    'max_leaf_nodes': [10, 20, 50],\n",
    "    'l2_regularization': [0.01, 0.05, 0.1, 0.2],\n",
    "    \"max_iter\": [500, 1000]\n",
    "}\n",
    "\n",
    "# Create a HistGradientBoostingClassifier instance\n",
    "HGBC = HistGradientBoostingClassifier(random_state=SEED, early_stopping=True, validation_fraction=0.2)\n",
    "\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=HGBC, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=n_iter,\n",
    "    scoring=tun_score,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=SEED,\n",
    "    verbose=0,n_jobs = n_cpu_for_tuning)\n",
    "\n",
    "# Perform the random search on the training data\n",
    "random_search.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Get the best parameters and best estimator\n",
    "best_params = random_search.best_params_\n",
    "HGBC = HistGradientBoostingClassifier(random_state=SEED, early_stopping=True, validation_fraction=0.2, **best_params)\n",
    "\n",
    "# Fit the best estimator on the entire training data\n",
    "HGBC.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b9ff66",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_HGBC = evaluate_and_plot_model(model = HGBC, threshold = optimal_threshold_HGBC, testset = X_test_OHE, y_test = y_test, class_labels = ['Negative', 'Positive'], filename='ROC_CM_HGBC.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da87373",
   "metadata": {},
   "source": [
    "##### BRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00edbefe-9f08-4732-a392-8596458cc426",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 150, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "}\n",
    "\n",
    "# Explicitly set sampling_strategy to 'all'\n",
    "brf = BalancedRandomForestClassifier(random_state=SEED, sampling_strategy='all', n_jobs=n_cpu_model_training)\n",
    "\n",
    "# Create RandomizedSearchCV object with balanced accuracy as the scoring metric\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=brf, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=n_iter,\n",
    "    scoring=tun_score,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=SEED,\n",
    "    verbose=0,n_jobs = n_cpu_for_tuning)\n",
    "\n",
    "# Fit the RandomizedSearchCV object to the data\n",
    "random_search.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Get the best parameters and best estimator from the random search\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Reinitialize a new brf model with the best parameters\n",
    "brf = BalancedRandomForestClassifier(random_state=SEED, sampling_strategy='all',n_jobs=n_cpu_model_training,  **best_params)\n",
    "\n",
    "# Train the new model on the training data\n",
    "brf.fit(X_train_OHE, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Parameters:\", best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5045ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_BRF = evaluate_and_plot_model(model = brf, threshold = optimal_threshold_BRF, testset = X_test_OHE, y_test = y_test, class_labels = ['Negative', 'Positive'], filename='ROC_CM_BRF.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ccb91b",
   "metadata": {},
   "source": [
    "##### LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1399d383-da66-460e-ac76-f1854cdea924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "# Define the classifier\n",
    "lgbm = lgb.LGBMClassifier(random_state=SEED,\n",
    "                          verbosity=verbosity_option,\n",
    "                          n_jobs=n_cpu_model_training,\n",
    "                          n_estimators=500,\n",
    "                          learning_rate=0.1)\n",
    "\n",
    "\n",
    "param_dist = {\n",
    "                'num_leaves': randint(6, 50),\n",
    "                'min_child_samples': randint(100, 500),\n",
    "                'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n",
    "                'subsample': uniform(loc=0.2, scale=0.8),\n",
    "                'colsample_bytree': uniform(loc=0.4, scale=0.6),\n",
    "                'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "                'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100],\n",
    "                'max_depth': randint(3, 15) \n",
    "            }\n",
    "# Define the search strategy and scoring metric\n",
    "# Create a RandomizedSearchCV instance\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=lgbm, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=n_iter,\n",
    "    scoring=tun_score,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=SEED,\n",
    "    verbose=0,\n",
    "    n_jobs = n_cpu_for_tuning)\n",
    "\n",
    "# Perform the random search on the data\n",
    "random_search.fit(X_train_withmissing, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Reinitialize a new lgbm model with the best parameters\n",
    "lgbm = lgb.LGBMClassifier(random_state=SEED,\n",
    "                          **best_params,\n",
    "                          verbosity=verbosity_option,\n",
    "                          n_jobs=n_cpu_model_training,\n",
    "                          n_estimators=500,\n",
    "                          learning_rate=0.1)\n",
    "\n",
    "# Train the new model on the training data\n",
    "lgbm.fit(X_train_withmissing, y_train, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f3449f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_LGBM = evaluate_and_plot_model(model = lgbm,\n",
    "                                          threshold =optimal_threshold_LGBM,\n",
    "                                          testset = X_test_withmissing,\n",
    "                                          y_test = y_test,\n",
    "                                          class_labels = ['Negative', 'Positive'],\n",
    "                                          filename='ROC_CM_LGBM.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9d0adb",
   "metadata": {},
   "source": [
    "##### CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada0d7e-6b2c-42e4-9aec-81b1176d0105",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, balanced_accuracy_score\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "# Check if there are categorical features\n",
    "if len(cat_features) > 0:\n",
    "    # If there are categorical features, use them\n",
    "    catb = CatBoostClassifier(random_state=SEED, cat_features=cat_features, verbose=0)\n",
    "else:\n",
    "    # If there are no categorical features, do not use cat_features parameter\n",
    "    catb = CatBoostClassifier(random_state=SEED, verbose=0)\n",
    "\n",
    "# Define the parameter grid for random search\n",
    "param_dist = {\n",
    "    'learning_rate': np.logspace(-3, 0, num=10),\n",
    "    'depth': np.arange(4, 11),\n",
    "    'l2_leaf_reg': np.logspace(-1, 3, num=10),\n",
    "    'iterations': np.arange(100, 1000, 10),\n",
    "    'subsample': np.linspace(0.5, 1, 10),\n",
    "    'random_strength': np.linspace(0, 10, 10)\n",
    "}\n",
    "\n",
    "# Perform random search\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=catb, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=n_iter,\n",
    "    scoring=tun_score,#scorer,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=SEED,\n",
    "    verbose=0,\n",
    "    n_jobs=n_cpu_for_tuning\n",
    ")\n",
    "\n",
    "# Fit the random search on your data\n",
    "random_search.fit(X_train_withmissing, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# Get the best parameters and best model\n",
    "best_params = random_search.best_params_\n",
    "\n",
    "# Check if there are categorical features\n",
    "if len(cat_features) > 0:\n",
    "    # If there are categorical features, use them\n",
    "    catb = CatBoostClassifier(random_state=SEED, cat_features=cat_features, verbose=0, **best_params)\n",
    "else:\n",
    "    # If there are no categorical features, do not use cat_features parameter\n",
    "    catb = CatBoostClassifier(random_state=SEED, verbose=0, **best_params)\n",
    "    \n",
    "\n",
    "# Train the new model on the training data\n",
    "catb.fit(X_train_withmissing, y_train, sample_weight=sample_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0da3d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_CB = evaluate_and_plot_model(model = catb,threshold = optimal_threshold_CB, testset = X_test_withmissing, y_test = y_test, class_labels = ['Negative', 'Positive'], filename='ROC_CM_CB.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb0d48e",
   "metadata": {},
   "source": [
    "#### Choose the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c4ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a list of models and their respective dataframes\n",
    "models_and_dfs = [\n",
    "    ('QLattice', results_df_QLattice),\n",
    "    ('LR', results_df_LR),\n",
    "    ('BRF', results_df_BRF),\n",
    "    ('HGBC', results_df_HGBC),\n",
    "    ('LGBM', results_df_LGBM),\n",
    "    ('CB', results_df_CB),\n",
    "    (\"NB\", results_df_NB)\n",
    "]\n",
    "\n",
    "# Merge data frames based on the \"Metric\" column\n",
    "merged_df = models_and_dfs[0][1].copy()\n",
    "\n",
    "for model, df in models_and_dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df, on='Metric', suffixes=(f'_{model}', ''))\n",
    "\n",
    "# Rename columns\n",
    "columns = ['Measures'] + [f'{model}_{col}' for model, _ in models_and_dfs for col in df.columns[1:]]\n",
    "merged_df.columns = columns\n",
    "\n",
    "# Copy the merged dataframe\n",
    "aggregated_results_test_all = merged_df.copy()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(aggregated_results_test_all)\n",
    "\n",
    "df = aggregated_results_test_all.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2290e70b-a2dd-4752-9ea1-ee4bfadac0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of metrics\n",
    "metrics = ['PPV', 'NPV', 'Sensitivity', 'Specificity', 'Balanced Accuracy', 'MCC', 'ROCAUC', 'PRAUC', 'Brier Score']\n",
    "# List of methods (e.g., QLattice, LR)\n",
    "methods = ['QLattice', 'LR', 'BRF', 'HGBC', 'LGBM', 'CB', \"NB\"]  # Add more if you have more methods\n",
    "\n",
    "# Iterate over each metric and method, merging the columns\n",
    "for metric in metrics:\n",
    "    for method in methods:\n",
    "        columns_to_merge = [f'{method}_Value', f'{method}_Lower Bound', f'{method}_Upper Bound']\n",
    "        new_column_name = f'{method}'\n",
    "\n",
    "        # Check if the columns to merge exist in the DataFrame\n",
    "        if all(col in df.columns for col in columns_to_merge):\n",
    "            # Merge the columns into a new column\n",
    "            df[new_column_name] = df[columns_to_merge].apply(lambda x: f'{x[0]} ({x[1]} {x[2]})', axis=1)\n",
    "\n",
    "            # Optionally, drop the individual columns if needed\n",
    "            df = df.drop(columns=columns_to_merge)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(df)\n",
    "# Save the results to an Excel file\n",
    "df.to_excel('aggregated_results_test_withCI_all.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c2ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define a list of models and their respective dataframes\n",
    "models_and_dfs = [\n",
    "    ('QLattice', results_df_QLattice),\n",
    "    ('LR', results_df_LR),\n",
    "    ('BRF', results_df_BRF),\n",
    "    ('HGBC', results_df_HGBC),\n",
    "    ('LGBM', results_df_LGBM),\n",
    "    ('CB', results_df_CB),\n",
    "    (\"NB\", results_df_NB)\n",
    "]\n",
    "\n",
    "# Merge data frames based on the \"Metric\" column\n",
    "merged_df = models_and_dfs[0][1][['Metric', 'Value']].copy()\n",
    "\n",
    "for model, df in models_and_dfs[1:]:\n",
    "    merged_df = pd.merge(merged_df, df[['Metric', 'Value']], on='Metric', suffixes=(f'_{model}', ''))\n",
    "\n",
    "# Rename columns\n",
    "columns = ['Measures'] + [f'{model}' for model, _ in models_and_dfs for col in ['Value']]\n",
    "merged_df.columns = columns\n",
    "\n",
    "# Copy the merged dataframe\n",
    "aggregated_results_test_all = merged_df.copy()\n",
    "\n",
    "# Print the aggregated results\n",
    "print(aggregated_results_test_all)\n",
    "\n",
    "# Save the results to an Excel file\n",
    "aggregated_results_test_all.to_excel('aggregated_results_test_all.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b6fb47-5f93-492c-91e0-c5101fd5bf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = aggregated_results_test_all.copy()\n",
    "\n",
    "# replace NA with 0 to calculate the following mean for ranking, correctly\n",
    "# NA comes from the cases where we have no negative (or positive) class in predictions\n",
    "data.fillna(0, inplace=True)\n",
    "\n",
    "# Get the average value for the numerical columns\n",
    "average_values = data.iloc[5:8, 2:].mean(axis=0) # QLattice excluded from comparison\n",
    "\n",
    "# Get the name of the one with the highest average\n",
    "highest_average = average_values.idxmax()\n",
    "\n",
    "model_dictionary = {\"BRF\": brf,\n",
    "                    \"HGBC\": HGBC,\n",
    "                    \"LGBM\": lgbm,\n",
    "                    \"CB\": catb,\n",
    "                    \"LR\": lr,\n",
    "                    \"NB\": nb_classifier\n",
    "}\n",
    "print(\"Selected Model:\", highest_average)\n",
    "selected_model =  model_dictionary[highest_average]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2850ed71",
   "metadata": {},
   "source": [
    "### Model interpretation (for the best alternative model to QLattice)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7bdb60",
   "metadata": {},
   "source": [
    "#### SHAP values association with predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1abf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import catboost as cb\n",
    "\n",
    "# Calculate SHAP values for the positive class\n",
    "positive_class_index = 1  # Adjust this index based on the class labels of your problem\n",
    "\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier)):\n",
    "    explainer = shap.TreeExplainer(selected_model,feature_perturbation='interventional')\n",
    "    shap_values = explainer.shap_values(X_test_OHE)\n",
    "    if isinstance(selected_model, BalancedRandomForestClassifier):\n",
    "        shap_values = shap_values[positive_class_index]\n",
    "elif isinstance(selected_model, cb.CatBoostClassifier):\n",
    "    explainer = shap.TreeExplainer(selected_model)\n",
    "    shap_values = explainer.shap_values(X_test_withmissing)\n",
    "elif isinstance(selected_model, LogisticRegression):\n",
    "    explainer = shap.LinearExplainer(selected_model, X_train_OHE)\n",
    "    shap_values = explainer.shap_values(X_test_OHE)\n",
    "elif isinstance(selected_model, GaussianNB):  \n",
    "    explainer = shap.KernelExplainer(selected_model.predict_proba, X_train_OHE)\n",
    "    shap_values = explainer.shap_values(X_test_OHE, nsamples=100)  \n",
    "    shap_values = shap_values[positive_class_index]\n",
    "else:\n",
    "    explainer = shap.TreeExplainer(selected_model)\n",
    "    shap_values = explainer.shap_values(X_test_withmissing)\n",
    "    shap_values = shap_values[positive_class_index]\n",
    "\n",
    "# Calculate the sum of SHAP values for each sample\n",
    "shap_sum = shap_values.sum(axis=1)\n",
    "\n",
    "# Get the predicted probabilities of the model\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier)):\n",
    "    predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "elif isinstance(selected_model, cb.CatBoostClassifier):\n",
    "    predicted_probabilities = selected_model.predict_proba(X_test_withmissing)[:, positive_class_index]\n",
    "elif isinstance(selected_model, (LogisticRegression, GaussianNB)):\n",
    "    predicted_probabilities = selected_model.predict_proba(X_test_OHE)[:, positive_class_index]\n",
    "else:\n",
    "    predicted_probabilities = selected_model.predict_proba(X_test_withmissing)[:, positive_class_index]\n",
    "\n",
    "# Plot the SHAP sum against the predicted probabilities\n",
    "plt.scatter(shap_sum, predicted_probabilities)\n",
    "plt.xlabel('Sum of SHAP values')\n",
    "plt.ylabel('Predicted Probability')\n",
    "plt.title('Sum of SHAP Values vs. Predicted Probability')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b1cc40",
   "metadata": {},
   "source": [
    "#### interpret the model based on SHAP analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute SHAP values\n",
    "abs_shap_values = np.abs(shap_values)\n",
    "\n",
    "# Compute the feature importance based on the sum of absolute SHAP values\n",
    "feature_importance = np.mean(abs_shap_values, axis=0)\n",
    "\n",
    "# Create a DataFrame to store feature importance\n",
    "# feature_importance_df = pd.DataFrame({'Feature': X_train.columns.tolist(), 'Importance': feature_importance})\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "    feature_importance_df = pd.DataFrame({'Feature': [data_dictionary.get(feature, feature) for feature in X_test_OHE.columns.tolist()], 'Importance': feature_importance})\n",
    "else:\n",
    "    feature_importance_df = pd.DataFrame({'Feature': [data_dictionary.get(feature, feature) for feature in X_test_withmissing.columns.tolist()], 'Importance': feature_importance})\n",
    "\n",
    "# Sort the features by importance in descending order\n",
    "feature_importance_df = feature_importance_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "# Print the top 20 most important features \n",
    "top_20_features = feature_importance_df.head(20)\n",
    "print(top_20_features)\n",
    "# Reverse the order of the sorted data\n",
    "top_20_features = top_20_features[::-1]\n",
    "\n",
    "# Reset the index of the DataFrame\n",
    "top_20_features.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Plot the top 20 most important features\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.barh(top_20_features.index, top_20_features['Importance'])\n",
    "plt.yticks(top_20_features.index, top_20_features['Feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Most Important Features')\n",
    "plt.rcParams['figure.autolayout'] = True  # Automatically adjust the figure margins\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('feature_importance_shap_plot.png', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd12903",
   "metadata": {},
   "source": [
    "#### SHAP summary plot\n",
    "\n",
    "Note: the plot cannot show categorical features in color codes and thus they are plotted in grey (not mistaken with missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b724cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve feature names from the data dictionary\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier,LogisticRegression, GaussianNB)):\n",
    "    feature_names_with_shapvalues = [\n",
    "        data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "        for feature, value in zip(X_test_OHE.columns, np.mean(np.abs(shap_values), axis=0)) \n",
    "    ]\n",
    "    shap.summary_plot(shap_values, X_test_OHE, feature_names=feature_names_with_shapvalues, show=False, alpha = 0.8, max_display=20)\n",
    "elif isinstance(selected_model, (cb.CatBoostClassifier)):\n",
    "    feature_names_with_shapvalues = [\n",
    "        data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "        for feature, value in zip(X_test_withmissing.columns, np.mean(np.abs(shap_values), axis=0)) \n",
    "    ]\n",
    "    shap.summary_plot(shap_values, X_test_withmissing, feature_names=feature_names_with_shapvalues, show=False, alpha = 0.8, max_display=20)\n",
    "else:\n",
    "    feature_names_with_shapvalues = [\n",
    "        data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "        for feature, value in zip(X_test_withmissing.columns, np.mean(np.abs(shap_values), axis=0)) \n",
    "    ]\n",
    "    shap.summary_plot(shap_values, X_test_withmissing, feature_names=feature_names_with_shapvalues, show=False, alpha = 0.8, max_display=20)\n",
    "\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('shap_summary_top20_plot.png', dpi=300)\n",
    "plt.rcParams['figure.autolayout'] = True  # Automatically adjust the figure margins\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990759cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our naive cutoff point is zero log odds (probability 0.5).\n",
    "# Get predictions for the dev set\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "\n",
    "    predictions = selected_model.predict_proba(X_test_OHE)\n",
    "    predictions = predictions[:, 1]\n",
    "    # print(predictions_class)\n",
    "    y_pred = [True if x >= 0.5 else False for x in predictions]\n",
    "    misclassified = y_pred != y_test\n",
    "elif isinstance(selected_model, (cb.CatBoostClassifier)):\n",
    "    predictions = selected_model.predict_proba(X_test_withmissing)\n",
    "    predictions = predictions[:, 1]\n",
    "    # print(predictions_class)\n",
    "    y_pred = [True if x >= 0.5 else False for x in predictions]\n",
    "    misclassified = y_pred != y_test\n",
    "else:\n",
    "    predictions = selected_model.predict_proba(X_test_withmissing)\n",
    "    predictions = predictions[:, 1]\n",
    "    # print(predictions_class)\n",
    "    y_pred = [True if x >= 0.5 else False for x in predictions]\n",
    "    misclassified = y_pred != y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc467198",
   "metadata": {},
   "source": [
    "#### SHAP decision plot \n",
    "only significantly contributed features could also be plotted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318b49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shap\n",
    "\n",
    "# Plot the SHAP decision plot with only significant features\n",
    "if isinstance(selected_model, (HistGradientBoostingClassifier)):\n",
    "    shap.decision_plot(explainer.expected_value, \n",
    "                       shap_values,\n",
    "                       X_test_OHE,\n",
    "                       alpha=0.5, feature_names=feature_names_with_shapvalues,\n",
    "                       link='logit',\n",
    "                       highlight=misclassified,ignore_warnings=True)\n",
    "elif isinstance(selected_model, (BalancedRandomForestClassifier)):\n",
    "    shap.decision_plot(explainer.expected_value[positive_class_index], \n",
    "                       shap_values,\n",
    "                       X_test_OHE,\n",
    "                       alpha=0.5, feature_names=feature_names_with_shapvalues,\n",
    "                       link='logit',\n",
    "                       highlight=misclassified,ignore_warnings=True)\n",
    "elif isinstance(selected_model, (LogisticRegression)):\n",
    "    feature_names_with_shapvalues = [\n",
    "        data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "        for feature, value in zip(X_test_OHE.columns, np.mean(np.abs(shap_values), axis=0)) \n",
    "    ]\n",
    "    shap.decision_plot(explainer.expected_value, \n",
    "                       shap_values,\n",
    "                       X_test_OHE,\n",
    "                       alpha=0.5, \n",
    "                       feature_names=feature_names_with_shapvalues,\n",
    "                       link='logit',\n",
    "                       highlight=misclassified,ignore_warnings=True)\n",
    "elif isinstance(selected_model, (cb.CatBoostClassifier)):\n",
    "    shap.decision_plot(explainer.expected_value, \n",
    "                       shap_values,\n",
    "                       X_test_withmissing,\n",
    "                       alpha=0.5, feature_names=feature_names_with_shapvalues,\n",
    "                       link='logit',\n",
    "                       highlight=misclassified,ignore_warnings=True)\n",
    "elif isinstance(selected_model, (GaussianNB)):\n",
    "    feature_names_with_shapvalues = [\n",
    "        data_dictionary.get(feature, feature) + \": \" + str(round(value, 2))\n",
    "        for feature, value in zip(X_test_OHE.columns, np.mean(np.abs(shap_values), axis=0))\n",
    "    ]\n",
    "    shap.decision_plot(explainer.expected_value[positive_class_index], \n",
    "                       shap_values,\n",
    "                       X_test_OHE,\n",
    "                       alpha=0.5, \n",
    "                       feature_names=feature_names_with_shapvalues,\n",
    "                       link='logit',\n",
    "                       highlight=misclassified, ignore_warnings=True)\n",
    "else:\n",
    "    shap.decision_plot(explainer.expected_value[positive_class_index], \n",
    "                       shap_values,\n",
    "                       X_test_withmissing,\n",
    "                       alpha=0.5, feature_names=feature_names_with_shapvalues,\n",
    "                       link='logit',\n",
    "                       highlight=misclassified, ignore_warnings=True)\n",
    "\n",
    "# Save the figure as a PNG file\n",
    "plt.savefig('shap_decision_allfeats_plot.png', dpi=300)\n",
    "plt.rcParams['figure.autolayout'] = True  # Automatically adjust the figure margins\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd2ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import shap\n",
    "from scipy.stats import spearmanr, ttest_rel\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.stats import linregress\n",
    "# Set the style to emulate the Nature journal style\n",
    "sns.set(style=\"ticks\")\n",
    "\n",
    "# Set the background color to white\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (10, 5)\n",
    "\n",
    "# Compute median absolute SHAP values for each feature\n",
    "median_abs_shap_values = np.median(np.abs(shap_values), axis=0)\n",
    "\n",
    "# Sort features by median absolute SHAP values in descending order\n",
    "sorted_features = np.argsort(median_abs_shap_values)[::-1]\n",
    "\n",
    "# Calculate the number of features to plot\n",
    "num_features_to_plot = min(np.sum(median_abs_shap_values > 0), 20)  # Plot up to 10 features\n",
    "\n",
    "# Set the number of columns for subplots\n",
    "num_cols = 3\n",
    "\n",
    "# Calculate the number of rows for subplots\n",
    "num_rows = int(np.ceil(num_features_to_plot / num_cols))\n",
    "\n",
    "# Initialize a subplot\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, 5*num_rows))\n",
    "axs = axs.ravel()\n",
    "\n",
    "# Track the current subplot index\n",
    "current_subplot = 0\n",
    "\n",
    "# Iterate over the top features\n",
    "for feature in sorted_features[:num_features_to_plot]:\n",
    "    # Get feature name\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        feature_name = X_test_OHE.columns[feature]\n",
    "        x = X_test_OHE.iloc[:, feature]\n",
    "    else:\n",
    "        feature_name = X_test_withmissing.columns[feature]\n",
    "        if X_test[feature_name].dtype.name == 'category':\n",
    "            # Convert categorical feature to numerical using LabelEncoder\n",
    "            encoder = LabelEncoder()\n",
    "            X_test_encoded = X_test_withmissing.copy()\n",
    "            X_test_encoded[feature_name] = encoder.fit_transform(X_test_withmissing[feature_name])\n",
    "            x = X_test_encoded.iloc[:, feature].astype(float)\n",
    "        else:\n",
    "            x = X_test_withmissing.iloc[:, feature].astype(float)\n",
    "    \n",
    "    # Handle missing values in feature values and SHAP values\n",
    "    mask_x = ~pd.isnull(x)\n",
    "    mask_shap = ~np.isnan(shap_values[:, feature])\n",
    "    mask = mask_x & mask_shap\n",
    "    \n",
    "    x_filtered = x[mask]\n",
    "    shap_values_filtered = shap_values[:, feature][mask]\n",
    "    predictions_filtered = predictions[mask]\n",
    "    misclassified_filtered = misclassified[mask]\n",
    "    \n",
    "    # Check if all x values are identical\n",
    "    if len(np.unique(x_filtered)) == 1:\n",
    "        print(f\"Skipped feature {feature_name} because all x values are identical.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate Spearman correlation coefficient and p-value\n",
    "    correlation, p_value = spearmanr(x_filtered, shap_values_filtered, nan_policy='omit')\n",
    "    \n",
    "    # Create scatter plot in the current subplot\n",
    "    scatter = axs[current_subplot].scatter(x_filtered, shap_values_filtered, c=predictions_filtered, cmap='viridis', alpha=0.7, s=50)\n",
    "    axs[current_subplot].set_xlabel(feature_name)\n",
    "    axs[current_subplot].set_ylabel(\"SHAP Value\")\n",
    "    \n",
    "    # Add correlation line\n",
    "    slope, intercept, r_value, p_value_corr, std_err = linregress(x_filtered, shap_values_filtered)\n",
    "    axs[current_subplot].plot(x_filtered, slope * x_filtered + intercept, color='red')\n",
    "    \n",
    "    # Mark misclassified samples with 'x'\n",
    "    axs[current_subplot].scatter(x_filtered[misclassified_filtered], shap_values_filtered[misclassified_filtered], marker=\"X\", color='red', alpha=0.5, s=50)\n",
    "    \n",
    "    # Customize colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=axs[current_subplot])\n",
    "    cbar.set_label(\"Predicted Probability\")\n",
    "    \n",
    "    # Check if correlation is statistically significant\n",
    "    if not np.isnan(correlation) and not np.isnan(p_value_corr):\n",
    "        _, p_value_corr_test = ttest_rel(x_filtered, shap_values_filtered)\n",
    "        p_value_text = f\"p < 0.05\" if p_value_corr_test < 0.05 else f\"p = {p_value_corr_test:.2f}\"\n",
    "        axs[current_subplot].set_title(f\"{feature_name} vs. SHAP Value\\nSpearman Correlation: {correlation:.2f}, {p_value_text}\")\n",
    "    else:\n",
    "        axs[current_subplot].set_title(f\"{feature_name} vs. SHAP Value\\nCorrelation: N/A\")\n",
    "    \n",
    "    # Increment the current subplot index\n",
    "    current_subplot += 1\n",
    "\n",
    "# Hide any remaining empty subplots\n",
    "for i in range(current_subplot, num_rows * num_cols):\n",
    "    axs[i].axis('off')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494c031e",
   "metadata": {},
   "source": [
    "SHAP (SHapley Additive exPlanations) values are a method for explaining individual predictions in machine learning models. They provide insights into the contribution of each feature towards the predicted outcome for a specific instance.\n",
    "\n",
    "The computation of SHAP values is based on cooperative game theory and the concept of Shapley values. SHAP values aim to fairly distribute the contribution of each feature across different subsets of features in the model. They provide a unified and consistent framework for feature importance estimation.\n",
    "\n",
    "Interpreting SHAP plots can be done in the following way:\n",
    "\n",
    "Feature Importance: The features are displayed on the y-axis, ordered from top to bottom based on their importance. The importance is determined by the magnitude of the SHAP values. Features at the top have the highest impact on the model's predictions.\n",
    "\n",
    "Impact Direction: Each feature is represented by a horizontal bar. The color of the bar indicates the value of the feature for a specific instance. Red indicates high feature values, while blue represents low values.\n",
    "\n",
    "SHAP Value: The length of the bar represents the magnitude of the SHAP value. Longer bars indicate higher contributions to increasing the prediction value, while shorter bars indicate lower contributions or even decreasing the prediction value.\n",
    "\n",
    "Interaction Effects: If multiple features are correlated or interact with each other, the plot may show interactions. The overlapping of bars indicates that the features are dependent, and their combined effect differs from their individual effects.\n",
    "\n",
    "Reference Point: The vertical gray dashed line represents the base value or the average prediction of the model. Features that push the prediction above this line contribute positively to the outcome, while features that pull it below the line contribute negatively.\n",
    "\n",
    "By analyzing the SHAP plots, you can identify which features have the most significant impact on predictions and understand the direction and magnitude of their contributions. This information helps in gaining insights into the model's behavior and the relationship between features and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fededfd-80c5-468c-b572-16cf35011c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, matthews_corrcoef\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr  \n",
    "\n",
    "# Convert the 'date' column to datetime\n",
    "testset_backup['date'] = pd.to_datetime(testset_backup['date'])\n",
    "\n",
    "# Initialize empty lists to store AUC and MCC values for each year\n",
    "auc_values = []\n",
    "mcc_values = []\n",
    "\n",
    "# Loop through each year bin\n",
    "for year_bin in range(2010, 2021):\n",
    "    # Filter data for the current year bin\n",
    "    test_data = testset_backup[testset_backup['date'].dt.year == year_bin]\n",
    "    y_test_year_bin = y_test[y_test.index.isin(test_data.index)]\n",
    "\n",
    "    if isinstance(selected_model, (HistGradientBoostingClassifier, BalancedRandomForestClassifier, LogisticRegression, GaussianNB)):\n",
    "        X_test_year_bin = X_test_OHE[X_test_OHE.index.isin(test_data.index)]\n",
    "        predictions_year_bin = selected_model.predict_proba(X_test_year_bin)\n",
    "        predictions_year_bin = predictions[:, 1]\n",
    "        # print(predictions_class)\n",
    "        y_pred_year_bin = [True if x >= 0.5 else False for x in predictions_year_bin]\n",
    "        misclassified_year_bin = y_pred_year_bin != y_test_year_bin\n",
    "    elif isinstance(selected_model, (cb.CatBoostClassifier)):\n",
    "        X_test_year_bin = X_test_withmissing[X_test_withmissing.index.isin(test_data.index)]\n",
    "        predictions_year_bin = selected_model.predict_proba(X_test_year_bin)\n",
    "        predictions_year_bin = predictions_year_bin[:, 1]\n",
    "        # print(predictions_class)\n",
    "        y_pred_year_bin = [True if x >= 0.5 else False for x in predictions_year_bin]\n",
    "        misclassified_year_bin = y_pred_year_bin != y_test_year_bin\n",
    "    else:\n",
    "        X_test_year_bin = X_test_withmissing[X_test_withmissing.index.isin(test_data.index)]\n",
    "        predictions_year_bin = selected_model.predict_proba(X_test_year_bin)\n",
    "        predictions_year_bin = predictions_year_bin[:, 1]\n",
    "        # print(predictions_class)\n",
    "        y_pred_year_bin = [True if x >= 0.5 else False for x in predictions_year_bin]\n",
    "        misclassified_year_bin = y_pred_year_bin != y_test_year_bin\n",
    "\n",
    "    # Calculate AUC and MCC\n",
    "    auc = roc_auc_score(y_test_year_bin, predictions_year_bin)\n",
    "    mcc = matthews_corrcoef(y_test_year_bin, y_pred_year_bin)\n",
    "\n",
    "    # Append values to the lists\n",
    "    auc_values.append(auc)\n",
    "    mcc_values.append(mcc)\n",
    "\n",
    "years = range(2010, 2021)\n",
    "# Perform Pearson correlation test between AUC and years\n",
    "correlation_coefficient, p_value = pearsonr(auc_values, years)\n",
    "\n",
    "# Print the correlation coefficient and p-value\n",
    "print(f\"Pearson Correlation Coefficient (AUC): {correlation_coefficient}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "# Plotting AUC and MCC over the years\n",
    "plt.plot(years, auc_values, label='AUC')\n",
    "plt.plot(years, mcc_values, label='MCC')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Metric Value')\n",
    "plt.title('AUC and MCC Over the Years')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0c1ed0-72ae-45b8-8bc2-319ba3718853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_index = y_test.index\n",
    "\n",
    "# Identify correct and incorrect predictions when y_test is True\n",
    "correctly_predicted_age_and_s = testset_backup.loc[test_index[(y_test == y_pred)], ['Age', 'Sex']]\n",
    "incorrectly_predicted_age_and_s = testset_backup.loc[test_index[(y_test != y_pred)], ['Age', 'Sex']]\n",
    "\n",
    "# Create DataFrames for correct and incorrect predictions\n",
    "correct_results_df = pd.DataFrame({\"Age\": correctly_predicted_age_and_s['Age'], 'Sex': correctly_predicted_age_and_s['Sex']})\n",
    "incorrect_results_df = pd.DataFrame({\"Age\": incorrectly_predicted_age_and_s['Age'], 'Sex': incorrectly_predicted_age_and_s['Sex']})\n",
    "\n",
    "# Save DataFrames to separate Excel files\n",
    "correct_results_df.to_excel(\"correctly_predicted_age_and_s.xlsx\", index=False)\n",
    "incorrect_results_df.to_excel(\"incorrectly_predicted_age_and_s.xlsx\", index=False)\n",
    "\n",
    "print(\"Results saved to correct_predictions.xlsx and incorrect_predictions.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76a9d7-6e6e-45da-85e9-a82c6c92ea73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "# Combine correct and incorrect predictions in a single DataFrame for plotting\n",
    "combined_df = pd.concat([correct_results_df.assign(Type='Correct'), incorrect_results_df.assign(Type='Incorrect')])\n",
    "\n",
    "# Plotting boxplot for correct and incorrect predictions\n",
    "plt.figure(figsize=(5, 6))\n",
    "\n",
    "sns.boxplot(x='Sex', y='Age', data=combined_df, hue='Type', palette='coolwarm')\n",
    "plt.title('Distribution of age for correct and incorrect predictions')\n",
    "\n",
    "# Statistical test (Mann-Whitney U test)\n",
    "statistic, p_value = mannwhitneyu(correct_results_df['Age'], incorrect_results_df['Age'])\n",
    "print(f\"Mann-Whitney U test p-value: {p_value}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f029aae-fca2-41bd-8e48-072e16c5c8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify correct and incorrect predictions when y_test is True\n",
    "correctly_predicted_ids = testset_backup.loc[test_index[(y_test == y_pred) & y_test], 'allbac']\n",
    "incorrectly_predicted_ids = testset_backup.loc[test_index[(y_test != y_pred) & y_test], 'allbac']\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrames for correct and incorrect predictions\n",
    "correct_results_df = pd.DataFrame({\"Correctly Predicted allbacs\": correctly_predicted_ids})\n",
    "incorrect_results_df = pd.DataFrame({\"Incorrectly Predicted allbacs\": incorrectly_predicted_ids})\n",
    "\n",
    "# Save DataFrames to separate Excel files\n",
    "correct_results_df.to_excel(\"correct_predictions.xlsx\", index=False)\n",
    "incorrect_results_df.to_excel(\"incorrect_predictions.xlsx\", index=False)\n",
    "\n",
    "print(\"Results saved to correct_predictions.xlsx and incorrect_predictions.xlsx\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6982971-8ae8-4c52-a78a-dc7ecad7f01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correctly_predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17dc584-11d1-49f7-be2e-8e043e58e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrectly_predicted_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cfd629-aa5b-4b06-8063-97f21996c869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split names in the 'names' column and create a list of all names\n",
    "all_names = [name.split(';') for name in correct_results_df['Correctly Predicted allbacs']]\n",
    "\n",
    "# Flatten the list of lists\n",
    "flat_list = [name for sublist in all_names for name in sublist]\n",
    "\n",
    "# Create a Pandas Series from the flattened list\n",
    "names_series = pd.Series(flat_list)\n",
    "\n",
    "# Get the counts of each name\n",
    "name_counts = names_series.value_counts()\n",
    "\n",
    "# Plot the most frequent names\n",
    "top_names = name_counts.head(10) \n",
    "top_names.plot(kind='bar', xlabel='Names', ylabel='Frequency', title='Top 10 most frequent pathogens correctly predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99117b8-95b3-4d51-8149-7f41795d2b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split names in the 'names' column and create a list of all names\n",
    "all_names = [name.split(';') for name in correct_results_df['Correctly Predicted allbacs']]\n",
    "\n",
    "# Flatten the list of lists\n",
    "flat_list = [name for sublist in all_names for name in sublist]\n",
    "\n",
    "# Create a Pandas Series from the flattened list\n",
    "names_series = pd.Series(flat_list)\n",
    "\n",
    "# Get the counts of each name\n",
    "name_counts = names_series.value_counts()\n",
    "\n",
    "# Plot the most frequent names\n",
    "top_names = name_counts.head(10)  \n",
    "top_names.plot(kind='bar', xlabel='Names', ylabel='Frequency', title='Top 10 most frequent pathogens correctly predicted')\n",
    "plt.show()\n",
    "\n",
    "# Split names in the 'names' column and create a list of all names\n",
    "all_names = [name.split(';') for name in incorrect_results_df['Incorrectly Predicted allbacs']]\n",
    "\n",
    "# Flatten the list of lists\n",
    "flat_list = [name for sublist in all_names for name in sublist]\n",
    "\n",
    "# Create a Pandas Series from the flattened list\n",
    "names_series = pd.Series(flat_list)\n",
    "\n",
    "# Get the counts of each name\n",
    "name_counts = names_series.value_counts()\n",
    "\n",
    "# Plot the most frequent names\n",
    "top_names = name_counts.head(10)  \n",
    "top_names.plot(kind='bar', xlabel='Names', ylabel='Frequency', title='Top 10 most frequent pathogens incorrectly predicted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd66534-da56-4a56-8ef9-9d3c4249ae35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Extract names from correct predictions\n",
    "all_names_correct = [name.split(';') for name in correct_results_df['Correctly Predicted allbacs']]\n",
    "flat_list_correct = [name for sublist in all_names_correct for name in sublist]\n",
    "names_series_correct = pd.Series(flat_list_correct)\n",
    "name_counts_correct = names_series_correct.value_counts()\n",
    "\n",
    "# Extract names from incorrect predictions\n",
    "all_names_incorrect = [name.split(';') for name in incorrect_results_df['Incorrectly Predicted allbacs']]\n",
    "flat_list_incorrect = [name for sublist in all_names_incorrect for name in sublist]\n",
    "names_series_incorrect = pd.Series(flat_list_incorrect)\n",
    "name_counts_incorrect = names_series_incorrect.value_counts()\n",
    "\n",
    "# Find common names\n",
    "common_names = list(set(name_counts_correct.index) & set(name_counts_incorrect.index))\n",
    "\n",
    "# Create a DataFrame with counts for each common name\n",
    "common_names_df = pd.DataFrame(index=common_names, columns=['Correct Counts', 'Incorrect Counts'])\n",
    "\n",
    "# Fill in the counts for correct predictions\n",
    "common_names_df['Correct Counts'] = name_counts_correct[common_names]\n",
    "\n",
    "# Fill in the counts for incorrect predictions\n",
    "common_names_df['Incorrect Counts'] = name_counts_incorrect[common_names]\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(common_names_df)\n",
    "common_names_df.to_excel(\"common_names_df.xlsx\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7695c9ac-c9f5-410c-8c60-5e1faaae0b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with counts for each common name\n",
    "common_names_df = pd.DataFrame(index=common_names, columns=['Correctly classified samples', 'Incorrectly classified samples'])\n",
    "\n",
    "# Fill in the counts for correct predictions\n",
    "common_names_df['Correctly classified samples'] = name_counts_correct[common_names]\n",
    "\n",
    "# Fill in the counts for incorrect predictions\n",
    "common_names_df['Incorrectly classified samples'] = name_counts_incorrect[common_names]\n",
    "\n",
    "# Sort by clusters (you might want to adjust the clustering method and metric)\n",
    "clustered_df = common_names_df.copy()\n",
    "clustered_df['Total Counts'] = clustered_df.sum(axis=1)\n",
    "clustered_df = clustered_df.sort_values(by='Total Counts', ascending=False)\n",
    "clustered_df = clustered_df.drop('Total Counts', axis=1)\n",
    "\n",
    "# Set smaller font size\n",
    "sns.set(font_scale=0.6)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(8, 10))\n",
    "sns.heatmap(clustered_df,annot=True,cmap='YlGnBu',fmt='g', cbar_kws={'label': 'Counts'})\n",
    "plt.title('classification results by pathogens')\n",
    "plt.savefig(\"classification_by_pathogens\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aee8c50-feaa-42b7-9e73-9fdb5ae4a4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = clustered_df.copy()\n",
    "# Calculate True Positive Rate (TPR) and add a new column\n",
    "df['TruePositiveRate'] = df['Correctly classified samples'] / (df['Correctly classified samples'] + df['Incorrectly classified samples'])\n",
    "\n",
    "# Sort the DataFrame by 'TruePositiveRate' in descending order\n",
    "df = df.sort_values(by='TruePositiveRate', ascending=False)\n",
    "\n",
    "# Plotting the True Positive Rate\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "df['TruePositiveRate'].plot(kind='bar', ax=ax, color='lightcoral', edgecolor='black')\n",
    "\n",
    "# Customize background color\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.set_facecolor('white')\n",
    "\n",
    "# Adding labels and title\n",
    "ax.set_xlabel('pathogens')\n",
    "ax.set_ylabel('true positive rate')\n",
    "ax.set_title('breakdown of true positive rates for pathogens in the test set')\n",
    "plt.savefig(\"TPR_by_pathogens\", dpi=300)\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8ad8ae-0f40-4cff-9119-17f73c957f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "testset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c75f9e-5454-4e3b-bd2a-f2d5ef7fa5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(devset_backup[\"ID\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1921b057-7661-4abd-a8fe-54594abec42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(testset_backup[\"ID\"].unique())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
